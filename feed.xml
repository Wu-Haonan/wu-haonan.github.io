<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://wu-haonan.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://wu-haonan.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-01T20:28:57+00:00</updated><id>https://wu-haonan.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Expectation of absolute distance in Random walk</title><link href="https://wu-haonan.github.io/blog/2023/Random_Walk/" rel="alternate" type="text/html" title="Expectation of absolute distance in Random walk"/><published>2023-12-09T00:00:00+00:00</published><updated>2023-12-09T00:00:00+00:00</updated><id>https://wu-haonan.github.io/blog/2023/Random_Walk</id><content type="html" xml:base="https://wu-haonan.github.io/blog/2023/Random_Walk/"><![CDATA[<p>This blog, I will post my proof about the expectation of absolute distance in Random walk is ${\Theta(\sqrt{i})}$ at ${i}$th step.</p> <h1 id="statement-of-problem">Statement of Problem</h1> <p>Consider a sequence of $n$ flips of an unbiased coin. Let $H_i$ denote the absolute value of the excess of then number of HEADS over the number of TAILS seen in the first $i$ flips. Show that $E[H_i] = \Theta(\sqrt{i})$.</p> <p>We can also treat is as a 1D random walk problem.</p> <h1 id="idea-of-my-proof">Idea of my proof</h1> <ol> <li> <p>Use the relation between <strong>Expectation</strong> and <strong>Variance</strong> to get the upper bound $\sqrt{i}$.</p> </li> <li> <p>Solve the recursive formula of $E[H_i]$, which can give us a sense of growth speed. We can prove the lower bound greater than ${C\sqrt{i}}$ by induction, where C ranges from $0$ to $1$.</p> </li> </ol> <h1 id="lemma-1-mathbbeh_i-satisfies-following-recursive-formula"><strong>Lemma 1:</strong> $\mathbb{E}[H_i]$ satisfies following recursive formula</h1> \[\begin{equation} \mathbb{E}[H_i] = \begin{cases} \mathbb{E}[H_{i-1}] + \frac{1}{2^{i-1}} \binom{i-1}{(i-1)/2}, &amp; n \text{ is odd,}\\ \mathbb{E}[H_{i-1}],&amp; n \text{ is even.} \end{cases}\nonumber \end{equation}\] <p><strong>Proof.</strong> By definition, we have</p> \[\begin{equation} \begin{aligned} \mathbb{E}[H_i] &amp;= \sum_{k=0}^{i} \vert -i + 2k \vert \Pr[k \text{ HEADS in first } i \text{ flips}] \\ &amp;= \sum_{k=0}^{i} \vert -i + 2k \vert \binom{i}{k} \frac{1}{2^i} \end{aligned}\nonumber \end{equation}\] <ul> <li>If $i$ is even and $i-1$ is odd. We have</li> </ul> \[\begin{equation} \begin{aligned} \mathbb{E}[H_i] &amp;= \sum_{k=0}^{i} \vert -i + 2k \vert \binom{i}{k} \frac{1}{2^i} \\ &amp; = 2 \cdot \sum_{k=i/2}^{i} \vert -i + 2k \vert \binom{i}{k} \frac{1}{2^i} \\ &amp; = \vert -i + 2(i/2) \vert \binom{i}{i/2} \frac{1}{2^i} + 2 \cdot \sum_{k=i/2+1}^{i} \vert -i + 2k \vert \binom{i}{k} \frac{1}{2^i} \\ &amp; = 2 \cdot \sum_{k=i/2}^{i} \vert -i + 2k \vert \binom{i}{k} \frac{1}{2^i} \\ &amp; = 2 \cdot \sum_{k=i/2}^{i-1} (-i + 2k) \binom{i}{k} \frac{1}{2^i} + 2 \vert -i + 2i \vert \binom{i}{i} \frac{1}{2^i} \\ &amp; = 2 \cdot \sum_{k=i/2}^{i-1} (-i + 2k) \binom{i}{k} \frac{1}{2^i} + \frac{i}{2^{i-1}} \end{aligned}\nonumber \end{equation}\] <p>And</p> \[\begin{equation} \begin{aligned} \mathbb{E}[H_{i-1}] &amp;= \sum_{k=0}^{i-1} \vert -i+1 + 2k \vert \binom{i-1}{k} \frac{1}{2^{i-1}} \\ &amp; = 2\cdot \sum_{k=i/2}^{i-1} \vert -i+1 + 2k \vert \binom{i-1}{k} \frac{1}{2^{i-1}}\\ &amp; = 2\cdot \sum_{k=i/2}^{i-1} ( -i+1 + 2k ) \binom{i-1}{k} \frac{1}{2^{i-1}}\\ \end{aligned}\nonumber \end{equation}\] <p>Then, we have</p> \[\begin{equation} \begin{aligned} \mathbb{E}[H_i] - \mathbb{E}[H_{i-1}] &amp;= 2 \cdot \sum_{k=i/2}^{i-1} (-i + 2k) \binom{i}{k} \frac{1}{2^i} + \frac{i}{2^{i-1}} - 2\cdot \sum_{k=i/2}^{i-1} ( -i+1 + 2k ) \binom{i-1}{k} \frac{1}{2^{i-1}} \\ &amp; = 2\left(\sum_{k=i/2}^{i-1} (-i + 2k) \binom{i}{k} \frac{1}{2^i} - ( -i+1 + 2k ) \binom{i-1}{k} \frac{1}{2^{i-1}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2\left(\sum_{k=i/2}^{i-1} \frac{(-i + 2k)}{2^i} \left( \binom{i-1}{k} + \binom{i-1}{k-1}\right)- \binom{i-1}{k} \frac{( -i+1 + 2k )}{2^{i-1}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2\left(\sum_{k=i/2}^{i-1} \frac{(-i + 2k)}{2^i} \binom{i-1}{k-1} + \binom{i-1}{k} \frac{( i -2 - 2k )}{2^{i}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2\left(\sum_{k=i/2}^{i-1} \frac{(-i + 2k)}{2^i} \binom{i-1}{k-1} - \sum_{k=i/2}^{i-1} \binom{i-1}{k} \frac{( -i +2 + 2k )}{2^{i}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2\left(\sum_{k=i/2}^{i-1} \frac{(-i + 2k)}{2^i} \binom{i-1}{k-1} - \sum_{j=i/2+1}^{i} \binom{i-1}{j-1} \frac{( -i + 2j )}{2^{i}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2\left( \frac{(-i + 2(i/2))}{2^i} \binom{i-1}{i/2-1} - \binom{i-1}{i-1} \frac{( -i + 2i )}{2^{i}} \right) + \frac{i}{2^{i-1}} \\ &amp; = 2\left( 0\cdot \binom{i-1}{i/2-1} - \binom{i-1}{i-1} \frac{i}{2^{i}} \right) + \frac{i}{2^{i-1}} \\ &amp; = 0 \end{aligned}\nonumber \end{equation}\] <ul> <li>If $i$ is odd and $i-1$ is even. We have</li> </ul> \[\begin{equation} \begin{aligned} \mathbb{E}[H_{i}] &amp;= \sum_{k=0}^{i} \vert -i + 2k \vert \binom{i}{k} \frac{1}{2^{i}} \\ &amp; = \sum_{k=0}^{(i-1)/2} \vert -i + 2k \vert \binom{i}{k} \frac{1}{2^{i}} + \sum_{k=(i+1)/2}^{i} \vert -i + 2k \vert \binom{i}{k} \frac{1}{2^{i}} \\ &amp; = 2\cdot \sum_{k=(i+1)/2}^{i} \vert -i + 2k \vert \binom{i}{k} \frac{1}{2^{i}}\\ &amp; = 2\cdot \sum_{k=(i+1)/2}^{i-1} ( -i + 2k ) \binom{i}{k} \frac{1}{2^{i}} + 2 \cdot ( -i + 2i ) \binom{i}{i} \frac{1}{2^{i}} \\ &amp; = 2\cdot \sum_{k=(i+1)/2}^{i-1} ( -i + 2k ) \binom{i}{k} \frac{1}{2^{i}} + \frac{i}{2^{i-1}} \\ \end{aligned}\nonumber \end{equation}\] <p>And</p> \[\begin{equation} \begin{aligned} \mathbb{E}[H_{i-1}] &amp;= \sum_{k=0}^{i-1} \vert -i+1 + 2k \vert \binom{i-1}{k} \frac{1}{2^{i-1}} \\ &amp; = 2 \cdot \sum_{k=(i+1)/2}^{i-1} (-i+1 + 2k) \binom{i-1}{k} \frac{1}{2^{i-1}} \end{aligned}\nonumber \end{equation}\] <p>Then, we have</p> \[\begin{equation} \begin{aligned} \mathbb{E}[H_i] - \mathbb{E}[H_{i-1}] &amp;= 2\cdot \sum_{k=(i+1)/2}^{i-1} ( -i + 2k ) \binom{i}{k} \frac{1}{2^{i}} + \frac{i}{2^{i-1}} - 2 \cdot \sum_{k=(i+1)/2}^{i-1} (-i+1 + 2k) \binom{i-1}{k} \frac{1}{2^{i-1}} \\ &amp; = 2 \cdot \left(\sum_{k=(i+1)/2}^{i-1} ( -i + 2k ) \binom{i}{k} \frac{1}{2^{i}} - (-i+1 + 2k) \binom{i-1}{k} \frac{1}{2^{i-1}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2 \cdot \left(\sum_{k=(i+1)/2}^{i-1} \frac{( -i + 2k )}{2^{i}} \left(\binom{i-1}{k} + \binom{i-1}{k-1}\right) -\binom{i-1}{k} \frac{ (-i+1 + 2k) }{2^{i-1}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2 \cdot \left(\sum_{k=(i+1)/2}^{i-1} \frac{( -i + 2k )}{2^{i}} \binom{i-1}{k-1}+\binom{i-1}{k} \frac{ (i-2 - 2k) }{2^{i}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2 \cdot \left(\sum_{k=(i+1)/2}^{i-1} \frac{( -i + 2k )}{2^{i}} \binom{i-1}{k-1} - \sum_{k=(i+1)/2}^{i-1} \binom{i-1}{k} \frac{ (-i+2 + 2k) }{2^{i}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2 \cdot \left(\sum_{k=(i+1)/2}^{i-1} \frac{( -i + 2k )}{2^{i}} \binom{i-1}{k-1} - \sum_{j=(i+3)/2}^{i} \binom{i-1}{j-1} \frac{ (-i+2 j) }{2^{i}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2 \cdot \left( \frac{( -i + 2((i+1)/2) )}{2^{i}} \binom{i-1}{(i+1)/2-1} - \binom{i-1}{i-1} \frac{ (-i+2 i) }{2^{i}} \right) + \frac{i}{2^{i-1}}\\ &amp; = \frac{1}{2^{i-1}} \binom{i-1}{(i-1)/2} \end{aligned}\nonumber \end{equation}\] <p>Hence, we prove the recursive formula</p> \[\begin{equation} \mathbb{E}[H_i] = \begin{cases} \mathbb{E}[H_{i-1}] + \frac{1}{2^{i-1}} \binom{i-1}{(i-1)/2}, &amp; n \text{ is odd,}\\ \mathbb{E}[H_{i-1}],&amp; n \text{ is even.} \end{cases}\nonumber \quad \quad \quad \square \end{equation}\] <h1 id="theorem-1-mathbbeh_i--thetasqrti"><strong>Theorem 1</strong>: $\mathbb{E}[H_i] = \Theta(\sqrt{i})$.</h1> <p><strong>Proof.</strong></p> <ol> <li>First, Let’s show $\mathbb{E}[H_i] \leq \sqrt{i}$ Denote the random variable $X_j, j = 1,2,\cdots, i$ as below</li> </ol> \[\begin{equation} X_j = \begin{cases} 1 &amp; \text{HEAD is seen in } i \text{th flip}\\ -1 &amp; \text{TAIL is seen in } i \text{th flip}\\ \end{cases}\nonumber \end{equation}\] <p>It’s easy to have $H_i = \vert \sum_{j=1}^i X_i \vert $, $\mathbb{E}[X_i]=0$ and we know $X_1,X_2,\cdots,X_i$ are pairwise independent. By the definition of variance $\mathbb{D}[H_i]$, we have $\mathbb{D}[H_i] = \mathbb{E}[H_i^2] - (\mathbb{E}[H_i])^2$. Then we have</p> \[\begin{equation} \begin{aligned} (\mathbb{E}[H_i])^2 = \mathbb{E}[H_i^2] - \mathbb{D}[H_i] \end{aligned}\nonumber \end{equation}\] <p>Because $X_1,X_2,\cdots,X_i$ are pairwise independent, we have</p> \[\begin{equation} \begin{aligned} \mathbb{E}[H_i^2] &amp; = \mathbb{E}\left[\left(\sum_{j=1}^i X_j\right)^2\right] \\ &amp; = \sum_{j=1}^i \mathbb{E}[X_j^2] + 2 \sum_{j&lt;k} \mathbb{E}[X_j X_k]\\ &amp; = \sum_{j=1}^i \mathbb{E}[X_j^2] + 2 \sum_{j&lt;k} \mathbb{E}[X_j] \mathbb{E}[X_k]\\ &amp; = i \end{aligned}\nonumber \end{equation}\] <p>$(\mathbb{E}[H_i])^2 = \mathbb{E}[H_i^2] - \mathbb{D}[H_i]$ that means $(\mathbb{E}[H_i])^2 \leq \mathbb{E}[H_i^2] = i$. Thus $\mathbb{E}[H_i] \leq \sqrt{i}$.</p> <ol> <li>We will show $\mathbb{E}[H_i] &gt; C \sqrt{i}, C= e^{-1} \sqrt{\frac{2}{\pi}}$ by induction (It’s clear $0&lt;C&lt;1$)</li> </ol> <p>Base case: For $i=1$, we know $\mathbb{E}[H_1] = \vert 1 \vert \cdot \frac{1}{2}+\vert -1 \vert \cdot \frac{1}{2} = 1 &gt; C \sqrt{1}$. And for $i=2$, we know $\mathbb{E}[H_2] = \vert 2 \vert \cdot \frac{1}{4} +\vert -2 \vert \cdot \frac{1}{4} + 0 \cdot \frac{1}{2} = 1 &gt; \frac{\sqrt{2}}{e} \sqrt{\frac{2}{\pi}} = C \sqrt{2} $.\</p> <p>Induction Hypothesis: $\mathbb{E}[H_j] &gt; \frac{\sqrt{j}}{2}, \forall j \leq k$</p> <p>We will show ${\mathbb{E}[H_{k+1}] &gt; \frac{\sqrt{k+1}}{2}}$ in the following</p> <ul> <li>If $k+1$ is even: By Lemma 1, $\mathbb{E}[H_{k+1}] = \mathbb{E}[H_{k}] = \mathbb{E}[H_{k-1}] + \frac{1}{2^{k-1}} \binom{k-1}{(k-1)/2} $. Then, we have</li> </ul> \[\begin{equation} \begin{aligned} \frac{1}{2^{k-1}} \binom{k-1}{(k-1)/2} &amp; = \frac{1}{2^{k-1}} \frac{(k-1)!}{\left(\left(\frac{k-1}{2}\right)!\right)^2} \end{aligned}\nonumber \end{equation}\] <p>By Stirling formula, we have</p> \[\begin{equation} \begin{aligned} \frac{1}{2^{k-1}} \binom{k-1}{(k-1)/2} &amp; = \frac{1}{2^{k-1}} \frac{(k-1)!}{\left(\left(\frac{k-1}{2}\right)!\right)^2} \\ &amp; &gt; \frac{1}{2^{k-1}} \frac{\sqrt{2 \pi (k-1)} \left(\frac{k-1}{e}\right)^{k-1}}{\pi (k-1) \left(\frac{k-1}{2e}\right)^{k-1} e^\frac{1}{12(k-1)}}\\ &amp; = e^{-\frac{1}{12(k-1)}} \sqrt{\frac{2}{\pi}} \frac{1}{\sqrt{k-1}}\\ &amp; &gt; e^{-1} \sqrt{\frac{2}{\pi}} \frac{1}{\sqrt{k-1}}\\ &amp; = e^{-1} \sqrt{\frac{2}{\pi}} \frac{2}{2\sqrt{k-1}}&gt; C \frac{2}{\sqrt{k+1} + \sqrt{k-1}} = C (\sqrt{k+1} - \sqrt{k-1}) \end{aligned}\nonumber \end{equation}\] <p>By induction hypothesis, $\mathbb{E}[H_{k-1}] &gt; C\sqrt{k-1}$, that means</p> \[\begin{equation} \begin{aligned} \mathbb{E}[H_{k+1}] &amp;= \mathbb{E}[H_{k}] \\ &amp;= \mathbb{E}[H_{k-1}] + \frac{1}{2^{k-1}} \binom{k-1}{(k-1)/2}\\ &amp; &gt; C\sqrt{k-1} + C (\sqrt{k+1} - \sqrt{k-1}) \\ &amp; = C \sqrt{k+1} \end{aligned}\nonumber \end{equation}\] <ul> <li>If $k+1$ is odd: By Lemma 1, $\mathbb{E}[H_{k+1}] = \mathbb{E}[H_{k}] + \frac{1}{2^{k}} \binom{k}{k/2} $. Then, we have</li> </ul> \[\begin{equation} \begin{aligned} \frac{1}{2^{k}} \binom{k}{k/2} &amp; = \frac{1}{2^{k}} \frac{k!}{\left(\left(\frac{k}{2}\right)!\right)^2} \end{aligned}\nonumber \end{equation}\] <p>By Stirling formula, we have</p> \[\begin{equation} \begin{aligned} \frac{1}{2^{k}} \binom{k}{k/2} &amp; &gt; \frac{1}{2^{k}} \frac{\sqrt{2 \pi k} \left(\frac{k}{e}\right)^{k}}{\pi k \left(\frac{k}{2e}\right)^{k} e^\frac{1}{12k}}\\ &amp; &gt; e^{-1} \sqrt{\frac{2}{\pi}} \frac{1}{\sqrt{k}}\\ &amp; &gt; e^{-1} \sqrt{\frac{2}{\pi}} \frac{1}{2\sqrt{k}}&gt; C \frac{1}{\sqrt{k+1} + \sqrt{k}} = C (\sqrt{k+1} - \sqrt{k}) \end{aligned}\nonumber \end{equation}\] <p>By induction hypothesis, $\mathbb{E}[H_{k}] &gt; C\sqrt{k}$, that means</p> \[\begin{equation} \begin{aligned} \mathbb{E}[H_{k+1}] &amp;= \mathbb{E}[H_{k}] + \frac{1}{2^{k}} \binom{k}{k/2}\\ &amp; &gt; C\sqrt{k} + C (\sqrt{k+1} - \sqrt{k}) \\ &amp; = C \sqrt{k+1} \end{aligned}\nonumber \end{equation}\] <p>Hence, we show $\mathbb{E}[H_i] &gt; C \sqrt{i}, C= e^{-1} \sqrt{\frac{2}{\pi}}$, and $\mathbb{E}[H_i] \leq \sqrt{i}$, which imply $\mathbb{E}[H_i] = \Theta(\sqrt{i})$. $\square$</p>]]></content><author><name></name></author><category term="Algorithm_Design_and_Analysis"/><category term="Random_walk"/><category term="Algorithm_Design_and_Analysis"/><category term="Random_walk"/><summary type="html"><![CDATA[This blog, I will post my proof about the expectation of absolute distance in Random walk is ${\Theta(\sqrt{i})}$ at ${i}$th step. Statement of Problem Consider a sequence of $n$ flips of an unbiased coin. Let $H_i$ denote the absolute value of the excess of then number of HEADS over the number of TAILS seen in the first $i$ flips. Show that $E[H_i] = \Theta(\sqrt{i})$. We can also treat is as a 1D random walk problem. Idea of my proof Use the relation between Expectation and Variance to get the upper bound $\sqrt{i}$. Solve the recursive formula of $E[H_i]$, which can give us a sense of growth speed. We can prove the lower bound greater than ${C\sqrt{i}}$ by induction, where C ranges from $0$ to $1$. Lemma 1: $\mathbb{E}[H_i]$ satisfies following recursive formula \[\begin{equation} \mathbb{E}[H_i] = \begin{cases} \mathbb{E}[H_{i-1}] + \frac{1}{2^{i-1}} \binom{i-1}{(i-1)/2}, &amp; n \text{ is odd,}\\ \mathbb{E}[H_{i-1}],&amp; n \text{ is even.} \end{cases}\nonumber \end{equation}\] Proof. By definition, we have \[\begin{equation} \begin{aligned} \mathbb{E}[H_i] &amp;= \sum_{k=0}^{i} \vert -i + 2k \vert \Pr[k \text{ HEADS in first } i \text{ flips}] \\ &amp;= \sum_{k=0}^{i} \vert -i + 2k \vert \binom{i}{k} \frac{1}{2^i} \end{aligned}\nonumber \end{equation}\] If $i$ is even and $i-1$ is odd. We have \[\begin{equation} \begin{aligned} \mathbb{E}[H_i] &amp;= \sum_{k=0}^{i} \vert -i + 2k \vert \binom{i}{k} \frac{1}{2^i} \\ &amp; = 2 \cdot \sum_{k=i/2}^{i} \vert -i + 2k \vert \binom{i}{k} \frac{1}{2^i} \\ &amp; = \vert -i + 2(i/2) \vert \binom{i}{i/2} \frac{1}{2^i} + 2 \cdot \sum_{k=i/2+1}^{i} \vert -i + 2k \vert \binom{i}{k} \frac{1}{2^i} \\ &amp; = 2 \cdot \sum_{k=i/2}^{i} \vert -i + 2k \vert \binom{i}{k} \frac{1}{2^i} \\ &amp; = 2 \cdot \sum_{k=i/2}^{i-1} (-i + 2k) \binom{i}{k} \frac{1}{2^i} + 2 \vert -i + 2i \vert \binom{i}{i} \frac{1}{2^i} \\ &amp; = 2 \cdot \sum_{k=i/2}^{i-1} (-i + 2k) \binom{i}{k} \frac{1}{2^i} + \frac{i}{2^{i-1}} \end{aligned}\nonumber \end{equation}\] And \[\begin{equation} \begin{aligned} \mathbb{E}[H_{i-1}] &amp;= \sum_{k=0}^{i-1} \vert -i+1 + 2k \vert \binom{i-1}{k} \frac{1}{2^{i-1}} \\ &amp; = 2\cdot \sum_{k=i/2}^{i-1} \vert -i+1 + 2k \vert \binom{i-1}{k} \frac{1}{2^{i-1}}\\ &amp; = 2\cdot \sum_{k=i/2}^{i-1} ( -i+1 + 2k ) \binom{i-1}{k} \frac{1}{2^{i-1}}\\ \end{aligned}\nonumber \end{equation}\] Then, we have \[\begin{equation} \begin{aligned} \mathbb{E}[H_i] - \mathbb{E}[H_{i-1}] &amp;= 2 \cdot \sum_{k=i/2}^{i-1} (-i + 2k) \binom{i}{k} \frac{1}{2^i} + \frac{i}{2^{i-1}} - 2\cdot \sum_{k=i/2}^{i-1} ( -i+1 + 2k ) \binom{i-1}{k} \frac{1}{2^{i-1}} \\ &amp; = 2\left(\sum_{k=i/2}^{i-1} (-i + 2k) \binom{i}{k} \frac{1}{2^i} - ( -i+1 + 2k ) \binom{i-1}{k} \frac{1}{2^{i-1}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2\left(\sum_{k=i/2}^{i-1} \frac{(-i + 2k)}{2^i} \left( \binom{i-1}{k} + \binom{i-1}{k-1}\right)- \binom{i-1}{k} \frac{( -i+1 + 2k )}{2^{i-1}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2\left(\sum_{k=i/2}^{i-1} \frac{(-i + 2k)}{2^i} \binom{i-1}{k-1} + \binom{i-1}{k} \frac{( i -2 - 2k )}{2^{i}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2\left(\sum_{k=i/2}^{i-1} \frac{(-i + 2k)}{2^i} \binom{i-1}{k-1} - \sum_{k=i/2}^{i-1} \binom{i-1}{k} \frac{( -i +2 + 2k )}{2^{i}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2\left(\sum_{k=i/2}^{i-1} \frac{(-i + 2k)}{2^i} \binom{i-1}{k-1} - \sum_{j=i/2+1}^{i} \binom{i-1}{j-1} \frac{( -i + 2j )}{2^{i}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2\left( \frac{(-i + 2(i/2))}{2^i} \binom{i-1}{i/2-1} - \binom{i-1}{i-1} \frac{( -i + 2i )}{2^{i}} \right) + \frac{i}{2^{i-1}} \\ &amp; = 2\left( 0\cdot \binom{i-1}{i/2-1} - \binom{i-1}{i-1} \frac{i}{2^{i}} \right) + \frac{i}{2^{i-1}} \\ &amp; = 0 \end{aligned}\nonumber \end{equation}\] If $i$ is odd and $i-1$ is even. We have \[\begin{equation} \begin{aligned} \mathbb{E}[H_{i}] &amp;= \sum_{k=0}^{i} \vert -i + 2k \vert \binom{i}{k} \frac{1}{2^{i}} \\ &amp; = \sum_{k=0}^{(i-1)/2} \vert -i + 2k \vert \binom{i}{k} \frac{1}{2^{i}} + \sum_{k=(i+1)/2}^{i} \vert -i + 2k \vert \binom{i}{k} \frac{1}{2^{i}} \\ &amp; = 2\cdot \sum_{k=(i+1)/2}^{i} \vert -i + 2k \vert \binom{i}{k} \frac{1}{2^{i}}\\ &amp; = 2\cdot \sum_{k=(i+1)/2}^{i-1} ( -i + 2k ) \binom{i}{k} \frac{1}{2^{i}} + 2 \cdot ( -i + 2i ) \binom{i}{i} \frac{1}{2^{i}} \\ &amp; = 2\cdot \sum_{k=(i+1)/2}^{i-1} ( -i + 2k ) \binom{i}{k} \frac{1}{2^{i}} + \frac{i}{2^{i-1}} \\ \end{aligned}\nonumber \end{equation}\] And \[\begin{equation} \begin{aligned} \mathbb{E}[H_{i-1}] &amp;= \sum_{k=0}^{i-1} \vert -i+1 + 2k \vert \binom{i-1}{k} \frac{1}{2^{i-1}} \\ &amp; = 2 \cdot \sum_{k=(i+1)/2}^{i-1} (-i+1 + 2k) \binom{i-1}{k} \frac{1}{2^{i-1}} \end{aligned}\nonumber \end{equation}\] Then, we have \[\begin{equation} \begin{aligned} \mathbb{E}[H_i] - \mathbb{E}[H_{i-1}] &amp;= 2\cdot \sum_{k=(i+1)/2}^{i-1} ( -i + 2k ) \binom{i}{k} \frac{1}{2^{i}} + \frac{i}{2^{i-1}} - 2 \cdot \sum_{k=(i+1)/2}^{i-1} (-i+1 + 2k) \binom{i-1}{k} \frac{1}{2^{i-1}} \\ &amp; = 2 \cdot \left(\sum_{k=(i+1)/2}^{i-1} ( -i + 2k ) \binom{i}{k} \frac{1}{2^{i}} - (-i+1 + 2k) \binom{i-1}{k} \frac{1}{2^{i-1}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2 \cdot \left(\sum_{k=(i+1)/2}^{i-1} \frac{( -i + 2k )}{2^{i}} \left(\binom{i-1}{k} + \binom{i-1}{k-1}\right) -\binom{i-1}{k} \frac{ (-i+1 + 2k) }{2^{i-1}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2 \cdot \left(\sum_{k=(i+1)/2}^{i-1} \frac{( -i + 2k )}{2^{i}} \binom{i-1}{k-1}+\binom{i-1}{k} \frac{ (i-2 - 2k) }{2^{i}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2 \cdot \left(\sum_{k=(i+1)/2}^{i-1} \frac{( -i + 2k )}{2^{i}} \binom{i-1}{k-1} - \sum_{k=(i+1)/2}^{i-1} \binom{i-1}{k} \frac{ (-i+2 + 2k) }{2^{i}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2 \cdot \left(\sum_{k=(i+1)/2}^{i-1} \frac{( -i + 2k )}{2^{i}} \binom{i-1}{k-1} - \sum_{j=(i+3)/2}^{i} \binom{i-1}{j-1} \frac{ (-i+2 j) }{2^{i}} \right) + \frac{i}{2^{i-1}}\\ &amp; = 2 \cdot \left( \frac{( -i + 2((i+1)/2) )}{2^{i}} \binom{i-1}{(i+1)/2-1} - \binom{i-1}{i-1} \frac{ (-i+2 i) }{2^{i}} \right) + \frac{i}{2^{i-1}}\\ &amp; = \frac{1}{2^{i-1}} \binom{i-1}{(i-1)/2} \end{aligned}\nonumber \end{equation}\] Hence, we prove the recursive formula \[\begin{equation} \mathbb{E}[H_i] = \begin{cases} \mathbb{E}[H_{i-1}] + \frac{1}{2^{i-1}} \binom{i-1}{(i-1)/2}, &amp; n \text{ is odd,}\\ \mathbb{E}[H_{i-1}],&amp; n \text{ is even.} \end{cases}\nonumber \quad \quad \quad \square \end{equation}\] Theorem 1: $\mathbb{E}[H_i] = \Theta(\sqrt{i})$. Proof. First, Let’s show $\mathbb{E}[H_i] \leq \sqrt{i}$ Denote the random variable $X_j, j=1,2,\cdots, i$ as below \[\begin{equation} X_j=\begin{cases} 1 &amp; \text{HEAD is seen in } i \text{th flip}\\ -1 &amp; \text{TAIL is seen in } i \text{th flip}\\ \end{cases}\nonumber \end{equation}\] It’s easy to have $H_i = \vert \sum_{j=1}^i X_i \vert $, $\mathbb{E}[X_i]=0$ and we know $X_1,X_2,\cdots,X_i$ are pairwise independent. By the definition of variance $\mathbb{D}[H_i]$, we have $\mathbb{D}[H_i] = \mathbb{E}[H_i^2] - (\mathbb{E}[H_i])^2$. Then we have \[\begin{equation} \begin{aligned} (\mathbb{E}[H_i])^2 = \mathbb{E}[H_i^2] - \mathbb{D}[H_i] \end{aligned}\nonumber \end{equation}\] Because $X_1,X_2,\cdots,X_i$ are pairwise independent, we have \[\begin{equation} \begin{aligned} \mathbb{E}[H_i^2] &amp; = \mathbb{E}\left[\left(\sum_{j=1}^i X_j\right)^2\right] \\ &amp; = \sum_{j=1}^i \mathbb{E}[X_j^2] + 2 \sum_{j&lt;k} \mathbb{E}[X_j X_k]\\ &amp; = \sum_{j=1}^i \mathbb{E}[X_j^2] + 2 \sum_{j&lt;k} \mathbb{E}[X_j] \mathbb{E}[X_k]\\ &amp; = i \end{aligned}\nonumber \end{equation}\] $(\mathbb{E}[H_i])^2 = \mathbb{E}[H_i^2] - \mathbb{D}[H_i]$ that means $(\mathbb{E}[H_i])^2 \leq \mathbb{E}[H_i^2] = i$. Thus $\mathbb{E}[H_i] \leq \sqrt{i}$. We will show $\mathbb{E}[H_i] &gt; C \sqrt{i}, C=e^{-1} \sqrt{\frac{2}{\pi}}$ by induction (It’s clear $0&lt;C&lt;1$) Base case: For $i=1$, we know $\mathbb{E}[H_1] = \vert 1 \vert \cdot \frac{1}{2}+\vert -1 \vert \cdot \frac{1}{2} = 1 &gt; C \sqrt{1}$. And for $i=2$, we know $\mathbb{E}[H_2] = \vert 2 \vert \cdot \frac{1}{4} +\vert -2 \vert \cdot \frac{1}{4} + 0 \cdot \frac{1}{2} = 1 &gt; \frac{\sqrt{2}}{e} \sqrt{\frac{2}{\pi}} = C \sqrt{2} $.\ Induction Hypothesis: $\mathbb{E}[H_j] &gt; \frac{\sqrt{j}}{2}, \forall j \leq k$ We will show ${\mathbb{E}[H_{k+1}] &gt; \frac{\sqrt{k+1}}{2}}$ in the following If $k+1$ is even: By Lemma 1, $\mathbb{E}[H_{k+1}] = \mathbb{E}[H_{k}] = \mathbb{E}[H_{k-1}] + \frac{1}{2^{k-1}} \binom{k-1}{(k-1)/2} $. Then, we have \[\begin{equation} \begin{aligned} \frac{1}{2^{k-1}} \binom{k-1}{(k-1)/2} &amp; = \frac{1}{2^{k-1}} \frac{(k-1)!}{\left(\left(\frac{k-1}{2}\right)!\right)^2} \end{aligned}\nonumber \end{equation}\] By Stirling formula, we have \[\begin{equation} \begin{aligned} \frac{1}{2^{k-1}} \binom{k-1}{(k-1)/2} &amp; = \frac{1}{2^{k-1}} \frac{(k-1)!}{\left(\left(\frac{k-1}{2}\right)!\right)^2} \\ &amp; &gt; \frac{1}{2^{k-1}} \frac{\sqrt{2 \pi (k-1)} \left(\frac{k-1}{e}\right)^{k-1}}{\pi (k-1) \left(\frac{k-1}{2e}\right)^{k-1} e^\frac{1}{12(k-1)}}\\ &amp; = e^{-\frac{1}{12(k-1)}} \sqrt{\frac{2}{\pi}} \frac{1}{\sqrt{k-1}}\\ &amp; &gt; e^{-1} \sqrt{\frac{2}{\pi}} \frac{1}{\sqrt{k-1}}\\ &amp; = e^{-1} \sqrt{\frac{2}{\pi}} \frac{2}{2\sqrt{k-1}}&gt; C \frac{2}{\sqrt{k+1} + \sqrt{k-1}} = C (\sqrt{k+1} - \sqrt{k-1}) \end{aligned}\nonumber \end{equation}\] By induction hypothesis, $\mathbb{E}[H_{k-1}] &gt; C\sqrt{k-1}$, that means \[\begin{equation} \begin{aligned} \mathbb{E}[H_{k+1}] &amp;= \mathbb{E}[H_{k}] \\ &amp;= \mathbb{E}[H_{k-1}] + \frac{1}{2^{k-1}} \binom{k-1}{(k-1)/2}\\ &amp; &gt; C\sqrt{k-1} + C (\sqrt{k+1} - \sqrt{k-1}) \\ &amp; = C \sqrt{k+1} \end{aligned}\nonumber \end{equation}\] If $k+1$ is odd: By Lemma 1, $\mathbb{E}[H_{k+1}] = \mathbb{E}[H_{k}] + \frac{1}{2^{k}} \binom{k}{k/2} $. Then, we have \[\begin{equation} \begin{aligned} \frac{1}{2^{k}} \binom{k}{k/2} &amp; = \frac{1}{2^{k}} \frac{k!}{\left(\left(\frac{k}{2}\right)!\right)^2} \end{aligned}\nonumber \end{equation}\] By Stirling formula, we have \[\begin{equation} \begin{aligned} \frac{1}{2^{k}} \binom{k}{k/2} &amp; &gt; \frac{1}{2^{k}} \frac{\sqrt{2 \pi k} \left(\frac{k}{e}\right)^{k}}{\pi k \left(\frac{k}{2e}\right)^{k} e^\frac{1}{12k}}\\ &amp; &gt; e^{-1} \sqrt{\frac{2}{\pi}} \frac{1}{\sqrt{k}}\\ &amp; &gt; e^{-1} \sqrt{\frac{2}{\pi}} \frac{1}{2\sqrt{k}}&gt; C \frac{1}{\sqrt{k+1} + \sqrt{k}} = C (\sqrt{k+1} - \sqrt{k}) \end{aligned}\nonumber \end{equation}\] By induction hypothesis, $\mathbb{E}[H_{k}] &gt; C\sqrt{k}$, that means \[\begin{equation} \begin{aligned} \mathbb{E}[H_{k+1}] &amp;= \mathbb{E}[H_{k}] + \frac{1}{2^{k}} \binom{k}{k/2}\\ &amp; &gt; C\sqrt{k} + C (\sqrt{k+1} - \sqrt{k}) \\ &amp; = C \sqrt{k+1} \end{aligned}\nonumber \end{equation}\] Hence, we show $\mathbb{E}[H_i] &gt; C \sqrt{i}, C=e^{-1} \sqrt{\frac{2}{\pi}}$, and $\mathbb{E}[H_i] \leq \sqrt{i}$, which imply $\mathbb{E}[H_i] = \Theta(\sqrt{i})$. $\square$]]></summary></entry><entry><title type="html">Randomized Algs:Permutation Routing Problem</title><link href="https://wu-haonan.github.io/blog/2023/ADA_Lec_27/" rel="alternate" type="text/html" title="Randomized Algs:Permutation Routing Problem"/><published>2023-11-22T00:00:00+00:00</published><updated>2023-11-22T00:00:00+00:00</updated><id>https://wu-haonan.github.io/blog/2023/ADA_Lec_27</id><content type="html" xml:base="https://wu-haonan.github.io/blog/2023/ADA_Lec_27/"><![CDATA[<p>This blog, we still talk about randomized algorithms. Let’s consider the parallel computation problem: A network of parallel processors is modeled by a directed graph ${G = (N, E)}$. The nodes ${N}$ represent the processors and the edges ${E}$ model the communication links between the processors. All communication between processors occurs in synchronous steps. Each link can carry at most one unit message (packet) in one step. During a step, a processor can send at most one packet to each of its neighbors. Each processor is uniquely identified by a number between 1 and ${N}$.</p> <h1 id="permutation-routing-problem">Permutation Routing Problem</h1> <p>We can abstract the above problem to following statement.</p> <p>Given a directed graph on ${N}$ nodes, where each node ${i}$ initially contains one <em>packet</em> destined for some node ${d(i)}$, s.t. ${d(\cdot)}$ is a permutation. In each <em>step</em>, every edge can carry a single packet. A node that may send a packet on each outgoing edge (if it has the packets).</p> <p>A <strong><em>route</em></strong> for a packet is a list of edges it can follow from its source to its destination.</p> <p>If two packets want to use the same edge, one may have to wait. The <strong><em>queueing discipline</em></strong> for an algorithm is how it decides which packet goes first (Like FIFO, first in first out).</p> <p>An <strong><em>oblivious algorithm</em></strong> for the permutation routing problem satisfies the following property: if the route followed by the packet starting at $v_i$ depends only on $d(i)$, not on $d(j)$ for any $j \neq i$. (Note: Oblivious routing algorithms are attractive for their simplicity of implementation : the communication hardware at each node in the network can determine the next link on its route, simply by looking at the source and destination information carried by a packet.)</p> <h2 id="lower-bound-of-oblivious-algorithm">Lower bound of oblivious algorithm</h2> <p><strong>Theorem 1.1</strong> For any deterministic oblivious permutation routing algorithm on a network of ${N}$ nodes each of out-degree ${d}$, there is an instance of permutation routing requiring $\Omega \left( \sqrt{\frac{N}{d}} \right)$ steps <sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>.</p> <p>Its proof will not be shown in this blog.</p> <p>Consider the implications of this theorem for the case when the network is the <strong><em>Boolean hypercube</em></strong>, a popular network for parallel processing. The Boolean hypercube has ${N=2^n}$ nodes connected in the following manner:</p> <p>If $(i_0, \ldots, i_{n-1})$ and $(j_0, \ldots, j_{n-1})$ are the (ordered) binary representations of node $i$ and node $j$ respectively, then there exists a directed edge $e_{ij} \in E$ and a directed edge $e_{ji} \in E$ if and only if $(i_0, \ldots, i_{n-1})$ and $(j_0, \ldots, j_{n-1})$ differ in exactly one position. Note that the maximum number of transitions is bounded by $\sqrt{2^n/n}$, which is an amazing huge number.</p> <h2 id="bit-fixing-strategy">Bit Fixing Strategy</h2> <ol> <li> <p>Given that the source and destination addresses are $n$-bit vectors, consider the following simple choice of route to send $v_i$ from $i$ to the node $\sigma(i)$:</p> </li> <li> <p>Scan the bits of $\sigma(i)$ from left to right, and compare them with the address of the current location of $v_i$.</p> </li> <li> <p>Send $v_i$ out of the current node along the edge corresponding to the leftmost bit in which the current position and $\sigma(i)$ differ.</p> </li> </ol> <p>The strategy is best explained through an example. In order to go from $(1, 1, 0, 1)$ to $(0, 0, 0, 0)$, the packet takes the following path:</p> \[(1, 1, 0, 1) \rightarrow (0, 1, 0, 1) \rightarrow (0, 0, 0, 1) \rightarrow (0, 0, 0, 0).\] <h1 id="randomized-routing">Randomized Routing</h1> <h2 id="algorithm">Algorithm</h2> <p>For each packet ${v_i}$, independently, define its route as follows:</p> <p><strong>Phase I</strong> Pick random ${\sigma(i)}$ from ${{1, \ldots, N}}$. Packet ${v_i}$ travels to ${\sigma(i)}$ using bit-fixing strategy.</p> <p><strong>Phase II</strong> Packet ${v_i}$ travels from ${\sigma(i)}$ to ${d(i)}$, using bit-fixing strategy.</p> <p>Queueing discipline: Arbitrary (FIFO).</p> <h2 id="analysis">Analysis</h2> <p>Let ${\text{delay}(v_i)}$ denote the number of steps ${v_i}$ spends in queues waiting for other packets to move during Phase I. Total #steps for ${v_i}$ in Phase I is at most ${n + \text{delay}(v_i)}$. Now, we hope to bound the ${\text{delay}(v_i)}$.</p> <p>**Lemma: ** Let ${p_i = (e_1, \ldots, e_k)}$ be the route for ${v_i}$, and let ${S_i}$ be the set of other paths intersecting ${p_i}$. Then $\text{delay}(v_i)\leq \vert {S_i}\vert$.</p> <p><strong>Proof.</strong></p> <p>To prove this lemma, we need to prove the following points:</p> <p><strong>Point 1:</strong> If the bit fixing algorithm is used to route a packet ${v_i}$ from $i$ to ${\sigma(i)}$ and ${v_j}$ from $j$ to ${\sigma(j)}$, then their routes do not rejoin after they separate.</p> <p>Suppose $k$ be the node at which the two paths separate and $l$ be the node at which they rejoin. Note that the route determined by the bit fixing scheme for ${v_i}$ and ${v_j}$ from $k$ to $l$ depends only on the bit representations of $k$ and $l$, therefore ${v_i}$ and ${v_j}$ must follow the same route. A contradiction to the assumption that $k$ is the last node before the paths separate. That means, if ${p_i,p_j}$ are intersected with an edge, their overlap may start from some ${k}$ and end at some.</p> <p><strong>Point 2:</strong> The lag of packet ${v_i}$ is at most ${\vert S_i \vert}$.</p> <p>If a packet is <strong>ready</strong> to follow edge ${e_j}$ at time ${t}$, we define its <strong>lag</strong> at time ${t}$ to be ${t-j}$. The <strong>lag</strong> of ${v_i}$ is initially zero, and the delay incurred by ${v_i}$ is its lag when it traversers ${e_k}$ (the end of the route).</p> <blockquote> <p>Note: 1) At beginning, we set time ${t=0}$. If ${v_i}$ passes the route without any wait, in the end ${t-k = k - k = 0}$. Hence, we can use ${t-j}$ to measure the delay before time ${t}$. 2) Additionally, there may be more than one packets that are ready to pass edge ${e_j}$ in time ${t}$, there <strong>lag</strong>s in time ${t}$ are same, that is ${t-j}$.</p> </blockquote> <p><strong>Goal:</strong> We will show that each step at which the lag of ${v_i}$ increases by one can be charged to a distinct member of ${S_i}$.</p> <p><strong>The way of charging: ** We will charge each increase ${\textbf{lag}(v_i)}$ to a packet in ${S_i}$ **leaving</strong> with <strong>lag</strong> as ${\ell}$.</p> <p><strong>Claim 1 (Existence of lag ${\ell}$): ** When **lag</strong> of ${v_i}$ increases from $l$ to $l+1$. There <strong>exists</strong> a packet that is waiting in the queue ready to pass $e_j$ at next time $t$, that means there exists a packet with <strong>lag</strong> as ${t-j= \textbf{lag}(v_i) = \ell}$.</p> <p><strong>Claim 2 (Leaving):</strong> Let $t’$ be the last time step at which any packet in $S$ has a lag $l$. Note that there are only a finite number of time steps. (Each packed in ${S}$ may have a series of <strong>lag</strong>s in the some time steps, now we consider the packet with <strong>lag</strong> as ${\ell}$ and with max time step ${t’}$, and we denote the packet as ${v}$ and ready to follow ${e_{j’}}$ at time ${t’}$ such that $t’ - j’ = l$.) Since $v$ demands ${e_j’}$, we assume a packet $\omega$ actually travels on ${e_j’}$ at time $t’$ (there may be many packet waiting in ${e_{j’}}$ at time ${t’}$, we denote the <strong>real issue</strong> one as ${\omega}$, it’s possible ${v = \omega}$). Then we have that $\omega$ <strong>must leave</strong> ${p_i}$ at time $t’$. Otherwise, ${\omega}$ would demand ${e_{j’+1}}$ at time $t’ + 1$ (because it just pass ${e_{j’}}$ at time ${t’}$) implying some packet must <strong>actually</strong> follow ${e_{j’+1}}$ at time $t’ + 1$, which violates the minimality of $t’$. By first item proved in this lemma, we know $\omega$ will never returns to ${p_i}$. Therefore, we this increase in lag to a packet ${\omega}$.</p> <p>Hence, we charge every increase to a distinct packet in ${S_i}$, which implies ${\text{delay}(v_i) \leq \vert S_i \vert }$. $\square$</p> <p><strong>Theorem 1.1</strong> With probability at least $1 - 2^{-5n}$, the packet ${v_i}$ reaches ${\sigma(i)}$ in $7n$ or fewer steps.</p> <p>**Proof. ** Define a random variable ${H_{ij} = 1}$ if ${p_i}$ and ${p_j}$ share at least one edge, and 0 otherwise. Then the total delay incurred by ${v_i}$ is at most ${\sum_{j=1}^N H_{ij}}$.</p> <p>Since, the ${\sigma(\cdot)}$ of various packets are chosen independently at random, the ${H_{ij}}$’s are independent Poisson trials for ${j \neq i}$. Thus, to bound the delay of packet ${v_i}$ from above using the Chernoff bound, it is enough to obtain an upper bound on the ${\sum_{j=1}^N H_{ij}}$. To do this we first bound ${E\left[\sum_{j=1}^N H_{ij}\right]}$.</p> <p>For an edge ${e}$ of the hypercube, let the random variable ${T(e)}$ be the number of routes that pass through ${e}$. If ${p_i = (e_1, \ldots, e_k)}$, then</p> \[\sum_{j=1}^N H_{ij} \leq \sum_{\ell=1}^k T(e_{\ell})\] <p>This follows from the fact that the total delay is bound by the total number of routes that pass through this route, and therefore</p> \[E\left[\sum_{j=1}^N H_{ij}\right] \leq E\left[\sum_{\ell=1}^k T(e_{\ell})\right]\] <p>The total number of edges in the directed graph is ${Nn}$ counting ${n}$ edges per node. The expected length of each route is ${\frac{n}{2}}$ (each bit with probability ${\frac{1}{2}}$ to be different, so the expected Hamming distance is ${\frac{n}{2}}$) and hence the expected length of the total route length summed over all packets is ${\frac{Nn}{2}}$. That means ${\mathbb{E}[\sum_{e\in E}T(e)] = \frac{Nn}{2}}$. Since, all edges in the hypercube are symmetric, ${E[T(e_l)] = E[T(e_m)]}$ for any two edges ${e_l}$ and ${e_m}$, we have \(E[T(e)] =\frac{1}{\vert E \vert} \mathbb{E}\left[\sum_{e\in E}T(e)\right] = \frac{N \cdot \frac{n}{2}}{Nn} = \frac{1}{2}\)</p> <p>Thus, for ${p_i = (e_1, \ldots, p_k)}$,</p> \[E\left[\sum_{j=1}^N H_{ij}\right] \leq \sum_{l=1}^k E[T(e_l)] = \frac{k}{2} \leq \frac{n}{2}\] <p>Now we can apply our first Chernoff bound to get</p> \[\begin{aligned} \Pr\left[\sum_{j=1}^N H_{ij} &gt; 6n\right] &amp;= \Pr\left[\sum_{j=1}^N H_{ij} &gt; (1+11)\frac{n}{2}\right] \\ &amp; &lt; \Pr\left[\sum_{j=1}^N H_{ij} &gt; (1+11)\mu \right] \\ &amp; &lt; \left(\frac{e^{11}}{(1+11)^{1+11}}\right)^{\mu}\\ &amp; &lt; \left(\frac{e^{11}}{(1+11)^{1+11}}\right)^{\frac{n}{2}}\\ &amp; &lt; 2^{-\left(1+11\right)\frac{n}{2}} = 2^{-6n} \end{aligned}\] <p>Since ${\text{delay}(v_i) \leq \sum_{j=1}^N H_{ij}}$, this gives</p> \[\Pr[\text{delay}(v_i) &gt; 6n] &lt; 2^{-6n}\] <p>Since the probability of a union of events is at most the sum of probabilities, the probability that any delay exceeds $6n$ is at most</p> \[\Pr[\max_i \text{delay}(v_i) &gt; 6n] \leq \sum_{i=1}^N \Pr[\text{delay}(v_i) &gt; 6n] &lt; N \cdot 2^{-6n} = 2^n \cdot 2^{-6n} = 2^{-5n}\] <p>Thus, since each path has length at most $n$</p> \[\Pr[\#Steps \text{ in Phase I} &gt; 7n] = \Pr[(n + \max_i \text{delay}(v_i) &gt; 7n] &lt; 2^{-5n} \quad \quad \square\] <p><strong>Theorem 1.2</strong> With a probability at least $1 - 2^{1-5n}$, every packet reaches its destination in $14n$ or fewer steps.</p> <p><strong>Proof.</strong> The Phase 2 of this scheme is identical to Phase 1, if the roles of the destination and source are interchanged. The above analysis is hence valid for the second phase also. Therefore, the probability that any packet fails to reach its target in either phase is less than $2 \times 2^{1-5n}= 2^{1-5n}$. ${\square}$</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p><a href="https://link.springer.com/article/10.1007/BF02090400">Kaklamanis, C., Krizanc, D. &amp; Tsantilas, T. Tight bounds for oblivious routing in the hypercube. <em>Math. Systems Theory</em> <strong>24</strong>, 223–232 (1991).</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="Algorithm_Design_and_Analysis"/><category term="Algorithm_Design_and_Analysis"/><summary type="html"><![CDATA[This blog, we still talk about randomized algorithms. Let’s consider the parallel computation problem: A network of parallel processors is modeled by a directed graph ${G = (N, E)}$. The nodes ${N}$ represent the processors and the edges ${E}$ model the communication links between the processors. All communication between processors occurs in synchronous steps. Each link can carry at most one unit message (packet) in one step. During a step, a processor can send at most one packet to each of its neighbors. Each processor is uniquely identified by a number between 1 and ${N}$. Permutation Routing Problem We can abstract the above problem to following statement. Given a directed graph on ${N}$ nodes, where each node ${i}$ initially contains one packet destined for some node ${d(i)}$, s.t. ${d(\cdot)}$ is a permutation. In each step, every edge can carry a single packet. A node that may send a packet on each outgoing edge (if it has the packets). A route for a packet is a list of edges it can follow from its source to its destination. If two packets want to use the same edge, one may have to wait. The queueing discipline for an algorithm is how it decides which packet goes first (Like FIFO, first in first out). An oblivious algorithm for the permutation routing problem satisfies the following property: if the route followed by the packet starting at $v_i$ depends only on $d(i)$, not on $d(j)$ for any $j \neq i$. (Note: Oblivious routing algorithms are attractive for their simplicity of implementation : the communication hardware at each node in the network can determine the next link on its route, simply by looking at the source and destination information carried by a packet.) Lower bound of oblivious algorithm Theorem 1.1 For any deterministic oblivious permutation routing algorithm on a network of ${N}$ nodes each of out-degree ${d}$, there is an instance of permutation routing requiring $\Omega \left( \sqrt{\frac{N}{d}} \right)$ steps 1. Its proof will not be shown in this blog. Consider the implications of this theorem for the case when the network is the Boolean hypercube, a popular network for parallel processing. The Boolean hypercube has ${N=2^n}$ nodes connected in the following manner: If $(i_0, \ldots, i_{n-1})$ and $(j_0, \ldots, j_{n-1})$ are the (ordered) binary representations of node $i$ and node $j$ respectively, then there exists a directed edge $e_{ij} \in E$ and a directed edge $e_{ji} \in E$ if and only if $(i_0, \ldots, i_{n-1})$ and $(j_0, \ldots, j_{n-1})$ differ in exactly one position. Note that the maximum number of transitions is bounded by $\sqrt{2^n/n}$, which is an amazing huge number. Bit Fixing Strategy Given that the source and destination addresses are $n$-bit vectors, consider the following simple choice of route to send $v_i$ from $i$ to the node $\sigma(i)$: Scan the bits of $\sigma(i)$ from left to right, and compare them with the address of the current location of $v_i$. Send $v_i$ out of the current node along the edge corresponding to the leftmost bit in which the current position and $\sigma(i)$ differ. The strategy is best explained through an example. In order to go from $(1, 1, 0, 1)$ to $(0, 0, 0, 0)$, the packet takes the following path: \[(1, 1, 0, 1) \rightarrow (0, 1, 0, 1) \rightarrow (0, 0, 0, 1) \rightarrow (0, 0, 0, 0).\] Randomized Routing Algorithm For each packet ${v_i}$, independently, define its route as follows: Phase I Pick random ${\sigma(i)}$ from ${{1, \ldots, N}}$. Packet ${v_i}$ travels to ${\sigma(i)}$ using bit-fixing strategy. Phase II Packet ${v_i}$ travels from ${\sigma(i)}$ to ${d(i)}$, using bit-fixing strategy. Queueing discipline: Arbitrary (FIFO). Analysis Let ${\text{delay}(v_i)}$ denote the number of steps ${v_i}$ spends in queues waiting for other packets to move during Phase I. Total #steps for ${v_i}$ in Phase I is at most ${n + \text{delay}(v_i)}$. Now, we hope to bound the ${\text{delay}(v_i)}$. **Lemma: ** Let ${p_i = (e_1, \ldots, e_k)}$ be the route for ${v_i}$, and let ${S_i}$ be the set of other paths intersecting ${p_i}$. Then $\text{delay}(v_i)\leq \vert {S_i}\vert$. Proof. To prove this lemma, we need to prove the following points: Point 1: If the bit fixing algorithm is used to route a packet ${v_i}$ from $i$ to ${\sigma(i)}$ and ${v_j}$ from $j$ to ${\sigma(j)}$, then their routes do not rejoin after they separate. Suppose $k$ be the node at which the two paths separate and $l$ be the node at which they rejoin. Note that the route determined by the bit fixing scheme for ${v_i}$ and ${v_j}$ from $k$ to $l$ depends only on the bit representations of $k$ and $l$, therefore ${v_i}$ and ${v_j}$ must follow the same route. A contradiction to the assumption that $k$ is the last node before the paths separate. That means, if ${p_i,p_j}$ are intersected with an edge, their overlap may start from some ${k}$ and end at some. Point 2: The lag of packet ${v_i}$ is at most ${\vert S_i \vert}$. If a packet is ready to follow edge ${e_j}$ at time ${t}$, we define its lag at time ${t}$ to be ${t-j}$. The lag of ${v_i}$ is initially zero, and the delay incurred by ${v_i}$ is its lag when it traversers ${e_k}$ (the end of the route). Note: 1) At beginning, we set time ${t=0}$. If ${v_i}$ passes the route without any wait, in the end ${t-k = k - k=0}$. Hence, we can use ${t-j}$ to measure the delay before time ${t}$. 2) Additionally, there may be more than one packets that are ready to pass edge ${e_j}$ in time ${t}$, there lags in time ${t}$ are same, that is ${t-j}$. Goal: We will show that each step at which the lag of ${v_i}$ increases by one can be charged to a distinct member of ${S_i}$. The way of charging: ** We will charge each increase ${\textbf{lag}(v_i)}$ to a packet in ${S_i}$ **leaving with lag as ${\ell}$. Claim 1 (Existence of lag ${\ell}$): ** When **lag of ${v_i}$ increases from $l$ to $l+1$. There exists a packet that is waiting in the queue ready to pass $e_j$ at next time $t$, that means there exists a packet with lag as ${t-j= \textbf{lag}(v_i) = \ell}$. Claim 2 (Leaving): Let $t’$ be the last time step at which any packet in $S$ has a lag $l$. Note that there are only a finite number of time steps. (Each packed in ${S}$ may have a series of lags in the some time steps, now we consider the packet with lag as ${\ell}$ and with max time step ${t’}$, and we denote the packet as ${v}$ and ready to follow ${e_{j’}}$ at time ${t’}$ such that $t’ - j’ = l$.) Since $v$ demands ${e_j’}$, we assume a packet $\omega$ actually travels on ${e_j’}$ at time $t’$ (there may be many packet waiting in ${e_{j’}}$ at time ${t’}$, we denote the real issue one as ${\omega}$, it’s possible ${v = \omega}$). Then we have that $\omega$ must leave ${p_i}$ at time $t’$. Otherwise, ${\omega}$ would demand ${e_{j’+1}}$ at time $t’ + 1$ (because it just pass ${e_{j’}}$ at time ${t’}$) implying some packet must actually follow ${e_{j’+1}}$ at time $t’ + 1$, which violates the minimality of $t’$. By first item proved in this lemma, we know $\omega$ will never returns to ${p_i}$. Therefore, we this increase in lag to a packet ${\omega}$. Hence, we charge every increase to a distinct packet in ${S_i}$, which implies ${\text{delay}(v_i) \leq \vert S_i \vert }$. $\square$ Theorem 1.1 With probability at least $1 - 2^{-5n}$, the packet ${v_i}$ reaches ${\sigma(i)}$ in $7n$ or fewer steps. **Proof. ** Define a random variable ${H_{ij} = 1}$ if ${p_i}$ and ${p_j}$ share at least one edge, and 0 otherwise. Then the total delay incurred by ${v_i}$ is at most ${\sum_{j=1}^N H_{ij}}$. Since, the ${\sigma(\cdot)}$ of various packets are chosen independently at random, the ${H_{ij}}$’s are independent Poisson trials for ${j \neq i}$. Thus, to bound the delay of packet ${v_i}$ from above using the Chernoff bound, it is enough to obtain an upper bound on the ${\sum_{j=1}^N H_{ij}}$. To do this we first bound ${E\left[\sum_{j=1}^N H_{ij}\right]}$. For an edge ${e}$ of the hypercube, let the random variable ${T(e)}$ be the number of routes that pass through ${e}$. If ${p_i = (e_1, \ldots, e_k)}$, then \[\sum_{j=1}^N H_{ij} \leq \sum_{\ell=1}^k T(e_{\ell})\] This follows from the fact that the total delay is bound by the total number of routes that pass through this route, and therefore \[E\left[\sum_{j=1}^N H_{ij}\right] \leq E\left[\sum_{\ell=1}^k T(e_{\ell})\right]\] The total number of edges in the directed graph is ${Nn}$ counting ${n}$ edges per node. The expected length of each route is ${\frac{n}{2}}$ (each bit with probability ${\frac{1}{2}}$ to be different, so the expected Hamming distance is ${\frac{n}{2}}$) and hence the expected length of the total route length summed over all packets is ${\frac{Nn}{2}}$. That means ${\mathbb{E}[\sum_{e\in E}T(e)] = \frac{Nn}{2}}$. Since, all edges in the hypercube are symmetric, ${E[T(e_l)] = E[T(e_m)]}$ for any two edges ${e_l}$ and ${e_m}$, we have \(E[T(e)] =\frac{1}{\vert E \vert} \mathbb{E}\left[\sum_{e\in E}T(e)\right] = \frac{N \cdot \frac{n}{2}}{Nn} = \frac{1}{2}\) Thus, for ${p_i = (e_1, \ldots, p_k)}$, \[E\left[\sum_{j=1}^N H_{ij}\right] \leq \sum_{l=1}^k E[T(e_l)] = \frac{k}{2} \leq \frac{n}{2}\] Now we can apply our first Chernoff bound to get \[\begin{aligned} \Pr\left[\sum_{j=1}^N H_{ij} &gt; 6n\right] &amp;= \Pr\left[\sum_{j=1}^N H_{ij} &gt; (1+11)\frac{n}{2}\right] \\ &amp; &lt; \Pr\left[\sum_{j=1}^N H_{ij} &gt; (1+11)\mu \right] \\ &amp; &lt; \left(\frac{e^{11}}{(1+11)^{1+11}}\right)^{\mu}\\ &amp; &lt; \left(\frac{e^{11}}{(1+11)^{1+11}}\right)^{\frac{n}{2}}\\ &amp; &lt; 2^{-\left(1+11\right)\frac{n}{2}} = 2^{-6n} \end{aligned}\] Since ${\text{delay}(v_i) \leq \sum_{j=1}^N H_{ij}}$, this gives \[\Pr[\text{delay}(v_i) &gt; 6n] &lt; 2^{-6n}\] Since the probability of a union of events is at most the sum of probabilities, the probability that any delay exceeds $6n$ is at most \[\Pr[\max_i \text{delay}(v_i) &gt; 6n] \leq \sum_{i=1}^N \Pr[\text{delay}(v_i) &gt; 6n] &lt; N \cdot 2^{-6n} = 2^n \cdot 2^{-6n} = 2^{-5n}\] Thus, since each path has length at most $n$ \[\Pr[\#Steps \text{ in Phase I} &gt; 7n] = \Pr[(n + \max_i \text{delay}(v_i) &gt; 7n] &lt; 2^{-5n} \quad \quad \square\] Theorem 1.2 With a probability at least $1 - 2^{1-5n}$, every packet reaches its destination in $14n$ or fewer steps. Proof. The Phase 2 of this scheme is identical to Phase 1, if the roles of the destination and source are interchanged. The above analysis is hence valid for the second phase also. Therefore, the probability that any packet fails to reach its target in either phase is less than $2 \times 2^{1-5n}= 2^{1-5n}$. ${\square}$ Kaklamanis, C., Krizanc, D. &amp; Tsantilas, T. Tight bounds for oblivious routing in the hypercube. Math. Systems Theory 24, 223–232 (1991). &#8617;]]></summary></entry><entry><title type="html">Randomized Algs:Chernoff Bound</title><link href="https://wu-haonan.github.io/blog/2023/ADA_Lec_26/" rel="alternate" type="text/html" title="Randomized Algs:Chernoff Bound"/><published>2023-11-20T00:00:00+00:00</published><updated>2023-11-20T00:00:00+00:00</updated><id>https://wu-haonan.github.io/blog/2023/ADA_Lec_26</id><content type="html" xml:base="https://wu-haonan.github.io/blog/2023/ADA_Lec_26/"><![CDATA[<p>This blog, we still talk about randomized algorithms. Today we will focus on a technique known as the Chernoff bound.</p> <h1 id="poisson-trials">Poisson Trials</h1> <p>Let ${0\leq p_1 , \cdots , p_n \leq 1}$, let ${X_1,\cdots,X_n}$ be independent indicator variables with ${\Pr[X_i = 1]= p_i}$ and let ${X = \sum_{i=1}^n X_i}$.</p> <p>We call ${X_1,\cdots,X_n}$ as <strong><em>Poisson Trials</em></strong> and say that ${X}$ has the <strong><em>Poisson Binomial Distribution</em></strong>.</p> <h1 id="first-chernoff-bound">First Chernoff Bound</h1> <p>Let ${X_1,\cdots,X_n}$ be independent Poisson Trials such that, for ${1\leq i \leq n}$, ${\Pr[X_i = 1]=p_i}$, where ${0 &lt; p_i &lt; 1}$. Then, for ${X = \sum_{i=1}^n X_i}$, ${\mu = \mathbb{E}[X] = \sum_{i=1}^n p_i}$ and ${\delta&gt;0}$, we have</p> \[\begin{aligned} \Pr[X &gt; (1+ \delta) \mu] &lt; \left(\frac{e^{\delta}}{(1+\delta)^{(1+\delta)}}\right)^{\mu} \end{aligned}\] <p>Note: Main Ideas of proof</p> <ul> <li>Analyze ${e^{tX}}$ (for some ${t &gt; 0}$) rather than ${X}$.</li> <li>Use independence to turn expectation of a product into a product of expectations.</li> <li>Pick ${t}$ to get best possible bound.</li> </ul> <p><strong>Proof.</strong> For any $t \geq 0$: \(\begin{aligned} \Pr[X &gt; (1 + \delta)\mu] &amp; = \Pr[e^{tX} &gt; e^{t(1+\delta)\mu}] \\ &amp; &lt; \frac{\mathbb{E}[e^{tX}]}{e^{t(1+\delta)\mu}} &amp; (\text{By Markov's inequality}) \\ &amp; = \frac{\prod_{i=1}^n \mathbb{E}[e^{tX_i}]}{e^{t(1+\delta)\mu}} &amp; (\text{By Independence}) \\ &amp; = \frac{\prod_{i=1}^n (e^{t\cdot 0} (1-p_i) + e^{t \cdot 1}p_i )}{e^{t(1+\delta)\mu}} &amp; (\text{By def of } \mathbb{E}) \\ &amp; =\frac{\prod_{i=1}^n (1 + p_i(e^t -1)) }{e^{t(1+\delta)\mu}} &amp; (\text{By def of } \mathbb{E}) \\ &amp; \leq \frac{\prod_{i=1}^n e^{p_i(e^{t}-1)}}{e^{t(1+\delta)\mu}} &amp; (\text{By } 1 + x \leq e^x) \\ &amp; = \frac{e^{(\sum_{i=1}^n p_i(e^{t}-1))}}{e^{t(1+\delta)\mu}} \\ &amp; = \frac{e^{\mu(e^{t}-1)}}{e^{t(1+\delta)\mu}} &amp; (\text{By } \mu = \sum_{i=1}^n p_i)\\ &amp; = \left( \frac{e^{(e^{t}-1)}}{e^{t(1+\delta)}} \right)^\mu \end{aligned}\)</p> <p>Denote ${f(t) = \left( \frac{e^{(e^{t}-1)}}{e^{t(1+\delta)}} \right)^\mu}$, now we find ${t}$ to minimize ${f(t)}$. First, we solve derivative of ${f(t)}$</p> \[\begin{aligned} f'(t) &amp;= \mu \left( \frac{e^{(e^{t}-1)}}{e^{t(1+\delta)}} \right) \left( \frac{e^{(e^{t}-1)}}{e^{t(1+\delta)}} \right)'\\ &amp; = \mu \left( \frac{e^{(e^{t}-1)}}{e^{t(1+\delta)}} \right) \left(e^{(e^t-1)-t(1+\delta)}\right)((e^t-1)-t(1+\delta))'\\ &amp; = \mu \left( \frac{e^{(e^{t}-1)}}{e^{t(1+\delta)}} \right) \left(e^{(e^t-1)-t(1+\delta)}\right) (e^t -1 - \delta) \end{aligned}\] <p>we know ${\mu \left( \frac{e^{(e^{t}-1)}}{e^{t(1+\delta)}} \right) \left(e^{(e^t-1)-t(1+\delta)}\right) &gt; 0}$. Hence, ${f’(t)=0}$ means ${t = \ln (1+\delta)}$. Thus, the ${\min f(t) = f(\ln (1+ \delta)) =\left( \frac{e^{\delta}}{(1+\delta)^{(1+\delta)}} \right)^\mu }$. We have</p> \[\Pr[X &gt; (1 + \delta)\mu] &lt; \left( \frac{e^{\delta}}{(1+\delta)^{(1+\delta)}} \right)^\mu \quad \quad \quad \square\] <h1 id="second-chernoff-bound">Second Chernoff Bound</h1> <p>Let ${X_1,\cdots,X_n}$ be independent Poisson Trials such that, for ${1\leq i \leq n}$, ${\Pr[X_i = 1]=p_i}$, where ${0 &lt; p_i &lt; 1}$. Then, for ${X = \sum_{i=1}^n X_i}$, ${\mu = \mathbb{E}[X] = \sum_{i=1}^n p_i}$ and <font color="red">${0&lt; \delta &lt; 0}$ </font>, we have</p> <p>\(\begin{aligned} \Pr[X &lt; (1- \delta) \mu] &lt; \left(\frac{e^{\delta}}{(1-\delta)^{(1-\delta)}}\right)^{\mu} &lt; e^{\frac{-\delta^2 \mu}{2}} \end{aligned}\) <strong>Proof.</strong> For any $t &gt; 0$:</p> \[\begin{aligned} \Pr[X &lt; (1 - \delta)\mu] &amp; = \Pr[e^{-tX} &gt; e^{-t(1-\delta)\mu}] \\ &amp; &lt; \frac{\mathbb{E}[e^{-tX}]}{e^{-t(1-\delta)\mu}} &amp; (\text{By Markov}) \\ &amp; = \frac{\prod_{i=1}^n \mathbb{E}[e^{-tX_i}]}{e^{-t(1-\delta)\mu}} &amp; (\text{By Independence}) \\ &amp; = \frac{\prod_{i=1}^n (1 + p_i(e^{-t} - 1))}{e^{-t(1-\delta)\mu}} &amp; (\text{By def of } \mathbb{E}) \\ &amp; \leq \frac{\prod_{i=1}^n e^{p_i(e^{-t}-1)}}{e^{-t(1-\delta)\mu}} &amp; (\text{By } 1 + x \leq e^x) \\ &amp; = \frac{e^{\sum_{i=1}^n p_i(e^{-t}-1)}}{e^{-t(1-\delta)\mu}} \\ &amp; = \frac{e^{\mu(e^{-t}-1)}}{e^{-t(1-\delta)\mu}} \\ &amp; = \left( \frac{e^{(e^{-t}-1)}}{e^{-t(1-\delta)}} \right)^\mu \end{aligned}\] <p>Denote ${f(t) = \left( \frac{e^{(e^{-t}-1)}}{e^{-t(1-\delta)}} \right)^\mu}$, now we find ${t}$ to minimize ${f(t)}$. First, we solve derivative of ${f(t)}$</p> \[\begin{aligned} f'(t) &amp;= \mu \left( \frac{e^{(e^{-t}-1)}}{e^{-t(1-\delta)}} \right) \left( \frac{e^{(e^{-t}-1)}}{e^{-t(1-\delta)}} \right)'\\ &amp; = \mu \left( \frac{e^{(e^{-t}-1)}}{e^{-t(1-\delta)}} \right) \left(e^{(e^{-t}-1)+t(1-\delta)}\right)((e^{-t}-1)+t(1-\delta))'\\ &amp; = \mu \left( \frac{e^{(e^{-t}-1)}}{e^{-t(1-\delta)}} \right) \left(e^{(e^{-t}-1)+t(1-\delta)}\right) (-e^{-t} +1 - \delta) \end{aligned}\] <p>we know ${\mu \left( \frac{e^{(e^{-t}-1)}}{e^{-t(1-\delta)}} \right) \left(e^{(e^{-t}-1)+t(1-\delta)}\right)&gt;0}$. Hence, ${f’(t)=0}$ means ${t = -\ln (1-\delta)}$. Thus, the ${\min f(t) = f(-\ln (1- \delta)) =\left( \frac{e^{-\delta}}{(1-\delta)^{(1-\delta)}} \right)^\mu }$. We have</p> \[\Pr[X &lt; (1 - \delta)\mu] &lt; \left( \frac{e^{-\delta}}{(1-\delta)^{(1-\delta)}} \right)^\mu\] <p>Now use the fact that for $-1 &lt; x &lt; 1$</p> \[\begin{align*} \ln(1 + x) &amp; = x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + \ldots &amp; (\text{By Maclaurin series}) \\ \ln(1 - x) &amp; = -x - \frac{x^2}{2} - \frac{x^3}{3} - \frac{x^4}{4} - \ldots \\ -\ln(1 - x) &amp; = x + \frac{x^2}{2} + \frac{x^3}{3} + \ldots &gt; x + \frac{x^2}{2} \end{align*}\] <p>so</p> \[\begin{aligned} (1 - \delta)^{1-\delta} &amp; = e^{(1-\delta)\ln(1-\delta)} \\ &amp; = e^{(-(1-\delta))(-\ln(1-\delta))} \\ &amp; &gt; e^{(\delta-1)(\delta + \frac{\delta^2}{2})} \\ &amp; = e^{-\delta+\frac{\delta^2}{2} + \frac{\delta^3}{2}} \\ &amp; &gt; e^{-\delta+\frac{\delta^2}{2}} \end{aligned}\] <p>We have shown that</p> \[\begin{aligned} \Pr[X &lt; (1 - \delta)\mu] &lt; \left( \frac{e^{-\delta}}{(1 - \delta)^{1-\delta}} \right)^\mu \leq \left( e^{-\frac{\delta^2}{2}} \right)^\mu = e^{-\mu\frac{\delta^2}{2}} \end{aligned} \quad \quad \quad\square\]]]></content><author><name></name></author><category term="Algorithm_Design_and_Analysis"/><category term="Algorithm_Design_and_Analysis"/><summary type="html"><![CDATA[This blog, we still talk about randomized algorithms. Today we will focus on a technique known as the Chernoff bound. Poisson Trials Let ${0\leq p_1 , \cdots , p_n \leq 1}$, let ${X_1,\cdots,X_n}$ be independent indicator variables with ${\Pr[X_i = 1]= p_i}$ and let ${X = \sum_{i=1}^n X_i}$. We call ${X_1,\cdots,X_n}$ as Poisson Trials and say that ${X}$ has the Poisson Binomial Distribution. First Chernoff Bound Let ${X_1,\cdots,X_n}$ be independent Poisson Trials such that, for ${1\leq i \leq n}$, ${\Pr[X_i = 1]=p_i}$, where ${0 &lt; p_i &lt; 1}$. Then, for ${X = \sum_{i=1}^n X_i}$, ${\mu = \mathbb{E}[X] = \sum_{i=1}^n p_i}$ and ${\delta&gt;0}$, we have \[\begin{aligned} \Pr[X &gt; (1+ \delta) \mu] &lt; \left(\frac{e^{\delta}}{(1+\delta)^{(1+\delta)}}\right)^{\mu} \end{aligned}\] Note: Main Ideas of proof Analyze ${e^{tX}}$ (for some ${t &gt; 0}$) rather than ${X}$. Use independence to turn expectation of a product into a product of expectations. Pick ${t}$ to get best possible bound. Proof. For any $t \geq 0$: \(\begin{aligned} \Pr[X &gt; (1 + \delta)\mu] &amp; = \Pr[e^{tX} &gt; e^{t(1+\delta)\mu}] \\ &amp; &lt; \frac{\mathbb{E}[e^{tX}]}{e^{t(1+\delta)\mu}} &amp; (\text{By Markov's inequality}) \\ &amp; = \frac{\prod_{i=1}^n \mathbb{E}[e^{tX_i}]}{e^{t(1+\delta)\mu}} &amp; (\text{By Independence}) \\ &amp; = \frac{\prod_{i=1}^n (e^{t\cdot 0} (1-p_i) + e^{t \cdot 1}p_i )}{e^{t(1+\delta)\mu}} &amp; (\text{By def of } \mathbb{E}) \\ &amp; =\frac{\prod_{i=1}^n (1 + p_i(e^t -1)) }{e^{t(1+\delta)\mu}} &amp; (\text{By def of } \mathbb{E}) \\ &amp; \leq \frac{\prod_{i=1}^n e^{p_i(e^{t}-1)}}{e^{t(1+\delta)\mu}} &amp; (\text{By } 1 + x \leq e^x) \\ &amp; = \frac{e^{(\sum_{i=1}^n p_i(e^{t}-1))}}{e^{t(1+\delta)\mu}} \\ &amp; = \frac{e^{\mu(e^{t}-1)}}{e^{t(1+\delta)\mu}} &amp; (\text{By } \mu = \sum_{i=1}^n p_i)\\ &amp; = \left( \frac{e^{(e^{t}-1)}}{e^{t(1+\delta)}} \right)^\mu \end{aligned}\) Denote ${f(t) = \left( \frac{e^{(e^{t}-1)}}{e^{t(1+\delta)}} \right)^\mu}$, now we find ${t}$ to minimize ${f(t)}$. First, we solve derivative of ${f(t)}$ \[\begin{aligned} f'(t) &amp;= \mu \left( \frac{e^{(e^{t}-1)}}{e^{t(1+\delta)}} \right) \left( \frac{e^{(e^{t}-1)}}{e^{t(1+\delta)}} \right)'\\ &amp; = \mu \left( \frac{e^{(e^{t}-1)}}{e^{t(1+\delta)}} \right) \left(e^{(e^t-1)-t(1+\delta)}\right)((e^t-1)-t(1+\delta))'\\ &amp; = \mu \left( \frac{e^{(e^{t}-1)}}{e^{t(1+\delta)}} \right) \left(e^{(e^t-1)-t(1+\delta)}\right) (e^t -1 - \delta) \end{aligned}\] we know ${\mu \left( \frac{e^{(e^{t}-1)}}{e^{t(1+\delta)}} \right) \left(e^{(e^t-1)-t(1+\delta)}\right) &gt; 0}$. Hence, ${f’(t)=0}$ means ${t = \ln (1+\delta)}$. Thus, the ${\min f(t) = f(\ln (1+ \delta)) =\left( \frac{e^{\delta}}{(1+\delta)^{(1+\delta)}} \right)^\mu }$. We have \[\Pr[X &gt; (1 + \delta)\mu] &lt; \left( \frac{e^{\delta}}{(1+\delta)^{(1+\delta)}} \right)^\mu \quad \quad \quad \square\] Second Chernoff Bound Let ${X_1,\cdots,X_n}$ be independent Poisson Trials such that, for ${1\leq i \leq n}$, ${\Pr[X_i = 1]=p_i}$, where ${0 &lt; p_i &lt; 1}$. Then, for ${X = \sum_{i=1}^n X_i}$, ${\mu = \mathbb{E}[X] = \sum_{i=1}^n p_i}$ and ${0&lt; \delta &lt; 0}$ , we have \(\begin{aligned} \Pr[X &lt; (1- \delta) \mu] &lt; \left(\frac{e^{\delta}}{(1-\delta)^{(1-\delta)}}\right)^{\mu} &lt; e^{\frac{-\delta^2 \mu}{2}} \end{aligned}\) Proof. For any $t &gt; 0$: \[\begin{aligned} \Pr[X &lt; (1 - \delta)\mu] &amp; = \Pr[e^{-tX} &gt; e^{-t(1-\delta)\mu}] \\ &amp; &lt; \frac{\mathbb{E}[e^{-tX}]}{e^{-t(1-\delta)\mu}} &amp; (\text{By Markov}) \\ &amp; = \frac{\prod_{i=1}^n \mathbb{E}[e^{-tX_i}]}{e^{-t(1-\delta)\mu}} &amp; (\text{By Independence}) \\ &amp; = \frac{\prod_{i=1}^n (1 + p_i(e^{-t} - 1))}{e^{-t(1-\delta)\mu}} &amp; (\text{By def of } \mathbb{E}) \\ &amp; \leq \frac{\prod_{i=1}^n e^{p_i(e^{-t}-1)}}{e^{-t(1-\delta)\mu}} &amp; (\text{By } 1 + x \leq e^x) \\ &amp; = \frac{e^{\sum_{i=1}^n p_i(e^{-t}-1)}}{e^{-t(1-\delta)\mu}} \\ &amp; = \frac{e^{\mu(e^{-t}-1)}}{e^{-t(1-\delta)\mu}} \\ &amp; = \left( \frac{e^{(e^{-t}-1)}}{e^{-t(1-\delta)}} \right)^\mu \end{aligned}\] Denote ${f(t) = \left( \frac{e^{(e^{-t}-1)}}{e^{-t(1-\delta)}} \right)^\mu}$, now we find ${t}$ to minimize ${f(t)}$. First, we solve derivative of ${f(t)}$ \[\begin{aligned} f'(t) &amp;= \mu \left( \frac{e^{(e^{-t}-1)}}{e^{-t(1-\delta)}} \right) \left( \frac{e^{(e^{-t}-1)}}{e^{-t(1-\delta)}} \right)'\\ &amp; = \mu \left( \frac{e^{(e^{-t}-1)}}{e^{-t(1-\delta)}} \right) \left(e^{(e^{-t}-1)+t(1-\delta)}\right)((e^{-t}-1)+t(1-\delta))'\\ &amp; = \mu \left( \frac{e^{(e^{-t}-1)}}{e^{-t(1-\delta)}} \right) \left(e^{(e^{-t}-1)+t(1-\delta)}\right) (-e^{-t} +1 - \delta) \end{aligned}\] we know ${\mu \left( \frac{e^{(e^{-t}-1)}}{e^{-t(1-\delta)}} \right) \left(e^{(e^{-t}-1)+t(1-\delta)}\right)&gt;0}$. Hence, ${f’(t)=0}$ means ${t = -\ln (1-\delta)}$. Thus, the ${\min f(t) = f(-\ln (1- \delta)) =\left( \frac{e^{-\delta}}{(1-\delta)^{(1-\delta)}} \right)^\mu }$. We have \[\Pr[X &lt; (1 - \delta)\mu] &lt; \left( \frac{e^{-\delta}}{(1-\delta)^{(1-\delta)}} \right)^\mu\] Now use the fact that for $-1 &lt; x &lt; 1$ \[\begin{align*} \ln(1 + x) &amp; = x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + \ldots &amp; (\text{By Maclaurin series}) \\ \ln(1 - x) &amp; = -x - \frac{x^2}{2} - \frac{x^3}{3} - \frac{x^4}{4} - \ldots \\ -\ln(1 - x) &amp; = x + \frac{x^2}{2} + \frac{x^3}{3} + \ldots &gt; x + \frac{x^2}{2} \end{align*}\] so \[\begin{aligned} (1 - \delta)^{1-\delta} &amp; = e^{(1-\delta)\ln(1-\delta)} \\ &amp; = e^{(-(1-\delta))(-\ln(1-\delta))} \\ &amp; &gt; e^{(\delta-1)(\delta + \frac{\delta^2}{2})} \\ &amp; = e^{-\delta+\frac{\delta^2}{2} + \frac{\delta^3}{2}} \\ &amp; &gt; e^{-\delta+\frac{\delta^2}{2}} \end{aligned}\] We have shown that \[\begin{aligned} \Pr[X &lt; (1 - \delta)\mu] &lt; \left( \frac{e^{-\delta}}{(1 - \delta)^{1-\delta}} \right)^\mu \leq \left( e^{-\frac{\delta^2}{2}} \right)^\mu = e^{-\mu\frac{\delta^2}{2}} \end{aligned} \quad \quad \quad\square\]]]></summary></entry><entry><title type="html">Randomized Algs:Randomized Selection</title><link href="https://wu-haonan.github.io/blog/2023/ADA_Lec_25/" rel="alternate" type="text/html" title="Randomized Algs:Randomized Selection"/><published>2023-11-15T00:00:00+00:00</published><updated>2023-11-15T00:00:00+00:00</updated><id>https://wu-haonan.github.io/blog/2023/ADA_Lec_25</id><content type="html" xml:base="https://wu-haonan.github.io/blog/2023/ADA_Lec_25/"><![CDATA[<p>This blog, we still talk about randomized algorithms. Today we will focus on Randomized Selection Problem.</p> <h1 id="selection-problem-and-lazyselect">Selection Problem and LazySelect</h1> <p>Given unsorted list $S$ with $n = \vert S \vert$ distinct elements, and ${k \in \{1, \ldots, n\}}$, find ${S_{(k)}}$, which is the $k$th element of S in sorted order.</p> <p>For $y \in S$, let $r_S(y) := \vert {y’ \in S \mid y’ \leq y} \vert$ be the <strong>rank</strong> of $y$ in $S$. The equivalent goal is to find $y \in S$ such that $r_S(y) = k$.</p> <p>Observe that $r_S(S_{(k)}) = k$ and $S_{r_S(y)} = y$. (Because, all the elements are distinct).</p> <p>Then, we give the pseudocode of LazySelect Algorithm as below</p> <p class="success">01: function LAZYSELECT(S, k) <br/> 02:   repeat <br/> 03:     R ← random ${n^{3/4}}$ elements from S, picked uniformly at random with replacement. <br/> 04:     Sort R in $O(\vert R \vert \ln \vert R \vert)$ steps. <br/> 05:    x ← $kn^{-1/4}$, $\ell$ ← ${\max \{\lfloor x - \sqrt{n} \rfloor , 1\}}$, $a$ ← $R_{(\ell)}$, $h$ ← ${\min \{ \lceil x - \sqrt{n} \rceil , 1\}}$, $b$ ← $R_{(h)}$. By comparing a and b to every $s \in S$, find $r_S(a)$ and $r_S(b)$. <br/> 06:     \(P \leftarrow \begin{cases} \{y \in S \mid y \leq b\} &amp; \text{if } k &lt; n^{3/4} \\ \{y \in S \mid \leq y \leq b\} &amp; \text{if } k \in [n^{3/4}, n - n^{3/4}] \\ \{y \in S \mid a \leq y\} &amp; \text{if } k &gt; n - n^{3/4} \end{cases}\) <br/> 07:  until $S_{(k)} \in P$ and $\vert P \vert &lt; 4n^{3/4} + 2$ <br/> 08:   Sort P in $O(\vert P \vert \ln \vert P \vert)$ steps. <br/> 09:   return $P_{(k-r_S(a)+1)}$ // This is $S_{(k)}$.</p> <h1 id="explanation-of-algorithm">Explanation of Algorithm</h1> <ul> <li>First, we randomly select ${n^{3/4}}$ elements from ${S}$ and construct list ${R}$.</li> <li>Second, we sort ${R}$ in $O(\vert R \vert \ln \vert R \vert)$ time.</li> <li>The likely position of ${S_{(k)}}$ in ${R}$ is ${x = n^{3/4} \cdot \frac{k}{n} = k n^{-1/4}}$. So, here we select a range around position ${x}$, that is $\ell$ ← ${\max \{\lfloor x - \sqrt{n} \rfloor , 1\}}$, $h$ ← ${\min \{ \lceil x - \sqrt{n} \rceil , 1\}}$. And, we can pick up corresponding element $a$ ← $R_{(\ell)}$, $b$ ← $ R_{(h)}$.</li> <li>Then, we can compare all the elements from $S$ to $a$ and $b$. In the meantime, we can divide $S$ into three categories ${y&lt;a, y\in [a,b],y&gt;b}$.</li> <li>Then, we construct list $P$, in fact we don’t need extra computation, we can select it by above step. <ul> <li>If $k$ is small, that is $k&lt;n^{3/4}$, then we pick up $y \leq b$.</li> <li>If $k$ is in the middle, that is $n^{3/4} &lt; k &lt; n - n^{3/4}$, then we pick up $ a \leq y \leq b$.</li> <li>If $k$ is large, that is $k&gt; n - n^{3/4}$, then we pick up $y \geq a$.</li> </ul> </li> <li>If $P$ contains $S_{(k)}$, and size is small, that is $\vert P \vert &lt; 4n^{3/4} + 2$. We will continue following steps.</li> <li>Sort ${P}$ in $O(\vert P \vert \ln \vert P \vert)$ time.</li> <li>Then, we can get ${S_{(k)}}$ by shifting in list ${P}$, that is $P_{(k-r_S(a)+1)}$.</li> </ul> <h1 id="lazyselect-analysis">LazySelect Analysis</h1> <p><strong>Lemma:</strong> Let ${X_{(i)} = \vert \{ y \in R \mid y \leq S_{(i)} \}\vert}$. Hence we have \(S_{(i)} &lt; R_{(j)} \Leftrightarrow X_{(i)} &lt; j\)</p> <p><strong>Proof.</strong></p> <p>${\Rightarrow}$: If ${S_{(i)} &lt; R_{(j)}}$, we know there at most ${j-1}$ elements in ${R}$ that are less or equal to ${S_{(i)}}$. So, ${X_{(i)} \leq j-1}$, that implies ${X_{(i)} &lt; j}$.</p> <p>${\Leftarrow}$: If ${X_{(i)} &lt; j}$, we know there at most ${j-1}$ elements in ${R}$ that are less or equal to ${S_{(i)}}$. So, ${S_{(i)} \leq R_{(j-1)}}$, that means ${S_{(i)} &lt; R_{(j)}}$. ${\square}$</p> <p>**Theorem: ** With probability at least ${1-n^{-1/4}}$, LazySelect can find ${S_{(k)}}$ after running the loop for only once, and thus it does only ${2n+o(n)}$ comparisons.</p> <p><strong>Proof.</strong> First, we can compute the running time of the algorithm. If the algorithm finished in one loop, we take ${2n}$ comparisons from computing ${r_s(a)}$ and ${r_s{(b)}}$ (we need compare ${a,b}$ to each element in ${S}$) and ${\mathcal{O}(n^{3/4}\ln n)}$ for sorting lists ${R}$ and ${P}$ <a href="https://wu-haonan.github.io/2023/03/05/AL_Lec_2.html#-o--and--omega--notation">(and ${\mathcal{O}(n^{3/4}\ln n )= \mathcal{o}}(n)$)</a>.</p> <p>Following, we need to show LazySelect can find ${S_{(k)}}$ after running the loop for only once with high probability. Here, we assume ${n^{3/4}}$ is an integer and only prove the cases ${n^{3/4} &lt; k &lt; n - n^{3/4}}$. The other two cases are similar and simpler.</p> <p>Assume ${n^{3/4} &lt; k &lt; n - n^{3/4}}$, then ${x \in [\sqrt{n},n^{3/4}-\sqrt{n}]}$. And we have two ways to fail:</p> <p>Type Ⅰ: ${S_{(k)}\notin P}$</p> <p>Type Ⅱ: ${S_{(k)}\in P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor}$</p> <p>So, we have</p> \[\begin{equation} \begin{aligned} \Pr [\text{multiple runs}] &amp;= \Pr[S_{(k)} \notin P \lor \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor] \\ &amp; = \Pr[S_{(k)} \notin P] + \Pr[\vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor] - \Pr[S_{(k)} \notin P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor]\\ &amp;= \Pr[(S_{(k)} \notin P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor) \lor ((S_{(k)} \in P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor)] \\ &amp; \quad + \Pr[S_{(k)} \notin P] - \Pr[S_{(k)} \notin P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor]\\ &amp; = \Pr[(S_{(k)} \notin P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor)] + \Pr[(S_{(k)} \in P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor)] \\ &amp; \quad + \Pr[S_{(k)} \notin P] - \Pr[S_{(k)} \notin P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor]\\ &amp; = \Pr[(S_{(k)} \in P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor)] + \Pr[S_{(k)} \notin P] \\ &amp; = \Pr[\text{Type I}] + \Pr[\text{Type II}] \end{aligned} \end{equation}\] <p>Hence, we can calculate the upper bound of ${\Pr[\text{Type I}] }$ and ${\Pr[\text{Type II}]}$.</p> \[\begin{equation} \begin{aligned} \Pr[S_{(k)} \notin P] = \Pr[S_{(k)} &lt; a] + \Pr[S_{(k)} &gt; a] \end{aligned} \end{equation}\] <p>We denote ${k_{\ell} = \max {1, k - 2 n^{3/4}}}$ and ${k_h = \min {k+ 2n^{3/4}, n}}$, then:</p> \[\begin{equation} \begin{aligned} \Pr[(S_{(k)} \in P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor)] \leq \Pr[S_{(k_\ell)} &gt; a] + \Pr[S_{(k_h)} &lt; b] \end{aligned} \end{equation}\] <p>(Note: here ${k}$ is the median in ${P}$ with high probability, so it’s upper bound is at least ${S_{(k_\ell)} &gt; a}$ or ${S_{(k_h)} &lt; b}$ is true.)</p> <p>Let ${X_i}$ indicate that the ${i}$th element picked for ${R}$ is less or equal to ${S_{(k)}}$, that is</p> \[\begin{equation} X_i = \begin{cases} 1 &amp; \text{If } R_{(i)} \leq S_{(k)}, \\ 0 &amp; \text{otherwise.} \end{cases} \end{equation}\] <p>Then ${\Pr[X_i = 1] = \frac{k}{n}}$ and we denote the random variable ${X}$ as ${X_{(k)}}$ (which means the number of elements in ${R}$ that are less than ${S_{(k)}}$). Hence, we have ${}$${X = \sum_{i=1}^{\vert R \vert} X_i = \sum_{i=1}^{n^{3/4}}} X_i$. We know ${X_i}$ are <strong><em>Bernoulli trials</em></strong> with success probability ${p = \frac{k}{n}}$. Thus,</p> \[\begin{aligned} \mu_X &amp;= \mathbb{E}\left[\sum_{i=1}^{n^{3/4}}X_i \right] = \sum_{i=1}^{n^{3/4}} \mathbb{E}[X_i] = p n^{3/4} = k n^{-1/4} = x\\ \sigma^2_X &amp;= \mathbb{E}[X^2] - \mathbb{E}[X]^2 \\ &amp;= \mathbb{E}\left[(\sum_{i=1}^{n^{3/4}} X_i)^2\right] - (p n^{3/4})^2\\ &amp;=\sum_{i=1}^{n^{3/4}} \mathbb{E}[X_i^2] - (p n^{3/4})^2 + \sum_{i=1}^{n^{3/4}} \sum_{j\neq i} \mathbb{E}[X_i]\mathbb{E}[X_j]\\ &amp;= pn^{3/4} - (p n^{3/4})^2 + p^2n^{3/4}(n^{3/4}-1)\\ &amp; = pn^{3/4} - p^2n^{3/4} \\ &amp; = p(1-p) n^{3/4} \\ &amp; \leq \frac{n^{3/4}}{4} \end{aligned}\] <p>Now, we compute the upper bound of probability of Type Ⅰ.</p> \[\begin{equation} \begin{aligned} \Pr[S_{(k)}&lt;a] &amp;= \Pr[S_{(k)} &lt; R_{(\ell)}] \quad \text{(By lemma)}\\ &amp;= \Pr[X_{(k)}&lt; \ell]\\ &amp;\leq \Pr[X &lt; \lfloor x -\sqrt{n}\rfloor + 1]\\ &amp;\leq \Pr[X \leq x -\sqrt{n}] \quad (\mu_X = x)\\ &amp; = \Pr[X - \mu_x \leq - \sqrt{n}] \\ &amp; \leq \Pr[\vert X - \mu_X \vert \geq \sqrt{n}] \\ &amp; \leq \Pr[\vert X - \mu_X \vert \geq 2n^{1/8} \cdot \frac{n^{3/8}}{2}] \quad (\sigma^2_X \leq \frac{n^{3/4}}{2})\\ &amp; \leq \Pr[\vert X - \mu_X \vert \geq 2n^{1/8} \sigma_X] \quad (\text{By Chebyshev's inequality})\\ &amp; \leq \frac{1}{(2n^{1/8})^2}\\ &amp; = \frac{1}{4} n^{-1/4} \end{aligned} \end{equation}\] <p>Similarly, we have</p> \[\begin{equation} \begin{aligned} \Pr[S_{(k)}&gt; b] &amp;= \Pr[S_{(k)} &gt; R_{(h)}] \quad \text{(By lemma)}\\ &amp;= \Pr[X_{(k)} \geq h]\\ &amp; = \Pr[X \geq \lceil x+ \sqrt{n} \rceil]\\ &amp; \leq \Pr[X \geq x+ \sqrt{n} ] \\ &amp;= \Pr[\vert X -\mu_X \vert \geq \sqrt{n}]\\ &amp; \leq \frac{1}{4} n^{-1/4} \end{aligned} \end{equation}\] <p>Now, we can use same strategy to compute Type Ⅱ failure</p> \[\begin{equation} \begin{aligned} \mu_{X_{(k_{\ell})}} &amp;= \frac{k_{\ell}}{n}n^{3/4} \leq \frac{k - 2n^{3/4}}{n}n^{3/4} = x - 2\sqrt{n} \\ \sigma_{X_{(k_{\ell})}} &amp;= p(1-p) n^{3/4} \leq \frac{n^{3/4}}{4} \end{aligned} \end{equation}\] <p>Hence, we have</p> \[\begin{equation} \begin{aligned} \Pr[S_{(k_{\ell})}&gt; a] &amp;= \Pr[S_{(k_{\ell})} &gt; R_{(\ell)}] \\ &amp;= \Pr[X_{(k_{\ell})} \geq \ell]\\ &amp; = \Pr[X_{(k_{\ell})} \geq \lfloor x - \sqrt{n} \rfloor]\\ &amp; \leq \Pr[X_{(k_{\ell})} \geq x - \sqrt{n} ] \\ &amp; = \Pr[X_{(k_{\ell})} - \mu_{X_{(k_{\ell})}} \geq \sqrt{n} ] \\ &amp; \leq \Pr[\vert X -\mu_{X_{(k_{\ell})}} \vert \geq \sqrt{n}]\\ &amp; \leq \frac{1}{4} n^{-1/4} \end{aligned} \end{equation}\] <p>Similarly, we have</p> \[\begin{equation} \begin{aligned} \mu_{X_{(k_{h})}} &amp;= \frac{k_{h}}{n}n^{3/4} \geq \frac{k + 2n^{3/4}}{n}n^{3/4} = x + 2\sqrt{n} \\ \sigma_{X_{(k_{h})}} &amp;= p(1-p) n^{3/4} \leq \frac{n^{3/4}}{4} \end{aligned} \end{equation}\] <p>Thus, we have</p> \[\begin{equation} \begin{aligned} \Pr[S_{(k_{h})} &lt; b] &amp;= \Pr[S_{(k_{h})} &lt; R_{(h)}] \\ &amp;= \Pr[X_{(k_{h})} &lt; h]\\ &amp; = \Pr[X_{(k_{h})} &lt; \lceil x - \sqrt{n} \rceil]\\ &amp; \leq \Pr[X_{(k_{h})} \leq x - \sqrt{n} ] \\ &amp; = \Pr[X_{(k_{h})} - \mu_{X_{(k_{h})}} \geq \sqrt{n} ] \\ &amp; \leq \Pr[\vert X -\mu_{X_{(k_{h})}} \vert \geq \sqrt{n}]\\ &amp; \leq \frac{1}{4} n^{-1/4} \end{aligned} \end{equation}\] <p>In summary, we have</p> \[\Pr [\text{multiple runs}] \leq \Pr[\text{Type I}] + \Pr[\text{Type II}] \leq 4*\frac{1}{4}n^{-1/4} = n^{-1/4}\] <p>We have shown that with high probability at least ${1-n^{-1/4}}$, <strong>LazySelect</strong> does only ${2n + \mathcal{o}(n)}$ comparisons. ${\square}$</p>]]></content><author><name></name></author><category term="Algorithm_Design_and_Analysis"/><category term="Algorithm_Design_and_Analysis"/><summary type="html"><![CDATA[This blog, we still talk about randomized algorithms. Today we will focus on Randomized Selection Problem. Selection Problem and LazySelect Given unsorted list $S$ with $n = \vert S \vert$ distinct elements, and ${k \in \{1, \ldots, n\}}$, find ${S_{(k)}}$, which is the $k$th element of S in sorted order. For $y \in S$, let $r_S(y) := \vert {y’ \in S \mid y’ \leq y} \vert$ be the rank of $y$ in $S$. The equivalent goal is to find $y \in S$ such that $r_S(y) = k$. Observe that $r_S(S_{(k)}) = k$ and $S_{r_S(y)} = y$. (Because, all the elements are distinct). Then, we give the pseudocode of LazySelect Algorithm as below 01: function LAZYSELECT(S, k) 02:   repeat 03:     R ← random ${n^{3/4}}$ elements from S, picked uniformly at random with replacement. 04:     Sort R in $O(\vert R \vert \ln \vert R \vert)$ steps. 05:    x ← $kn^{-1/4}$, $\ell$ ← ${\max \{\lfloor x - \sqrt{n} \rfloor , 1\}}$, $a$ ← $R_{(\ell)}$, $h$ ← ${\min \{ \lceil x - \sqrt{n} \rceil , 1\}}$, $b$ ← $R_{(h)}$. By comparing a and b to every $s \in S$, find $r_S(a)$ and $r_S(b)$. 06:     \(P \leftarrow \begin{cases} \{y \in S \mid y \leq b\} &amp; \text{if } k &lt; n^{3/4} \\ \{y \in S \mid \leq y \leq b\} &amp; \text{if } k \in [n^{3/4}, n - n^{3/4}] \\ \{y \in S \mid a \leq y\} &amp; \text{if } k &gt; n - n^{3/4} \end{cases}\) 07:  until $S_{(k)} \in P$ and $\vert P \vert &lt; 4n^{3/4} + 2$ 08:   Sort P in $O(\vert P \vert \ln \vert P \vert)$ steps. 09:   return $P_{(k-r_S(a)+1)}$ // This is $S_{(k)}$. Explanation of Algorithm First, we randomly select ${n^{3/4}}$ elements from ${S}$ and construct list ${R}$. Second, we sort ${R}$ in $O(\vert R \vert \ln \vert R \vert)$ time. The likely position of ${S_{(k)}}$ in ${R}$ is ${x = n^{3/4} \cdot \frac{k}{n} = k n^{-1/4}}$. So, here we select a range around position ${x}$, that is $\ell$ ← ${\max \{\lfloor x - \sqrt{n} \rfloor , 1\}}$, $h$ ← ${\min \{ \lceil x - \sqrt{n} \rceil , 1\}}$. And, we can pick up corresponding element $a$ ← $R_{(\ell)}$, $b$ ← $ R_{(h)}$. Then, we can compare all the elements from $S$ to $a$ and $b$. In the meantime, we can divide $S$ into three categories ${y&lt;a, y\in [a,b],y&gt;b}$. Then, we construct list $P$, in fact we don’t need extra computation, we can select it by above step. If $k$ is small, that is $k&lt;n^{3/4}$, then we pick up $y \leq b$. If $k$ is in the middle, that is $n^{3/4} &lt; k &lt; n - n^{3/4}$, then we pick up $ a \leq y \leq b$. If $k$ is large, that is $k&gt; n - n^{3/4}$, then we pick up $y \geq a$. If $P$ contains $S_{(k)}$, and size is small, that is $\vert P \vert &lt; 4n^{3/4} + 2$. We will continue following steps. Sort ${P}$ in $O(\vert P \vert \ln \vert P \vert)$ time. Then, we can get ${S_{(k)}}$ by shifting in list ${P}$, that is $P_{(k-r_S(a)+1)}$. LazySelect Analysis Lemma: Let ${X_{(i)} = \vert \{ y \in R \mid y \leq S_{(i)} \}\vert}$. Hence we have \(S_{(i)} &lt; R_{(j)} \Leftrightarrow X_{(i)} &lt; j\) Proof. ${\Rightarrow}$: If ${S_{(i)} &lt; R_{(j)}}$, we know there at most ${j-1}$ elements in ${R}$ that are less or equal to ${S_{(i)}}$. So, ${X_{(i)} \leq j-1}$, that implies ${X_{(i)} &lt; j}$. ${\Leftarrow}$: If ${X_{(i)} &lt; j}$, we know there at most ${j-1}$ elements in ${R}$ that are less or equal to ${S_{(i)}}$. So, ${S_{(i)} \leq R_{(j-1)}}$, that means ${S_{(i)} &lt; R_{(j)}}$. ${\square}$ **Theorem: ** With probability at least ${1-n^{-1/4}}$, LazySelect can find ${S_{(k)}}$ after running the loop for only once, and thus it does only ${2n+o(n)}$ comparisons. Proof. First, we can compute the running time of the algorithm. If the algorithm finished in one loop, we take ${2n}$ comparisons from computing ${r_s(a)}$ and ${r_s{(b)}}$ (we need compare ${a,b}$ to each element in ${S}$) and ${\mathcal{O}(n^{3/4}\ln n)}$ for sorting lists ${R}$ and ${P}$ (and ${\mathcal{O}(n^{3/4}\ln n )= \mathcal{o}}(n)$). Following, we need to show LazySelect can find ${S_{(k)}}$ after running the loop for only once with high probability. Here, we assume ${n^{3/4}}$ is an integer and only prove the cases ${n^{3/4} &lt; k &lt; n - n^{3/4}}$. The other two cases are similar and simpler. Assume ${n^{3/4} &lt; k &lt; n - n^{3/4}}$, then ${x \in [\sqrt{n},n^{3/4}-\sqrt{n}]}$. And we have two ways to fail: Type Ⅰ: ${S_{(k)}\notin P}$ Type Ⅱ: ${S_{(k)}\in P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor}$ So, we have \[\begin{equation} \begin{aligned} \Pr [\text{multiple runs}] &amp;= \Pr[S_{(k)} \notin P \lor \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor] \\ &amp; = \Pr[S_{(k)} \notin P] + \Pr[\vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor] - \Pr[S_{(k)} \notin P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor]\\ &amp;= \Pr[(S_{(k)} \notin P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor) \lor ((S_{(k)} \in P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor)] \\ &amp; \quad + \Pr[S_{(k)} \notin P] - \Pr[S_{(k)} \notin P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor]\\ &amp; = \Pr[(S_{(k)} \notin P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor)] + \Pr[(S_{(k)} \in P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor)] \\ &amp; \quad + \Pr[S_{(k)} \notin P] - \Pr[S_{(k)} \notin P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor]\\ &amp; = \Pr[(S_{(k)} \in P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor)] + \Pr[S_{(k)} \notin P] \\ &amp; = \Pr[\text{Type I}] + \Pr[\text{Type II}] \end{aligned} \end{equation}\] Hence, we can calculate the upper bound of ${\Pr[\text{Type I}] }$ and ${\Pr[\text{Type II}]}$. \[\begin{equation} \begin{aligned} \Pr[S_{(k)} \notin P] = \Pr[S_{(k)} &lt; a] + \Pr[S_{(k)} &gt; a] \end{aligned} \end{equation}\] We denote ${k_{\ell} = \max {1, k - 2 n^{3/4}}}$ and ${k_h = \min {k+ 2n^{3/4}, n}}$, then: \[\begin{equation} \begin{aligned} \Pr[(S_{(k)} \in P \land \vert P \vert &gt; \lfloor 4 n^3/4 + 2 \rfloor)] \leq \Pr[S_{(k_\ell)} &gt; a] + \Pr[S_{(k_h)} &lt; b] \end{aligned} \end{equation}\] (Note: here ${k}$ is the median in ${P}$ with high probability, so it’s upper bound is at least ${S_{(k_\ell)} &gt; a}$ or ${S_{(k_h)} &lt; b}$ is true.) Let ${X_i}$ indicate that the ${i}$th element picked for ${R}$ is less or equal to ${S_{(k)}}$, that is \[\begin{equation} X_i=\begin{cases} 1 &amp; \text{If } R_{(i)} \leq S_{(k)}, \\ 0 &amp; \text{otherwise.} \end{cases} \end{equation}\] Then ${\Pr[X_i = 1] = \frac{k}{n}}$ and we denote the random variable ${X}$ as ${X_{(k)}}$ (which means the number of elements in ${R}$ that are less than ${S_{(k)}}$). Hence, we have ${}$${X = \sum_{i=1}^{\vert R \vert} X_i=\sum_{i=1}^{n^{3/4}}} X_i$. We know ${X_i}$ are Bernoulli trials with success probability ${p = \frac{k}{n}}$. Thus, \[\begin{aligned} \mu_X &amp;= \mathbb{E}\left[\sum_{i=1}^{n^{3/4}}X_i \right] = \sum_{i=1}^{n^{3/4}} \mathbb{E}[X_i] = p n^{3/4} = k n^{-1/4} = x\\ \sigma^2_X &amp;= \mathbb{E}[X^2] - \mathbb{E}[X]^2 \\ &amp;= \mathbb{E}\left[(\sum_{i=1}^{n^{3/4}} X_i)^2\right] - (p n^{3/4})^2\\ &amp;=\sum_{i=1}^{n^{3/4}} \mathbb{E}[X_i^2] - (p n^{3/4})^2 + \sum_{i=1}^{n^{3/4}} \sum_{j\neq i} \mathbb{E}[X_i]\mathbb{E}[X_j]\\ &amp;= pn^{3/4} - (p n^{3/4})^2 + p^2n^{3/4}(n^{3/4}-1)\\ &amp; = pn^{3/4} - p^2n^{3/4} \\ &amp; = p(1-p) n^{3/4} \\ &amp; \leq \frac{n^{3/4}}{4} \end{aligned}\] Now, we compute the upper bound of probability of Type Ⅰ. \[\begin{equation} \begin{aligned} \Pr[S_{(k)}&lt;a] &amp;= \Pr[S_{(k)} &lt; R_{(\ell)}] \quad \text{(By lemma)}\\ &amp;= \Pr[X_{(k)}&lt; \ell]\\ &amp;\leq \Pr[X &lt; \lfloor x -\sqrt{n}\rfloor + 1]\\ &amp;\leq \Pr[X \leq x -\sqrt{n}] \quad (\mu_X = x)\\ &amp; = \Pr[X - \mu_x \leq - \sqrt{n}] \\ &amp; \leq \Pr[\vert X - \mu_X \vert \geq \sqrt{n}] \\ &amp; \leq \Pr[\vert X - \mu_X \vert \geq 2n^{1/8} \cdot \frac{n^{3/8}}{2}] \quad (\sigma^2_X \leq \frac{n^{3/4}}{2})\\ &amp; \leq \Pr[\vert X - \mu_X \vert \geq 2n^{1/8} \sigma_X] \quad (\text{By Chebyshev's inequality})\\ &amp; \leq \frac{1}{(2n^{1/8})^2}\\ &amp; = \frac{1}{4} n^{-1/4} \end{aligned} \end{equation}\] Similarly, we have \[\begin{equation} \begin{aligned} \Pr[S_{(k)}&gt; b] &amp;= \Pr[S_{(k)} &gt; R_{(h)}] \quad \text{(By lemma)}\\ &amp;= \Pr[X_{(k)} \geq h]\\ &amp; = \Pr[X \geq \lceil x+ \sqrt{n} \rceil]\\ &amp; \leq \Pr[X \geq x+ \sqrt{n} ] \\ &amp;= \Pr[\vert X -\mu_X \vert \geq \sqrt{n}]\\ &amp; \leq \frac{1}{4} n^{-1/4} \end{aligned} \end{equation}\] Now, we can use same strategy to compute Type Ⅱ failure \[\begin{equation} \begin{aligned} \mu_{X_{(k_{\ell})}} &amp;= \frac{k_{\ell}}{n}n^{3/4} \leq \frac{k - 2n^{3/4}}{n}n^{3/4} = x - 2\sqrt{n} \\ \sigma_{X_{(k_{\ell})}} &amp;= p(1-p) n^{3/4} \leq \frac{n^{3/4}}{4} \end{aligned} \end{equation}\] Hence, we have \[\begin{equation} \begin{aligned} \Pr[S_{(k_{\ell})}&gt; a] &amp;= \Pr[S_{(k_{\ell})} &gt; R_{(\ell)}] \\ &amp;= \Pr[X_{(k_{\ell})} \geq \ell]\\ &amp; = \Pr[X_{(k_{\ell})} \geq \lfloor x - \sqrt{n} \rfloor]\\ &amp; \leq \Pr[X_{(k_{\ell})} \geq x - \sqrt{n} ] \\ &amp; = \Pr[X_{(k_{\ell})} - \mu_{X_{(k_{\ell})}} \geq \sqrt{n} ] \\ &amp; \leq \Pr[\vert X -\mu_{X_{(k_{\ell})}} \vert \geq \sqrt{n}]\\ &amp; \leq \frac{1}{4} n^{-1/4} \end{aligned} \end{equation}\] Similarly, we have \[\begin{equation} \begin{aligned} \mu_{X_{(k_{h})}} &amp;= \frac{k_{h}}{n}n^{3/4} \geq \frac{k + 2n^{3/4}}{n}n^{3/4} = x + 2\sqrt{n} \\ \sigma_{X_{(k_{h})}} &amp;= p(1-p) n^{3/4} \leq \frac{n^{3/4}}{4} \end{aligned} \end{equation}\] Thus, we have \[\begin{equation} \begin{aligned} \Pr[S_{(k_{h})} &lt; b] &amp;= \Pr[S_{(k_{h})} &lt; R_{(h)}] \\ &amp;= \Pr[X_{(k_{h})} &lt; h]\\ &amp; = \Pr[X_{(k_{h})} &lt; \lceil x - \sqrt{n} \rceil]\\ &amp; \leq \Pr[X_{(k_{h})} \leq x - \sqrt{n} ] \\ &amp; = \Pr[X_{(k_{h})} - \mu_{X_{(k_{h})}} \geq \sqrt{n} ] \\ &amp; \leq \Pr[\vert X -\mu_{X_{(k_{h})}} \vert \geq \sqrt{n}]\\ &amp; \leq \frac{1}{4} n^{-1/4} \end{aligned} \end{equation}\] In summary, we have \[\Pr [\text{multiple runs}] \leq \Pr[\text{Type I}] + \Pr[\text{Type II}] \leq 4*\frac{1}{4}n^{-1/4} = n^{-1/4}\] We have shown that with high probability at least ${1-n^{-1/4}}$, LazySelect does only ${2n + \mathcal{o}(n)}$ comparisons. ${\square}$]]></summary></entry><entry><title type="html">Randomized Algs:Occupancy Problems &amp;amp; Markov’s and Chebyshev’s inequalities</title><link href="https://wu-haonan.github.io/blog/2023/ADA_Lec_24/" rel="alternate" type="text/html" title="Randomized Algs:Occupancy Problems &amp;amp; Markov’s and Chebyshev’s inequalities"/><published>2023-11-13T00:00:00+00:00</published><updated>2023-11-13T00:00:00+00:00</updated><id>https://wu-haonan.github.io/blog/2023/ADA_Lec_24</id><content type="html" xml:base="https://wu-haonan.github.io/blog/2023/ADA_Lec_24/"><![CDATA[<p>This blog, we still talk about randomized algorithms. Today we will focus on some analysis tools in randomized algorithms. In this blog, we want to explore the probability that some extreme event happens, that is the random variable takes “large” value. So, Markov’s and Chebyshev’s inequalities are also called Tail Inequalities.</p> <h1 id="occupancy-problems">Occupancy Problems</h1> <p>Imagine we have ${m}$ indistinguishable objects (“balls”), that we randomly assign to ${n}$ distinct classes (“bins”).</p> <ul> <li>What is expected maximum number of balls in any bin?</li> <li>What is the expected number of bins with ${k}$ balls?</li> </ul> <p>These are called occupancy problems.</p> <p>Let ${m = n \geq 3}$, and for ${i = 1, \ldots, n}$ let random variable ${X_i}$ be the number of balls in the ${i}$th bin. And ${X_i}$ is a binomial distribution, ${X_i \sim B(n,\frac{1}{n})}$.</p> <p>We want to find ${k}$ such that, with very high probability, no bin contains more than ${k}$ balls. Let ${\mathcal{E}_j(k)}$ be the event that bin ${j}$ contains at least ${k}$ balls $({X_j \geq k})$.</p> <p>First consider ${\mathcal{E}_i(k)}$,</p> \[\begin{aligned} \Pr[X_i = j] &amp;= \binom{n}{j} \left(\frac{1}{n}\right)^j \left(1 - \frac{1}{n}\right)^{n-j} \\ &amp;\leq \binom{n}{j} \left(\frac{1}{n}\right)^j \\ &amp;\leq \left(\frac{ne}{j}\right)^j \left(\frac{1}{n}\right)^j \\ &amp;= \left(\frac{e}{j}\right)^j \\ \end{aligned}\] <p>Note: in above scaling we use Stirling’s approximation in step 3, that is ${n! = \sqrt{2 \pi n} \left(\frac{n}{e}\right)^n e^{c_n}, 0&lt; c_n &lt; \frac{1}{12n} }$. So we have</p> \[\begin{aligned} \binom{n}{j} &amp;= \frac{n!}{j!(n-j)!} \\ &amp;\leq \frac{n^j}{j!} \\ &amp;\leq \frac{n^j}{\sqrt{2 \pi j} \left( \frac{j}{e}\right)^j}\\ &amp;\leq \left(\frac{ne}{j}\right)^j \end{aligned}\] <p>Next, we have</p> \[\begin{aligned} \Pr[\mathcal{E}_i(k)] &amp;\leq \sum_{j=k}^{n} \binom{n}{j} \left(\frac{1}{n}\right)^j \\ &amp;\leq \sum_{j=k}^{n} \left(\frac{e}{j}\right)^j \\ &amp;\leq \left(\frac{e}{k}\right)^k \left(1 + \frac{e}{k} + \left(\frac{e}{k}\right)^2 + \cdots + \left(\frac{e}{k}\right)^{n-k+1} \right) \\ &amp;\leq \left(\frac{e}{k}\right)^k \left(\frac{1}{1 - \frac{e}{k}}\right) \end{aligned}\] <p>Note: in above scaling we use the union bound in step 1,</p> \[\begin{aligned} \Pr[\mathcal{E}_i(k)] &amp;= Pr\left[\cup_{j=k}^n [X_i = j]\right]\\ &amp; \leq \sum_{j=k}^n Pr[X_i = j]\\ &amp;\leq \sum_{j=k}^{n} \binom{n}{j} \left(\frac{1}{n}\right)^j \\ \end{aligned}\] <p>Let ${k^* = \min\{n+1, \left\lceil \frac{2e}{1-e} \frac{\ln n}{\ln \ln n} \right\rceil\}} \leq \left\lceil 3.164 \frac{\ln n}{\ln \ln n} \right\rceil$, then</p> \[\begin{aligned} \Pr[\mathcal{E}_i(k^*)] &amp;\leq \left( \frac{e}{k^*} \right)^{k^*} \left( \frac{1}{1 - \frac{e}{k^*}} \right) \leq n^{-2} \end{aligned}\] <p>The same holds for all ${i}$, so</p> \[\begin{aligned} \Pr\left[\bigcup_{i=1}^n \mathcal{E}_i(k^*)\right] &amp;\leq \sum_{i=1}^n \Pr[\mathcal{E}_i(k^*)] \leq \frac{1}{n} \end{aligned}\] <p>We have shown:</p> <p><strong>Theorem</strong> With probability at least ${1 - \frac{1}{n}}$, every bin has less than ${k^* = \min\{n+1, \left\lceil \frac{2e}{1-e} \frac{\ln n}{\ln \ln n} \right\rceil\}} $ balls in it.</p> <h2 id="birthday-problem">Birthday Problem</h2> <p>Suppose ${m}$ balls are randomly assigned to ${n}$ bins. What is the probability that all balls land in distinct bins?</p> <p>For ${n = 365}$ the question can be interpreted as “how large must a group of people be before it is likely two people have the same birthday”?</p> <p>Let ${\mathcal{E}_i}$ be the event that the ${i}$th ball lands in an empty bin. From <a href="https://wu-haonan.github.io/2023/11/08/ADA_Lec_23.html#analysis-of-algorithm">previous blog</a> we know:</p> \[\begin{aligned} \Pr\left[\bigcap_{i=2}^m \mathcal{E}_i\right] &amp;= \Pr[\mathcal{E}_2] \cdot \Pr[\mathcal{E}_3 | \mathcal{E}_2] \cdot \ldots \cdot \Pr\left[\mathcal{E}_m | \cap_{i=2}^{m-1} \mathcal{E}_i\right]\\ &amp;= \prod_{i=2}^m \left(1 - \frac{i - 1}{n}\right)\\ &amp;\leq \prod_{i=2}^m e^{-\frac{i-1}{n}} = e^{-\frac{m(m-1)}{2n}} \end{aligned}\] <p>For ${m \geq \sqrt{2n} + 1}$, the probability that all are distinct is at most ${1/e}$.</p> <h1 id="markovs-inequality">Markov’s Inequality</h1> <p><strong>Theorem:</strong> Let $Y$ be a random variable taking only non-negative values. Then for all $t &gt; 0$:</p> \[\Pr[Y \geq t] \leq \frac{\mathbb{E}[Y]}{t}\] <p>equivalently, for $k &gt; 0$:</p> \[\Pr[Y \geq k\mathbb{E}[Y]] \leq \frac{1}{k}\] <p><strong>Proof.</strong> Let $Z$ be indicator variable for the event $Y \geq t$, that is</p> \[\begin{equation} Z = \begin{cases} 1 &amp; Y \geq t\\ 0 &amp; Y &lt; t \end{cases} \end{equation}\] <p>Then $Z \leq \frac{Y}{t}$, and thus</p> \[\Pr[Y \geq t] =\Pr[Z=1]=0 \cdot \Pr[Z=0]+1 \cdot\Pr[X=1]= \mathbb{E}[Z] \leq \mathbb{E}\left[\frac{Y}{t}\right] = \frac{\mathbb{E}[Y]}{t}\] <p>Setting $t = k\mathbb{E}[Y]$ we get</p> <p>\(Pr[Y \geq k\mathbb{E}[Y]] = Pr[Y \geq t] \leq \frac{\mathbb{E}[Y]}{t} = \frac{1}{k}\) Hence, we prove the Markov’s Inequality. ${\square}$</p> <p>There’s another proof, I think, which is easier to understand</p> <p><strong>Proof.</strong> By definition, $\mathbb{E}[Y]={\sum_{y} y \Pr[Y=y]}$, we have</p> \[\begin{aligned} \mathbb{E}[Y] &amp;={\sum_{y} y \Pr[Y=y]}\\ &amp;= {\sum_{y\geq a} y \Pr[Y=y]} + {\sum_{y&lt;a} y \Pr[Y=y]}\\ &amp; \geq {\sum_{y\geq a} a \Pr[Y=y]} + 0 \\ &amp; = a \Pr[Y\geq a] \end{aligned}\] <p>Hence, we have ${\Pr[Y \geq t] \leq \frac{\mathbb{E}[Y]}{t}}$. ${\square}$</p> <h1 id="chebyshevs-inequality">Chebyshev’s Inequality</h1> <p>Given a random variable $X$ with expectation $\mathbb{E}[X] = \mu_X$, define its <strong><font color="red">variance</font></strong> as $\sigma_X^2 := \mathbb{E}[(X - \mu_X)^2]$, and its <strong><font color="red">standard deviation</font></strong> as $\sigma_X := \sqrt{\mathbb{E}[(X - \mu_X)^2]}$.</p> <p><strong>Theorem:</strong> Let $X$ be a random variable with expectation $\mu_X$ and standard deviation $\sigma_X$. Then for all $t &gt; 0$:</p> \[\Pr[|X - \mu_X| \geq t\sigma_X] \leq \frac{1}{t^2}\] <p><strong>Proof.</strong> Let $k = t^2$ and $Y = (X - \mu_X)^2$. Then $\sigma_X^2 = \mathbb{E}[Y]$ (by definition) and</p> \[\Pr[|X - \mu_X| \geq t\sigma_X] = \Pr[(X - \mu_X)^2 \geq t^2\sigma_X^2] = \Pr[Y \geq k \mathbb{E}[Y]]\] <p>By Markov’s inequality, we have</p> \[\Pr[Y \geq k \mathbb{E}[Y]] \leq \frac{1}{k} = \frac{1}{t^2}\] <h1 id="summing-2-independent-variances">Summing 2-Independent Variances</h1> <p>Random variables $X_1, \ldots, X_m \in \mathcal{X}$ are <strong>pairwise independent</strong> iff for all $i \neq j$ and all possible $x, y \in \mathbb{R}$, $\Pr[X_i = x \mid X_j = y] = \Pr[X_i = x]$.</p> <p><strong>Claim:</strong> If $X_1, \ldots, X_m \in \mathcal{X}$ are <strong>pairwise independent</strong>, for all $i \neq j$ we have ${\mathbb{E}[X_i X_j] =\mathbb{E}[X_i]\mathbb{E}[X_j] }$.</p> <p><strong>Proof.</strong> ${\Pr[X_i = x \cap X_j = y] = \Pr[X_i = x \mid X_j = y] \cdot \Pr[X_j = y] = \Pr[X_i = x]\Pr[X_j = y] }$, then we have \(\begin{aligned} \mathbb{E}[X_i X_j] &amp;= \sum_{x} \sum_{y} xy \Pr[X_i = x \cap X_j = y]\\ &amp; = \sum_{x} \sum_{y} xy \Pr[X_i = x]\Pr[X_j = y] \\ &amp; = \left(\sum_{x} x \Pr[X_i = x]\right)\left(\sum_{y} y \Pr[X_j = y]\right) \end{aligned}\)</p> <p>Thus, we prove ${\mathbb{E}[X_i X_j] =\mathbb{E}[X_i]\mathbb{E}[X_j] }$. ${\square}$</p> <p><strong>Lemma:</strong> Let $X_1, \ldots, X_m$ be pairwise independent random variables, and let $X = \sum_{i=1}^m X_i$. Then $\sigma_X^2 = \sum_{i=1}^m \sigma_{X_i}^2$.</p> <p><strong>Proof.</strong> Let ${\mu_i = \mathbb{E}[X_i]}$ and ${\mu = \sum_{i=1}^m \mu_i}$. By definition,</p> \[\begin{aligned} \sigma_X^2 &amp;= \mathbb{E}[(X - \mu)^2] \\ &amp;= \mathbb{E}\left[\left(\sum_{i=1}^m X_i - \mathbb{E}[X]\right)^2\right]\\ &amp; =\mathbb{E}\left[\left(\sum_{i=1}^m X_i - \mathbb{E}\left[\sum_{i=1}^m X_i\right]\right)^2\right] \\ &amp; = \mathbb{E}\left[\left(\sum_{i=1}^m X_i - \sum_{i=1}^m \mathbb{E}[X_i]\right]\right)^2\\ &amp;= \mathbb{E}\left[\left(\sum_{i=1}^m (X_i - \mu_i)\right)^2\right] \\ &amp;= \left(\sum_{i=1}^m E[X_i - \mu_i]\right)^2 \\ &amp;= \sum_{i=1}^m \mathbb{E}[(X_i - \mu_i)^2] + 2 \sum_{i&lt;j} \mathbb{E}[(X_i - \mu_i)(X_j - \mu_j)] \text{ (By claim)}\\ &amp;= \sum_{i=1}^m \mathbb{E}[(X_i - \mu_i)^2] + 2 \sum_{i&lt;j} \mathbb{E}[X_i - \mu_i]\mathbb{E}[X_j - \mu_j] \\ &amp;= \sum_{i=1}^m \sigma_{X_i}^2 + 2 \sum_{i&lt;j} 0 \cdot 0 \end{aligned}\] <p>Hence, we prove $\sigma_X^2 = \sum_{i=1}^m \sigma_{X_i}^2$. ${\square}$</p>]]></content><author><name></name></author><category term="Algorithm_Design_and_Analysis"/><category term="Algorithm_Design_and_Analysis"/><summary type="html"><![CDATA[This blog, we still talk about randomized algorithms. Today we will focus on some analysis tools in randomized algorithms. In this blog, we want to explore the probability that some extreme event happens, that is the random variable takes “large” value. So, Markov’s and Chebyshev’s inequalities are also called Tail Inequalities. Occupancy Problems Imagine we have ${m}$ indistinguishable objects (“balls”), that we randomly assign to ${n}$ distinct classes (“bins”). What is expected maximum number of balls in any bin? What is the expected number of bins with ${k}$ balls? These are called occupancy problems. Let ${m = n \geq 3}$, and for ${i = 1, \ldots, n}$ let random variable ${X_i}$ be the number of balls in the ${i}$th bin. And ${X_i}$ is a binomial distribution, ${X_i \sim B(n,\frac{1}{n})}$. We want to find ${k}$ such that, with very high probability, no bin contains more than ${k}$ balls. Let ${\mathcal{E}_j(k)}$ be the event that bin ${j}$ contains at least ${k}$ balls $({X_j \geq k})$. First consider ${\mathcal{E}_i(k)}$, \[\begin{aligned} \Pr[X_i = j] &amp;= \binom{n}{j} \left(\frac{1}{n}\right)^j \left(1 - \frac{1}{n}\right)^{n-j} \\ &amp;\leq \binom{n}{j} \left(\frac{1}{n}\right)^j \\ &amp;\leq \left(\frac{ne}{j}\right)^j \left(\frac{1}{n}\right)^j \\ &amp;= \left(\frac{e}{j}\right)^j \\ \end{aligned}\] Note: in above scaling we use Stirling’s approximation in step 3, that is ${n! = \sqrt{2 \pi n} \left(\frac{n}{e}\right)^n e^{c_n}, 0&lt; c_n &lt; \frac{1}{12n} }$. So we have \[\begin{aligned} \binom{n}{j} &amp;= \frac{n!}{j!(n-j)!} \\ &amp;\leq \frac{n^j}{j!} \\ &amp;\leq \frac{n^j}{\sqrt{2 \pi j} \left( \frac{j}{e}\right)^j}\\ &amp;\leq \left(\frac{ne}{j}\right)^j \end{aligned}\] Next, we have \[\begin{aligned} \Pr[\mathcal{E}_i(k)] &amp;\leq \sum_{j=k}^{n} \binom{n}{j} \left(\frac{1}{n}\right)^j \\ &amp;\leq \sum_{j=k}^{n} \left(\frac{e}{j}\right)^j \\ &amp;\leq \left(\frac{e}{k}\right)^k \left(1 + \frac{e}{k} + \left(\frac{e}{k}\right)^2 + \cdots + \left(\frac{e}{k}\right)^{n-k+1} \right) \\ &amp;\leq \left(\frac{e}{k}\right)^k \left(\frac{1}{1 - \frac{e}{k}}\right) \end{aligned}\] Note: in above scaling we use the union bound in step 1, \[\begin{aligned} \Pr[\mathcal{E}_i(k)] &amp;= Pr\left[\cup_{j=k}^n [X_i = j]\right]\\ &amp; \leq \sum_{j=k}^n Pr[X_i = j]\\ &amp;\leq \sum_{j=k}^{n} \binom{n}{j} \left(\frac{1}{n}\right)^j \\ \end{aligned}\] Let ${k^* = \min\{n+1, \left\lceil \frac{2e}{1-e} \frac{\ln n}{\ln \ln n} \right\rceil\}} \leq \left\lceil 3.164 \frac{\ln n}{\ln \ln n} \right\rceil$, then \[\begin{aligned} \Pr[\mathcal{E}_i(k^*)] &amp;\leq \left( \frac{e}{k^*} \right)^{k^*} \left( \frac{1}{1 - \frac{e}{k^*}} \right) \leq n^{-2} \end{aligned}\] The same holds for all ${i}$, so \[\begin{aligned} \Pr\left[\bigcup_{i=1}^n \mathcal{E}_i(k^*)\right] &amp;\leq \sum_{i=1}^n \Pr[\mathcal{E}_i(k^*)] \leq \frac{1}{n} \end{aligned}\] We have shown: Theorem With probability at least ${1 - \frac{1}{n}}$, every bin has less than ${k^* = \min\{n+1, \left\lceil \frac{2e}{1-e} \frac{\ln n}{\ln \ln n} \right\rceil\}} $ balls in it. Birthday Problem Suppose ${m}$ balls are randomly assigned to ${n}$ bins. What is the probability that all balls land in distinct bins? For ${n = 365}$ the question can be interpreted as “how large must a group of people be before it is likely two people have the same birthday”? Let ${\mathcal{E}_i}$ be the event that the ${i}$th ball lands in an empty bin. From previous blog we know: \[\begin{aligned} \Pr\left[\bigcap_{i=2}^m \mathcal{E}_i\right] &amp;= \Pr[\mathcal{E}_2] \cdot \Pr[\mathcal{E}_3 | \mathcal{E}_2] \cdot \ldots \cdot \Pr\left[\mathcal{E}_m | \cap_{i=2}^{m-1} \mathcal{E}_i\right]\\ &amp;= \prod_{i=2}^m \left(1 - \frac{i - 1}{n}\right)\\ &amp;\leq \prod_{i=2}^m e^{-\frac{i-1}{n}} = e^{-\frac{m(m-1)}{2n}} \end{aligned}\] For ${m \geq \sqrt{2n} + 1}$, the probability that all are distinct is at most ${1/e}$. Markov’s Inequality Theorem: Let $Y$ be a random variable taking only non-negative values. Then for all $t &gt; 0$: \[\Pr[Y \geq t] \leq \frac{\mathbb{E}[Y]}{t}\] equivalently, for $k &gt; 0$: \[\Pr[Y \geq k\mathbb{E}[Y]] \leq \frac{1}{k}\] Proof. Let $Z$ be indicator variable for the event $Y \geq t$, that is \[\begin{equation} Z=\begin{cases} 1 &amp; Y \geq t\\ 0 &amp; Y &lt; t \end{cases} \end{equation}\] Then $Z \leq \frac{Y}{t}$, and thus \[\Pr[Y \geq t] =\Pr[Z=1]=0 \cdot \Pr[Z=0]+1 \cdot\Pr[X=1]= \mathbb{E}[Z] \leq \mathbb{E}\left[\frac{Y}{t}\right] = \frac{\mathbb{E}[Y]}{t}\] Setting $t = k\mathbb{E}[Y]$ we get \(Pr[Y \geq k\mathbb{E}[Y]] = Pr[Y \geq t] \leq \frac{\mathbb{E}[Y]}{t} = \frac{1}{k}\) Hence, we prove the Markov’s Inequality. ${\square}$ There’s another proof, I think, which is easier to understand Proof. By definition, $\mathbb{E}[Y]={\sum_{y} y \Pr[Y=y]}$, we have \[\begin{aligned} \mathbb{E}[Y] &amp;={\sum_{y} y \Pr[Y=y]}\\ &amp;= {\sum_{y\geq a} y \Pr[Y=y]} + {\sum_{y&lt;a} y \Pr[Y=y]}\\ &amp; \geq {\sum_{y\geq a} a \Pr[Y=y]} + 0 \\ &amp; = a \Pr[Y\geq a] \end{aligned}\] Hence, we have ${\Pr[Y \geq t] \leq \frac{\mathbb{E}[Y]}{t}}$. ${\square}$ Chebyshev’s Inequality Given a random variable $X$ with expectation $\mathbb{E}[X] = \mu_X$, define its variance as $\sigma_X^2 := \mathbb{E}[(X - \mu_X)^2]$, and its standard deviation as $\sigma_X := \sqrt{\mathbb{E}[(X - \mu_X)^2]}$. Theorem: Let $X$ be a random variable with expectation $\mu_X$ and standard deviation $\sigma_X$. Then for all $t &gt; 0$: \[\Pr[|X - \mu_X| \geq t\sigma_X] \leq \frac{1}{t^2}\] Proof. Let $k = t^2$ and $Y = (X - \mu_X)^2$. Then $\sigma_X^2 = \mathbb{E}[Y]$ (by definition) and \[\Pr[|X - \mu_X| \geq t\sigma_X] = \Pr[(X - \mu_X)^2 \geq t^2\sigma_X^2] = \Pr[Y \geq k \mathbb{E}[Y]]\] By Markov’s inequality, we have \[\Pr[Y \geq k \mathbb{E}[Y]] \leq \frac{1}{k} = \frac{1}{t^2}\] Summing 2-Independent Variances Random variables $X_1, \ldots, X_m \in \mathcal{X}$ are pairwise independent iff for all $i \neq j$ and all possible $x, y \in \mathbb{R}$, $\Pr[X_i = x \mid X_j=y] = \Pr[X_i = x]$. Claim: If $X_1, \ldots, X_m \in \mathcal{X}$ are pairwise independent, for all $i \neq j$ we have ${\mathbb{E}[X_i X_j] =\mathbb{E}[X_i]\mathbb{E}[X_j] }$. Proof. ${\Pr[X_i = x \cap X_j=y] = \Pr[X_i = x \mid X_j=y] \cdot \Pr[X_j = y] = \Pr[X_i = x]\Pr[X_j = y] }$, then we have \(\begin{aligned} \mathbb{E}[X_i X_j] &amp;= \sum_{x} \sum_{y} xy \Pr[X_i = x \cap X_j=y]\\ &amp; = \sum_{x} \sum_{y} xy \Pr[X_i = x]\Pr[X_j = y] \\ &amp; = \left(\sum_{x} x \Pr[X_i = x]\right)\left(\sum_{y} y \Pr[X_j = y]\right) \end{aligned}\) Thus, we prove ${\mathbb{E}[X_i X_j] =\mathbb{E}[X_i]\mathbb{E}[X_j] }$. ${\square}$ Lemma: Let $X_1, \ldots, X_m$ be pairwise independent random variables, and let $X = \sum_{i=1}^m X_i$. Then $\sigma_X^2 = \sum_{i=1}^m \sigma_{X_i}^2$. Proof. Let ${\mu_i = \mathbb{E}[X_i]}$ and ${\mu = \sum_{i=1}^m \mu_i}$. By definition, \[\begin{aligned} \sigma_X^2 &amp;= \mathbb{E}[(X - \mu)^2] \\ &amp;= \mathbb{E}\left[\left(\sum_{i=1}^m X_i - \mathbb{E}[X]\right)^2\right]\\ &amp; =\mathbb{E}\left[\left(\sum_{i=1}^m X_i - \mathbb{E}\left[\sum_{i=1}^m X_i\right]\right)^2\right] \\ &amp; = \mathbb{E}\left[\left(\sum_{i=1}^m X_i - \sum_{i=1}^m \mathbb{E}[X_i]\right]\right)^2\\ &amp;= \mathbb{E}\left[\left(\sum_{i=1}^m (X_i - \mu_i)\right)^2\right] \\ &amp;= \left(\sum_{i=1}^m E[X_i - \mu_i]\right)^2 \\ &amp;= \sum_{i=1}^m \mathbb{E}[(X_i - \mu_i)^2] + 2 \sum_{i&lt;j} \mathbb{E}[(X_i - \mu_i)(X_j - \mu_j)] \text{ (By claim)}\\ &amp;= \sum_{i=1}^m \mathbb{E}[(X_i - \mu_i)^2] + 2 \sum_{i&lt;j} \mathbb{E}[X_i - \mu_i]\mathbb{E}[X_j - \mu_j] \\ &amp;= \sum_{i=1}^m \sigma_{X_i}^2 + 2 \sum_{i&lt;j} 0 \cdot 0 \end{aligned}\] Hence, we prove $\sigma_X^2 = \sum_{i=1}^m \sigma_{X_i}^2$. ${\square}$]]></summary></entry><entry><title type="html">Randomized Algs:Randomized Min-Cut</title><link href="https://wu-haonan.github.io/blog/2023/ADA_Lec_23/" rel="alternate" type="text/html" title="Randomized Algs:Randomized Min-Cut"/><published>2023-11-08T00:00:00+00:00</published><updated>2023-11-08T00:00:00+00:00</updated><id>https://wu-haonan.github.io/blog/2023/ADA_Lec_23</id><content type="html" xml:base="https://wu-haonan.github.io/blog/2023/ADA_Lec_23/"><![CDATA[<p>This blog, we still talk about randomized algorithms. Today we will focus on Randomized Min-Cut Algorithm.</p> <h1 id="problem-definition-and-randomized-algorithm">Problem definition and Randomized Algorithm</h1> <p><strong>Min-Cut Problem:</strong> Given a connected graph ${G = (V, E)}$, Our goal is to <em>find smallest ${C \subseteq E}$ that splits ${G}$.</em></p> <p>${C}$ is called a min-cut, and we <strong>define</strong> ${\lambda(G) := \vert C\vert}$ is the edge connectivity of ${G}$.</p> <p>Then, we give the pseudocode of Randomized Min-Cut Algorithm as below</p> <figure class="highlight"><pre><code class="language-pseudocode" data-lang="pseudocode"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre>function RandMinCut(V, E)
    while |V| &gt; 2 do
        Pick e ∈ E uniformly at random.
        Contract e and remove self-loops.
    return E
</pre></td></tr></tbody></table></code></pre></figure> <p>Note: In this algorithm, we contract an edge in graph ${G}$ each time, and finally, we left two “nodes” in graph. In fact, the two nodes represent two components, and we return the edges between the two components as result.</p> <h1 id="analysis-of-algorithm">Analysis of Algorithm</h1> <p><strong>Observation 1</strong>: RandMinCut${(G)}$ may return a cut of size greater than ${\lambda(G)}$.</p> <p><strong>Observation 2</strong>: A specific min-cut ${C}$ is returned, if and only if any of the edge from ${C}$ isn’t contracted.</p> <p><strong>Lemma</strong>: ${G_i = (V_i, E_i)}$ with ${n_i = n - i}$ vertices, we have ${\lambda(G_i) \geq \vert C \vert}$.</p> <p><strong>Proof.</strong> We will show following statement to prove this lemma</p> <p>Given a graph ${G=(V,E)}$, we contract an edge ${e={u,v}}$ and get graph ${G/e}$. Then we have ${\lambda(G/e) \geq \lambda(G)}$.</p> <p>We denote ${G/e=(V’,E’)}$, and</p> \[V'=V\cup \{w\}\setminus\{u,v\}\] \[E'=E\cup \left\{\{w,t\} \mid \{v,t\} \{u,t\} \in E\right\} \setminus N(v) \setminus N(u)\] <p>Suppose ${\lambda(G/e) &lt; \lambda(G)}$, we denote the minimum cut of ${G/e}$ is ${C’}$.</p> <ul> <li>Case 1: ${N(w) \cap C’ = \emptyset}$, that means vertex ${w}$ is not incident to any edge in ${C’}$. Hence, let ${C=C’}$, we know ${C}$ is also a cut of ${G}$. We have ${\vert C \vert = \vert C’ \vert = \lambda(G/e)}&lt; \lambda(G)$, which contradicts to the definition of ${\lambda(G)}$.</li> <li>Case 2: ${N(w) \cap C’ \neq \emptyset}$, that means vertex ${w}$ is incident to some edge ${e = \{w,t\} \in C’}$. If we replace the edges in ${C’}$ like ${\{w,t\}}$ as original edges ${\{v,t\} }$ or ${\{u,t\}}$ and construct ${C \subseteq E}$. We have ${\vert C \vert = \vert C’ \vert = \lambda(G/e)}&lt; \lambda(G)$, which contradicts to the definition of ${\lambda(G)}$. ${}$</li> </ul> <p>By, this statement we have \(\vert C \vert = \lambda(G_0) \leq \lambda(G_1) \leq \cdots \leq \lambda(G_i)\) Hence, we prove the lemma. ${\square}$</p> <p><strong>Theorem</strong>: For any min-cut ${C}$, the probability that RandMinCut${(G)}$ returns ${C}$ is ${\geq \frac{2}{n(n-1)}}$.</p> <p><strong>Proof</strong>. Let ${e_1,\cdots,e_{n-2}}$ be the contracted edges, ${G_0 = G}$ and ${G_i = G_{i-1} / e_i}$. We define event ${\mathcal{E}_i := [e_i \not\in C]}$. $C$ is returned iff ${\mathcal{E}_1 \cap \ldots \cap \mathcal{E}_{n-2}}$.</p> <p>Now, we need compute the Conditional Probabilities</p> <p>Given events ${\mathcal{E}_1, \mathcal{E}_2}$, the <em>conditional probability</em> of ${\mathcal{E}_2}$ given ${\mathcal{E}_1}$ is defined as</p> \[Pr[\mathcal{E}_2|\mathcal{E}_1] = \frac{Pr[\mathcal{E}_1 \cap \mathcal{E}_2]}{Pr[\mathcal{E}_1]}\] <p>Then we have that \(Pr[\mathcal{E}_1 \cap \mathcal{E}_2] = Pr[\mathcal{E}_1] \cdot Pr[\mathcal{E}_2|\mathcal{E}_1]\)</p> <p>By induction, for events ${\mathcal{E}_1, \ldots, \mathcal{E}_k}$, we have</p> \[Pr\left[\bigcap_{i=1}^k \mathcal{E}_i\right] = Pr[\mathcal{E}_1] \cdot Pr[\mathcal{E}_2|\mathcal{E}_1] \cdots Pr\left[\mathcal{E}_k \Bigg| \bigcap_{i=1}^{k-1} \mathcal{E}_i\right]\] <p>Hence, we have the probability that ${C}$ is returned</p> \[\begin{aligned} Pr[C \text{ returned}] &amp;= Pr[\mathcal{E}_1 \cap \ldots \cap \mathcal{E}_{n-2}] \\ &amp;= Pr[\mathcal{E}_1] \cdot Pr[\mathcal{E}_2|\mathcal{E}_1] \cdot \ldots \cdot Pr[\mathcal{E}_{n-2}|\mathcal{E}_1 \cap \ldots \cap \mathcal{E}_{n-3}] \\ &amp;= \prod_{i=1}^{n-2} p_i \end{aligned}\] <p>where ${p_i = Pr[\mathcal{E}_i \mid \mathcal{E}_1 \cap \cdots \cap \mathcal{E}_{i-1}] }$.</p> <p>By lemma, we have ${\lambda(G_i)\geq \vert C \vert}$. So ${\vert E_i \vert = \frac{1}{2}\sum_{v \in V_i} d(v) \geq \frac{n_i}{2}\lambda(G_i) \geq \frac{n_i}{2}\vert C\vert}$ (Otherwise for some ${v \in V_i, d(v) \leq \lambda(G_i)}$, we can pick up all the edges that are incident to vertex ${v}$ as a cut that is smaller than min-cut).</p> <p>Then, we have (denote the ${n_i:= \vert V_i \vert = n-i}$, and we also use the assumption that no edge from $C$ has been contracted yet.)</p> \[\begin{aligned} 1 - p_i &amp;= Pr[\text{select }e \in E_{i-1} \text{ in } C \big| \cap^{i-1}_{j=1}\mathcal{E}] \\ &amp;= \frac{|C|}{|E_{i-1}|} \\ &amp;\leq \frac{|C|}{\frac{1}{2}n_{i-1}|C|} \\ &amp;= \frac{2}{n - (i - 1)} = \frac{2}{n + 1 - i} \end{aligned}\] <p>Thus, we have</p> \[p_i \geq 1 - \frac{2}{n + 1 - i} = \frac{n - 1 - i}{n + 1 - i}\] \[\require{cancel} \begin{aligned} Pr[C \text{ returned}] &amp;= \prod_{i=1}^{n-2} p_i \quad \text{where } p_i = Pr[\mathcal{E}_i | \mathcal{E}_1 \cap \ldots \cap \mathcal{E}_{i-1}] \\ &amp;\geq \prod_{i=1}^{n-2} \frac{n - 1 - i}{n + 1 - i} \\ &amp;= \frac{\bcancel{n - 2}}{n} \cdot \frac{\bcancel{n - 3}}{n - 1} \cdot \frac{\bcancel{n - 4}}{\bcancel{n - 2}} \cdot \cdots \frac{\bcancel{3}}{\bcancel{5}} \cdot \frac{2}{\bcancel{4}} \cdot \frac{1}{\bcancel{3}} \\ &amp;= \frac{2}{n(n - 1)} \end{aligned}\] <p>Hence, we prove ${Pr[C \text{ returned}]\geq \frac{2}{n(n - 1)} }$. ${\square}$</p> <h1 id="repeat-makes-better">Repeat makes better</h1> <p>Actually, the above probability is not a good result, but we can repeatedly call RandMinCut${(G)}$ to get better result and return the smallest cut. If we call it ${t\frac{n(n-1)}{2}}$ times, we have \(\begin{aligned} \Pr[\text{not a min-cut}] &amp;\leq \left(1 - \frac{2}{n(n - 1)}\right)^{t\frac{n(n-1)}{2}} \\ &amp;\leq \left(e^{-\frac{2}{n(n-1)}}\right)^{t\frac{n(n-1)}{2}} \\ &amp;= e^{-t} \end{aligned}\)</p> <p>(This uses that $1 + x \leq e^x$ for all $x \in \mathbb{R}$.)</p> <h1 id="las-vegas-vs-monte-carlo">Las Vegas vs Monte Carlo</h1> <p>What is the main difference between RandQS and RandMinCut?</p> <p>Actually, they represents different strategies in randomized algorithm. That are –</p> <p><strong>Las Vegas:</strong></p> <ul> <li>Always returns correct answer.</li> <li>Number of steps used is a random variable. (The expectation of running time is polynomial. )</li> </ul> <p><strong>Monte Carlo:</strong></p> <ul> <li>Some probability of error. (The probability of failure is small.)</li> <li>Number of steps used is determined.</li> </ul>]]></content><author><name></name></author><category term="Algorithm_Design_and_Analysis"/><category term="Algorithm_Design_and_Analysis"/><summary type="html"><![CDATA[This blog, we still talk about randomized algorithms. Today we will focus on Randomized Min-Cut Algorithm. Problem definition and Randomized Algorithm Min-Cut Problem: Given a connected graph ${G = (V, E)}$, Our goal is to find smallest ${C \subseteq E}$ that splits ${G}$. ${C}$ is called a min-cut, and we define ${\lambda(G) := \vert C\vert}$ is the edge connectivity of ${G}$. Then, we give the pseudocode of Randomized Min-Cut Algorithm as below 1 2 3 4 5 function RandMinCut(V, E) while |V| &gt; 2 do Pick e ∈ E uniformly at random. Contract e and remove self-loops. return E Note: In this algorithm, we contract an edge in graph ${G}$ each time, and finally, we left two “nodes” in graph. In fact, the two nodes represent two components, and we return the edges between the two components as result. Analysis of Algorithm Observation 1: RandMinCut${(G)}$ may return a cut of size greater than ${\lambda(G)}$. Observation 2: A specific min-cut ${C}$ is returned, if and only if any of the edge from ${C}$ isn’t contracted. Lemma: ${G_i = (V_i, E_i)}$ with ${n_i = n - i}$ vertices, we have ${\lambda(G_i) \geq \vert C \vert}$. Proof. We will show following statement to prove this lemma Given a graph ${G=(V,E)}$, we contract an edge ${e={u,v}}$ and get graph ${G/e}$. Then we have ${\lambda(G/e) \geq \lambda(G)}$. We denote ${G/e=(V’,E’)}$, and \[V'=V\cup \{w\}\setminus\{u,v\}\] \[E'=E\cup \left\{\{w,t\} \mid \{v,t\} \{u,t\} \in E\right\} \setminus N(v) \setminus N(u)\] Suppose ${\lambda(G/e) &lt; \lambda(G)}$, we denote the minimum cut of ${G/e}$ is ${C’}$. Case 1: ${N(w) \cap C’ = \emptyset}$, that means vertex ${w}$ is not incident to any edge in ${C’}$. Hence, let ${C=C’}$, we know ${C}$ is also a cut of ${G}$. We have ${\vert C \vert = \vert C’ \vert = \lambda(G/e)}&lt; \lambda(G)$, which contradicts to the definition of ${\lambda(G)}$. Case 2: ${N(w) \cap C’ \neq \emptyset}$, that means vertex ${w}$ is incident to some edge ${e = \{w,t\} \in C’}$. If we replace the edges in ${C’}$ like ${\{w,t\}}$ as original edges ${\{v,t\} }$ or ${\{u,t\}}$ and construct ${C \subseteq E}$. We have ${\vert C \vert = \vert C’ \vert = \lambda(G/e)}&lt; \lambda(G)$, which contradicts to the definition of ${\lambda(G)}$. ${}$ By, this statement we have \(\vert C \vert = \lambda(G_0) \leq \lambda(G_1) \leq \cdots \leq \lambda(G_i)\) Hence, we prove the lemma. ${\square}$ Theorem: For any min-cut ${C}$, the probability that RandMinCut${(G)}$ returns ${C}$ is ${\geq \frac{2}{n(n-1)}}$. Proof. Let ${e_1,\cdots,e_{n-2}}$ be the contracted edges, ${G_0 = G}$ and ${G_i = G_{i-1} / e_i}$. We define event ${\mathcal{E}_i := [e_i \not\in C]}$. $C$ is returned iff ${\mathcal{E}_1 \cap \ldots \cap \mathcal{E}_{n-2}}$. Now, we need compute the Conditional Probabilities Given events ${\mathcal{E}_1, \mathcal{E}_2}$, the conditional probability of ${\mathcal{E}_2}$ given ${\mathcal{E}_1}$ is defined as \[Pr[\mathcal{E}_2|\mathcal{E}_1] = \frac{Pr[\mathcal{E}_1 \cap \mathcal{E}_2]}{Pr[\mathcal{E}_1]}\] Then we have that \(Pr[\mathcal{E}_1 \cap \mathcal{E}_2] = Pr[\mathcal{E}_1] \cdot Pr[\mathcal{E}_2|\mathcal{E}_1]\) By induction, for events ${\mathcal{E}_1, \ldots, \mathcal{E}_k}$, we have \[Pr\left[\bigcap_{i=1}^k \mathcal{E}_i\right] = Pr[\mathcal{E}_1] \cdot Pr[\mathcal{E}_2|\mathcal{E}_1] \cdots Pr\left[\mathcal{E}_k \Bigg| \bigcap_{i=1}^{k-1} \mathcal{E}_i\right]\] Hence, we have the probability that ${C}$ is returned \[\begin{aligned} Pr[C \text{ returned}] &amp;= Pr[\mathcal{E}_1 \cap \ldots \cap \mathcal{E}_{n-2}] \\ &amp;= Pr[\mathcal{E}_1] \cdot Pr[\mathcal{E}_2|\mathcal{E}_1] \cdot \ldots \cdot Pr[\mathcal{E}_{n-2}|\mathcal{E}_1 \cap \ldots \cap \mathcal{E}_{n-3}] \\ &amp;= \prod_{i=1}^{n-2} p_i \end{aligned}\] where ${p_i = Pr[\mathcal{E}_i \mid \mathcal{E}_1 \cap \cdots \cap \mathcal{E}_{i-1}] }$. By lemma, we have ${\lambda(G_i)\geq \vert C \vert}$. So ${\vert E_i \vert = \frac{1}{2}\sum_{v \in V_i} d(v) \geq \frac{n_i}{2}\lambda(G_i) \geq \frac{n_i}{2}\vert C\vert}$ (Otherwise for some ${v \in V_i, d(v) \leq \lambda(G_i)}$, we can pick up all the edges that are incident to vertex ${v}$ as a cut that is smaller than min-cut). Then, we have (denote the ${n_i:= \vert V_i \vert = n-i}$, and we also use the assumption that no edge from $C$ has been contracted yet.) \[\begin{aligned} 1 - p_i &amp;= Pr[\text{select }e \in E_{i-1} \text{ in } C \big| \cap^{i-1}_{j=1}\mathcal{E}] \\ &amp;= \frac{|C|}{|E_{i-1}|} \\ &amp;\leq \frac{|C|}{\frac{1}{2}n_{i-1}|C|} \\ &amp;= \frac{2}{n - (i - 1)} = \frac{2}{n + 1 - i} \end{aligned}\] Thus, we have \[p_i \geq 1 - \frac{2}{n + 1 - i} = \frac{n - 1 - i}{n + 1 - i}\] \[\require{cancel} \begin{aligned} Pr[C \text{ returned}] &amp;= \prod_{i=1}^{n-2} p_i \quad \text{where } p_i=Pr[\mathcal{E}_i | \mathcal{E}_1 \cap \ldots \cap \mathcal{E}_{i-1}] \\ &amp;\geq \prod_{i=1}^{n-2} \frac{n - 1 - i}{n + 1 - i} \\ &amp;= \frac{\bcancel{n - 2}}{n} \cdot \frac{\bcancel{n - 3}}{n - 1} \cdot \frac{\bcancel{n - 4}}{\bcancel{n - 2}} \cdot \cdots \frac{\bcancel{3}}{\bcancel{5}} \cdot \frac{2}{\bcancel{4}} \cdot \frac{1}{\bcancel{3}} \\ &amp;= \frac{2}{n(n - 1)} \end{aligned}\] Hence, we prove ${Pr[C \text{ returned}]\geq \frac{2}{n(n - 1)} }$. ${\square}$ Repeat makes better Actually, the above probability is not a good result, but we can repeatedly call RandMinCut${(G)}$ to get better result and return the smallest cut. If we call it ${t\frac{n(n-1)}{2}}$ times, we have \(\begin{aligned} \Pr[\text{not a min-cut}] &amp;\leq \left(1 - \frac{2}{n(n - 1)}\right)^{t\frac{n(n-1)}{2}} \\ &amp;\leq \left(e^{-\frac{2}{n(n-1)}}\right)^{t\frac{n(n-1)}{2}} \\ &amp;= e^{-t} \end{aligned}\) (This uses that $1 + x \leq e^x$ for all $x \in \mathbb{R}$.) Las Vegas vs Monte Carlo What is the main difference between RandQS and RandMinCut? Actually, they represents different strategies in randomized algorithm. That are – Las Vegas: Always returns correct answer. Number of steps used is a random variable. (The expectation of running time is polynomial. ) Monte Carlo: Some probability of error. (The probability of failure is small.) Number of steps used is determined.]]></summary></entry><entry><title type="html">Randomized Algs:Randomized QuickSort</title><link href="https://wu-haonan.github.io/blog/2023/ADA_Lec_22/" rel="alternate" type="text/html" title="Randomized Algs:Randomized QuickSort"/><published>2023-11-06T00:00:00+00:00</published><updated>2023-11-06T00:00:00+00:00</updated><id>https://wu-haonan.github.io/blog/2023/ADA_Lec_22</id><content type="html" xml:base="https://wu-haonan.github.io/blog/2023/ADA_Lec_22/"><![CDATA[<p>From this blog, we will touch on the randomized algorithms. And today we will focus on Randomized Quicksort Algorithm.</p> <p>Why we need Randomized Algorithms?</p> <ul> <li>Faster, but weaker guarantees.</li> <li>Simpler code, but harder to analyze.</li> <li>Sometimes only option, e.g. Big Data, Machine Learning, Security, etc.</li> </ul> <p>First, Let’s review the QuickSort Algorithm</p> <figure class="highlight"><pre><code class="language-pseudocode" data-lang="pseudocode"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre>function QS(S)  //Assumes all elements in S are distinct.
   if |S| ≤ 1 then
      return S
   else
      Pick pivot x ∈ S
      L ← {y ∈ S | y &lt; x}
      R ← {y ∈ S | y &gt; x}
      return QS(L) + [x] + QS(R)
</pre></td></tr></tbody></table></code></pre></figure> <p>For details, you can check it in another <a href="https://wu-haonan.github.io/2023/05/11/AL_Lec_4.html">blog</a>. We know the average-case running time is ${O(n \lg n)}$ and worst-case running time is ${O(n^2)}$ when the input array is in reverse order.</p> <h1 id="randomized-quicksort">Randomized Quicksort</h1> <p>We just need to modify the strategy in “pivot selecting” step.</p> <figure class="highlight"><pre><code class="language-pseudocode" data-lang="pseudocode"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre>function RandQS(S)
   Assumes all elements in S are distinct.
   if |S| ≤ 1 then
      return S
   else
      Pick pivot x ∈ S, uniformly at random
      L ← {y ∈ S | y &lt; x}
      R ← {y ∈ S | y &gt; x}
      return RandQS(L) + [x] + RandQS(R)
</pre></td></tr></tbody></table></code></pre></figure> <p>Following, we will show the analysis of Randomized QuickSort.</p> <h1 id="analysis-of-rand-quicksort">Analysis of Rand QuickSort</h1> <p>Let ${\left[S_{(1)}, \ldots, S_{(n)}\right] := \text{RandQS}(S)}$.</p> <p>For ${i &lt; j}$ let random variable ${X_{ij} \in \{0, 1\}}$ be the number of times that ${S_{(i)}}$ and ${S_{(j)}}$ are compared.</p> \[\mathbb{E}[\#\text{comparisons}] = \mathbb{E}\left[\sum_{1 \leq i &lt; j \leq n} X_{ij}\right] = \sum_{1 \leq i &lt; j \leq n} \mathbb{E}[X_{ij}]\] <details><summary> Here we uses linearity of expectation </summary> $$ \begin{equation} \begin{aligned} \mathbb{E}[A + B] &amp;= \sum_x \sum_y (x+y) \mathbb{P}(X=x,Y=y)\\ &amp;= \sum_x \sum_y x \mathbb{P}(X=x,Y=y) + \sum_x \sum_y y \mathbb{P}(X=x,y=y)\\ &amp;=\sum_x x \sum_y \mathbb{P}(X=x,Y=y) + \sum_y y \sum_x \mathbb{P}(X=x,Y=y) \\ &amp;= \sum_x \sum_y x \mathbb{P}(X=x) + \sum_x \sum_y y \mathbb{P}(Y=y) \\ &amp;=\mathbb{E}[A] + \mathbb{E}[B] \end{aligned} \end{equation} $$ <font color="red">Note: we don't need ${X}$ and ${Y}$ are **independent** here. </font> </details> <p>Since ${X_{ij} \in \{0, 1\}}$, it is an <em>indicator random variable</em> for the event that ${S_{(i)}}$ and ${S_{(j)}}$ are compared.</p> <p>Let ${p_{ij}}$ be the probability of this event. Then</p> \[\mathbb{E}[X_{ij}] = 0 \cdot (1 - p_{ij}) + 1 \cdot p_{ij} = p_{ij}\] <p>Thus the expectation of an indicator variable equals the probability of the indicated event.</p> \[\mathbb{E}[\#\text{comparisons}] = \sum_{i&lt;j} \mathbb{E}[X_{ij}] = \sum_{i&lt;j} p_{ij}\] <p><strong>Lemma</strong>: ${S_{(i)}}$ and ${S_{(j)}}$ are compared <strong>iff</strong> ${S_{(i)}}$ or ${S_{(j)}}$ is first of ${S_{(i)}, \ldots, S_{(j)}}$ to be chosen as pivot.</p> <p><strong>Proof.</strong> Each recursive call returns ${\left[S_{(a)}, \ldots, S_{(b)}\right]}$. Let $x = S_{(c)}$ be the pivot. Suppose $a \leq i &lt; j \leq b$, that looks like</p> <table> <tbody> <tr> <td>${a}$</td> <td>${\cdots}$</td> <td>${i}$</td> <td>${\cdots}$</td> <td>${j}$</td> <td>${\cdots}$</td> <td>${b}$</td> </tr> </tbody> </table> <ul> <li> <p>Case 1: ${c &lt; i}$ or ${c &gt; j}$, ${S_{(i)}}$ and ${S_{(j)}}$ not compared now, but together in recursion.</p> </li> <li> <p>Case 2: ${i &lt; c &lt; j}$, ${S_{(i)}}$ and ${S_{(j)}}$ never compared.</p> </li> <li> <p>Case 3: ${c = i}$ or ${c = j}$, ${S_{(i)}}$ and ${S_{(j)}}$ compared once.</p> </li> </ul> <p>So decision only made when ${i &lt; c &lt; j}$. ${\square}$</p> <p>Thus, the probability of the event that ${S_{(i)}}$ and ${S_{(j)}}$ are compared is</p> \[p_{ij} = \frac{2}{j + 1 - i}\] <p>And, we have (denote ${H_n = 1+ \frac{1}{2} + \frac{1}{3}+ \cdots \frac{1}{n}}$)</p> <p>\(\begin{aligned} \mathbb{E}[\#\text{comparisons}] &amp;= \sum_{i&lt;j} p_{ij} \\ &amp;= \sum_{i&lt;j} \frac{2}{j + 1 - i} \\ &amp;= \sum_{i=1}^{n-1} \sum_{j=i+1}^n \frac{2}{j + 1 - i} (\text{ replace } j+1-i \text{ as } k \text{, that is } j=k+i-1)\\ &amp;= \sum_{i=1}^{n-1} \sum_{k=2}^{n+1-i} \frac{2}{k}\\ &amp;&lt;\sum_{i=1}^{n} \sum_{k=1}^{n} \frac{2}{k}\\ &amp; = \sum_{i=1}^{n} 2H_n \\ &amp; = 2n H_n \end{aligned}\) We know ${H_n = O(\ln n)}$, so the expected number of comparisons is ${O(n \ln n)}$. In fact, we can also show RandQS can finish in ${O(n \ln n)}$ with high probability. (We don’t show it here.)</p>]]></content><author><name></name></author><category term="Algorithm_Design_and_Analysis"/><category term="Algorithm_Design_and_Analysis"/><summary type="html"><![CDATA[From this blog, we will touch on the randomized algorithms. And today we will focus on Randomized Quicksort Algorithm. Why we need Randomized Algorithms? Faster, but weaker guarantees. Simpler code, but harder to analyze. Sometimes only option, e.g. Big Data, Machine Learning, Security, etc. First, Let’s review the QuickSort Algorithm 1 2 3 4 5 6 7 8 function QS(S) //Assumes all elements in S are distinct. if |S| ≤ 1 then return S else Pick pivot x ∈ S L ← {y ∈ S | y &lt; x} R ← {y ∈ S | y &gt; x} return QS(L) + [x] + QS(R) For details, you can check it in another blog. We know the average-case running time is ${O(n \lg n)}$ and worst-case running time is ${O(n^2)}$ when the input array is in reverse order. Randomized Quicksort We just need to modify the strategy in “pivot selecting” step. 1 2 3 4 5 6 7 8 9 function RandQS(S) Assumes all elements in S are distinct. if |S| ≤ 1 then return S else Pick pivot x ∈ S, uniformly at random L ← {y ∈ S | y &lt; x} R ← {y ∈ S | y &gt; x} return RandQS(L) + [x] + RandQS(R) Following, we will show the analysis of Randomized QuickSort. Analysis of Rand QuickSort Let ${\left[S_{(1)}, \ldots, S_{(n)}\right] := \text{RandQS}(S)}$. For ${i &lt; j}$ let random variable ${X_{ij} \in \{0, 1\}}$ be the number of times that ${S_{(i)}}$ and ${S_{(j)}}$ are compared. \[\mathbb{E}[\#\text{comparisons}] = \mathbb{E}\left[\sum_{1 \leq i &lt; j \leq n} X_{ij}\right] = \sum_{1 \leq i &lt; j \leq n} \mathbb{E}[X_{ij}]\] Here we uses linearity of expectation $$ \begin{equation} \begin{aligned} \mathbb{E}[A + B] &amp;= \sum_x \sum_y (x+y) \mathbb{P}(X=x,Y=y)\\ &amp;= \sum_x \sum_y x \mathbb{P}(X=x,Y=y) + \sum_x \sum_y y \mathbb{P}(X=x,y=y)\\ &amp;=\sum_x x \sum_y \mathbb{P}(X=x,Y=y) + \sum_y y \sum_x \mathbb{P}(X=x,Y=y) \\ &amp;= \sum_x \sum_y x \mathbb{P}(X=x) + \sum_x \sum_y y \mathbb{P}(Y=y) \\ &amp;=\mathbb{E}[A] + \mathbb{E}[B] \end{aligned} \end{equation} $$ Note: we don't need ${X}$ and ${Y}$ are **independent** here. Since ${X_{ij} \in \{0, 1\}}$, it is an indicator random variable for the event that ${S_{(i)}}$ and ${S_{(j)}}$ are compared. Let ${p_{ij}}$ be the probability of this event. Then \[\mathbb{E}[X_{ij}] = 0 \cdot (1 - p_{ij}) + 1 \cdot p_{ij} = p_{ij}\] Thus the expectation of an indicator variable equals the probability of the indicated event. \[\mathbb{E}[\#\text{comparisons}] = \sum_{i&lt;j} \mathbb{E}[X_{ij}] = \sum_{i&lt;j} p_{ij}\] Lemma: ${S_{(i)}}$ and ${S_{(j)}}$ are compared iff ${S_{(i)}}$ or ${S_{(j)}}$ is first of ${S_{(i)}, \ldots, S_{(j)}}$ to be chosen as pivot. Proof. Each recursive call returns ${\left[S_{(a)}, \ldots, S_{(b)}\right]}$. Let $x = S_{(c)}$ be the pivot. Suppose $a \leq i &lt; j \leq b$, that looks like ${a}$ ${\cdots}$ ${i}$ ${\cdots}$ ${j}$ ${\cdots}$ ${b}$ Case 1: ${c &lt; i}$ or ${c &gt; j}$, ${S_{(i)}}$ and ${S_{(j)}}$ not compared now, but together in recursion. Case 2: ${i &lt; c &lt; j}$, ${S_{(i)}}$ and ${S_{(j)}}$ never compared. Case 3: ${c = i}$ or ${c = j}$, ${S_{(i)}}$ and ${S_{(j)}}$ compared once. So decision only made when ${i &lt; c &lt; j}$. ${\square}$ Thus, the probability of the event that ${S_{(i)}}$ and ${S_{(j)}}$ are compared is \[p_{ij} = \frac{2}{j + 1 - i}\] And, we have (denote ${H_n = 1+ \frac{1}{2} + \frac{1}{3}+ \cdots \frac{1}{n}}$) \(\begin{aligned} \mathbb{E}[\#\text{comparisons}] &amp;= \sum_{i&lt;j} p_{ij} \\ &amp;= \sum_{i&lt;j} \frac{2}{j + 1 - i} \\ &amp;= \sum_{i=1}^{n-1} \sum_{j=i+1}^n \frac{2}{j + 1 - i} (\text{ replace } j+1-i \text{ as } k \text{, that is } j=k+i-1)\\ &amp;= \sum_{i=1}^{n-1} \sum_{k=2}^{n+1-i} \frac{2}{k}\\ &amp;&lt;\sum_{i=1}^{n} \sum_{k=1}^{n} \frac{2}{k}\\ &amp; = \sum_{i=1}^{n} 2H_n \\ &amp; = 2n H_n \end{aligned}\) We know ${H_n = O(\ln n)}$, so the expected number of comparisons is ${O(n \ln n)}$. In fact, we can also show RandQS can finish in ${O(n \ln n)}$ with high probability. (We don’t show it here.)]]></summary></entry><entry><title type="html">Rounding and Dynamic Programming:Parallel Machines Job Scheduling</title><link href="https://wu-haonan.github.io/blog/2023/ADA_Lec_21/" rel="alternate" type="text/html" title="Rounding and Dynamic Programming:Parallel Machines Job Scheduling"/><published>2023-11-01T00:00:00+00:00</published><updated>2023-11-01T00:00:00+00:00</updated><id>https://wu-haonan.github.io/blog/2023/ADA_Lec_21</id><content type="html" xml:base="https://wu-haonan.github.io/blog/2023/ADA_Lec_21/"><![CDATA[<p>This blog is still talking about “rounding data” strategy and dynamic programing. And this blog will focus on the bin-packing problem.</p> <h1 id="backgroud">Backgroud</h1> <h2 id="definition-of-bin-packing">Definition of Bin-Packing</h2> <p>Given ${ n }$ items with size ${ a_1,a_2,\cdots,a_n }$, where ${ 1 \geq a_1 \geq a_2 \geq \cdots \geq a_n \geq 0 }$. We need pack the items into bins where each bin can hold any subset of items of total size as most ${ 1 }$. And we need to minimize the number of used bins.</p> <h2 id="partition-problem--leq_p--bin-packing">Partition Problem ${ \leq_p }$ Bin-Packing</h2> <p><b>Partition Problem</b>: Given ${ n }$ positive integers ${ b_1,b_2,\cdots, b_n }$, where ${ B = \sum{i=1}^n b_i }$ is even. It’s a decision problem that if we can partition the set ${ \{1,2,\cdots n\} }$ into two sets ${ S }$ and ${ T }$ s.t. ${ \sum_{i\in S} b_i = \sum_{i\in T} b_i}$. In fact, Partition Problem is a NP-Complete problem. (we don’t prove here).</p> <p>Now we will show Partition Problem ${ \leq_p }$ Bin-Packing.</p> <p>We set ${ a_i = 2b_i /B }$, and use algorithm of Bin-Packing to check if we can pack all pieces into two bins.</p> <h1 id="asymptotic-ptas-for-bin-packing">Asymptotic PTAS for Bin-Packing</h1> <h2 id="definition-of-aptas">Definition of APTAS</h2> <p><b>Definition</b>: An asymptotic polynomial time approximation scheme (APTAS) is a family of algorithms ${ A_{\epsilon} }$, along with a constant ${ c }$, where there is an algorithm for each ${ \epsilon \geq 0}$, such that ${ A_{\epsilon} }$ is a ${(1+\epsilon)+c }$-approximation algorithm (for minimization problems) or a ${(1-\epsilon)-c }$-approximation algorithm (for maximization problems)</p> <h2 id="big-picture-of-idea">Big picture of idea</h2> <p>The Dynamic Programming strategy used in schueduling jobs on ${ m }$ machine inspires us. Giving target time equals to the limit of bins as ${ 1 }$. And here we need to figure out the minimum bins to be used.</p> <p>Addtionally, we also divide the pieces into large and small ones.</p> <ol> <li> <p>Suppose we have an algorithm that can pack large pieces in at most ${ (1+\epsilon)OPT + 1 }$ bins. And we will show we can still use at most ${ (1+\epsilon)OPT + 1 }$ bins after packing the remaining small pieces.</p> </li> <li> <p>We still use the rounding strategy for following dynamic Programming, and we will show the relation between rounded instance ${ I’ }$ and original input ${ I }$ satisfying ${ OPT(I’) \leq OPT(I) \leq OPT(I’) + k}$, ${ k }$ is a given constant depending on ${ \epsilon }$</p> </li> <li> <p>We will show the Dynamic Programming will finish in polynomial time.</p> </li> </ol> <h2 id="large-and-small-pieces">Large and small pieces</h2> <p>We define that a piece ${ i }$ is large if ${ a_i \geq \epsilon /2 }$, else call it small one.</p> <ul> <li> <p>Step 1: Find packing for large items in at most ${ (1+\epsilon)OPT + 1 }$ bins.</p> </li> <li> <p>Step 2: Sort the small pieces in non-increasing order as their size.</p> </li> <li> <p>Step 3: Use first-fit algorithm, that is pick each item one by one and pack it to first bin in which it can fit.</p> </li> </ul> <p><b>Lemma 1</b>: Suppose packing for large items is in at most ${ (1+\epsilon)OPT + 1 }$ bins, the above APTAS gives an ${ (1+\epsilon)OPT + 1 }$-approximation algorithm.</p> <p>Proof. If there no extra bins used in step 3. The algorithm gives an ${ (1+\epsilon)OPT + 1 }$-approximation.</p> <p>If there exists a new bin used in step 3. Let’s denote the total used bins is ${ k+1 }$. That means when placing some piece, it can not fit in any fist ${ k }$ bins, which leads to the using of ${ (k+1)^{\text{th}} }$ bin. Hence, we have each first ${ k }$ bins must be occupied at least ${ 1- \epsilon /2}$ size, otherwise, we can pack a small piece into it. Let’s denote the input as ${ I }$, and the total size as ${ SIZE(I) = \sum{i=1}^n a_i }$. So, ${ SIZE(I) \geq \left(1-\frac{\epsilon}{2}\right) k }$. Hence, we have</p> <center>$$ k \leq \frac{SIZE(I)}{\left(1-\frac{\epsilon}{2}\right)} \leq \left(1 + \frac{\epsilon / 2}{1 - \epsilon / 2} \right) SIZE(I) \leq \left(1 + \frac{\epsilon}{2 - \epsilon} \right) SIZE(I) \leq (1+\epsilon) SIZE(I) $$</center> <p>And, we know the ${ OPT(I) \geq SIZE(I) }$ (we must use bins that is greater than the total size of all pieces.) Thus, we got</p> <center>$$ k+1 \leq (1+\epsilon) SIZE(I) + 1 \leq (1+\epsilon) OPT(I) + 1 $$</center> <p>We prove the lemma 1. ${ \square }$</p> <h2 id="rounding-large-items">Rounding Large Items</h2> <p>Given a integer parameter ${ k }$, we round the pieces in following way</p> <ol> <li> <p>Group the pieces as</p> <ul> <li> <p>Group 1: first ${ k }$ largest pieces.</p> </li> <li> <p>Group 2: second ${ k }$ largest pieces.</p> </li> </ul> <p>      ${ \vdots }$</p> <ul> <li>Group ${ i }$: ${ i^{\text{th}} }$ ${ k }$ largest pieces.</li> </ul> <p>      ${ \vdots }$</p> <ul> <li>Last Group: remiaing ${ h }$ smallest pieces, where ${ h \leq k }$</li> </ul> </li> <li> <p>Discard the Group 1.</p> </li> <li> <p>For each group ${ i }$, round the pieces in group ${ i }$ as the largest-size piece.</p> </li> </ol> <p><b>Lemma 2</b>: Denote the original input as ${ I }$ and rounded input as ${ I’ }$, we have</p> <center>$$ OPT(I') \leq OPT(I) \leq OPT(I') + k $$</center> <p>Proof. For the first inequality, if we have a packing of ${ I }$. We can still group them, and applythe packing of group ${ i }$ in ${ I }$ to group ${ i }$ in ${ I’ }$. Because ${ I’ }$ discards the first group, so we can group ${ i }$ in ${ I’ }$ can fit the positions of group ${ i }$ in ${ I }$. It’s clear that, ${ OPT(I’) \leq OPT(I) }$.</p> <p>For second inequality, if we have a packing of ${ I’ }$. We keep the original positions of each pieces in ${ I }$, and we use at most ${ k }$ bins to contain the discarding first ${ k }$ pieces. That implies the ${ OPT(I) \leq OPT(I’) + k}$. ${ \square }$</p> <p><b>Corollary</b>: If we set ${ k = \lfloor \epsilon SIZE(I) \rfloor }$, suppose we find the optimal packing for ${ I’ }$, we will have ${ (1+\epsilon) OPT(I) }$-approximation for ${ I }$.</p> <p>Proof. By Lemma 2, ${ OPT(I’) + k = OPT(I’) + \lfloor \epsilon SIZE(I) \rfloor \leq OPT(I) + \epsilon SIZE(I) \leq OPT(I) + \epsilon OPT = (1+\epsilon) OPT(I) }$. ${ \square }$</p> <h2 id="dynamic-programming">Dynamic Programming</h2> <p>We know the ${ 1 \leq OPT \leq n }$, so we can call Dynamic Programming at most ${ n }$ times to decide the ${ OPT(I’) }$. (By using bisearch, we can do ${ \ln n }$).</p> <p>Following, we will show DP can finish in polynomial time. <a href="https://wu-haonan.github.io/2023/10/30/ADA_Lec_20.html#run-time-in-polynomial">Recall DP in previous blog</a>, we just need to show (1) The number of distinct item sizes is constant (like ${ r }$ is constant) (2) The number of large rounded pieces per bin is a constant.</p> <p><b>Claim 1</b>: The number of larege rounded jobs per bin is a constant.</p> <p>Proof. Large pieces have size at least ${ \epsilon /2 }$. So, larege rounded jobs per bin is ${ 2 / \epsilon }$ is a constant. ${ \square }$</p> <p><b>Claim 2</b>: The number of distinct item sizes is constant</p> <p>Proof. First we have ${ SIZE(I) \geq n \epsilon /2 }$, because each large pieces have size at least ${ \epsilon /2 }$. And the distinct size is ${ \Bigl\lceil \frac{n}{k} \Bigr\rceil -1 = \Bigl\lfloor \frac{n}{k} \Bigr\rfloor \leq \frac{n}{k} = \frac{n}{\lfloor \epsilon SIZE(I) \rfloor }}$</p> <ul> <li> <p>If ${ \epsilon SIZE(I) &lt;1 }$, we know there at most ${ \frac{SIZE(I)}{ \epsilon /2} \leq \frac{1/\epsilon}{\epsilon /2} = \frac{2}{\epsilon^2}}$. We don’t need to round them, we can just use DP (that is ${ k=1 }$).</p> </li> <li> <p>If ${ \epsilon SIZE(I) \geq 1 }$, (we know ${ \lfloor \alpha \rfloor \geq \frac{\alpha}{2} }$) we have</p> </li> </ul> <center>$$ \Bigl\lceil \frac{n}{k} \Bigr\rceil -1 = \Bigl\lfloor \frac{n}{k} \Bigr\rfloor \leq \frac{n}{k} = \frac{n}{\lfloor \epsilon SIZE(I) \rfloor } \leq \frac{n}{\epsilon SIZE(I)} \leq \frac{n}{\lfloor \epsilon SIZE(I) \rfloor } \leq \frac{n}{\epsilon SIZE(I) / 2} \leq \frac{n}{(n \epsilon /2)/2} = \frac{4}{\epsilon^4} $$</center> <p>Hence we prove claim 2. ${ \square }$</p> <p>Furthermore, we can have ${ \vert \mathcal{C} \vert \leq O\left(\frac{1}{\epsilon^2}\right)^{2 / \epsilon}}$ (the distinct sizes is ${ \frac{2}{\epsilon^2} }$ or ${\frac{4}{\epsilon^2} }$).</p> <p>And we know ${ \hat{n_i} \leq k }$, so the DP take at most ${ k \cdot O\left(\frac{1}{\epsilon^2}\right) = O\left(\frac{SIZE(I)}{\epsilon}\right) \leq O\left(\frac{n}{\epsilon}\right)}$ iterations.</p>]]></content><author><name></name></author><category term="Algorithm_Design_and_Analysis"/><category term="Algorithm_Design_and_Analysis"/><summary type="html"><![CDATA[This blog is still talking about “rounding data” strategy and dynamic programing. And this blog will focus on the bin-packing problem.]]></summary></entry><entry><title type="html">Rounding and Dynamic Programming:Parallel Macines Job Scheduling</title><link href="https://wu-haonan.github.io/blog/2023/ADA_Lec_20/" rel="alternate" type="text/html" title="Rounding and Dynamic Programming:Parallel Macines Job Scheduling"/><published>2023-10-30T00:00:00+00:00</published><updated>2023-10-30T00:00:00+00:00</updated><id>https://wu-haonan.github.io/blog/2023/ADA_Lec_20</id><content type="html" xml:base="https://wu-haonan.github.io/blog/2023/ADA_Lec_20/"><![CDATA[<p>From this blog, we will talk about “rounding data” strategy and dynamic programing applied in approximation algorithm in detail. And this blog will focus on the problem “scheduling jobs on identical parallel macines”.</p> <h1 id="polynomial-time-approximation-scheme-ptas">Polynomial time approximation Scheme (PTAS)</h1> <p><b>Definition</b>: A polynomial time approximation scheme (PTAS) is a family of algorithms ${ A_{\epsilon} }$, where there is an algorithm for each ${ \epsilon \geq 0}$, such that ${ A_{\epsilon} }$ is a ${(1+\epsilon) }$-approximation algorithm (for minimization problems) or a ${(1-\epsilon) }$-approximation algorithm (for maximization problems)</p> <p>We should noticed that</p> <ol> <li> <p>Running time depends on ${ \epsilon }$, for fixing ${ \epsilon }$, the runing time is a polynomial function of ${ n }$.</p> </li> <li> <p>PTAS is stronger than other approximation, because there is no lower bound on the approximation ratio.</p> </li> </ol> <h1 id="problem-definition-and-whole-idea">Problem Definition and whole idea</h1> <h2 id="definition-of-problem">Definition of problem</h2> <p>Given ${ n }$ jobs and ${ m }$ machines each able to process one job at a time.</p> <p>Each job is associated with a process time ${ p_j }$. All jobs are available at time ${ 0 }$.</p> <p>We need to assign all the ${ n }$ jobs to ${ m }$ machines. Denote the time that job ${ j }$ is completed as ${ C_j }$.</p> <p>The objective is minimize the makespan ${ C_{max} = \max_j C_j }$.</p> <h2 id="big-picture-of-idea">Big picture of idea</h2> <ol> <li> <p>First we will split the jobs into two kinds – long jobs and short jobs. We will show <b>if</b> we can schedule these long jobs with ${ (1+\epsilon) }$-approximation, then the result is <b>still ${ (1+\epsilon) }$-approximation</b> after scheduling these short jobs.</p> </li> <li> <p>So we move to how to scheduling long jobs with ${ (1+\epsilon) }$-approximation. We can give a series time ${ T }$ and find an algorithm to <b>check if we can finish all the jobs in given time ${ T }$</b>. We select a series of ${ T }$ like ${ 1, (1+\epsilon), (1+\epsilon)^2, \cdots, (1+\epsilon)^n }$, and we find the minimum ${ T^* }$ such that we can complete all the jobs, that will guarantee ${ (1+\epsilon) }$-approximation.</p> </li> <li> <p>Now we need to find an algorithm to check if we can finish all the jobs in given time ${ T }$. We will take the <b>rounding strategy</b>. We will round process time of each job ${ j }$ as ${ \lfloor \frac{p_j}{\mu} \rfloor}$, so each process time is a multiple of ${ \mu }$, it’s more easy to apply Dynamic Programming. And we also prove that <b>if</b> we can schedule these rounding jobs with ${ (1+\epsilon) }$-approximation, then the scheduling plan is <b>still ${ (1+\epsilon) }$-approximation</b> for original jobs.</p> </li> <li> <p>Then, we will design the <b>Dynamic Programming</b> algorithm for rounding jobs and prove it can be done in polynomial time.</p> </li> </ol> <h1 id="long-and-short-jobs">Long and short jobs</h1> <p>We can choose some ${ T^* }$ such that ${ T^* \leq OPT }$. We define a job as long job if ${ p_j \geq \epsilon T^* }$, otherwise it is a short job.</p> <p>Suppose we have an algorithm that can schedule long jobs within ${ (1+\epsilon)OPT }$. We can get following PTAS</p> <ul> <li> <p>Step 1: Use “oracle” to schedule long jobs.</p> </li> <li> <p>Step 2: Add the short jobs one by one, each time placing the job on the least loaded machine.</p> </li> </ul> <h2 id="aproximation-analysis">aproximation analysis</h2> <p><b>Lemma 1</b>：The PTAS gives a sechedule of length ${ C_{max} = (1+\epsilon)OPT }$.</p> <p>Proof. Let ${ \ell }$ be the job that finishes last in the schedule</p> <ul> <li> <p>Case 1: Job ${ \ell }$ is a long job. Then, the makespan doesn’t change after adding short jobs. So, ${ C_{max} = (1+\epsilon)OPT }$.</p> </li> <li> <p>Case 2: Job ${ \ell }$ is a long job. Let ${ S_{\ell} }$ be the start time of job ${ \ell }$. Because, all machines have load at least ${ S_{\ell} }$ (By step2, we place the job on the least loaded machine). Hence, ${ S_{\ell} }$ must be not greater than average process time, that is ${ S_{\ell} \leq \sum_{j \neq \ell} p_j /m \leq \sum_{j} p_j /m }$. And we know the optimal makespan can not be less than average process time. Denote ${ P = \sum_{j} p_j }$, we get</p> </li> </ul> <center>$$ S_{\ell} \leq \frac{P}{m} \leq OPT $$</center> <p>Thus, we get</p> <center>$$ C_{max} = S_{\ell} + p_{\ell} \leq OPT + p_{\ell} \leq OPT + \epsilon T^* \leq OPT + \epsilon OPT = (1+\epsilon) OPT $$</center> <p>We prove lemma 1. ${ \square }$</p> <h2 id="runing-time-analysis">Runing time analysis</h2> <p>It’s clear Step 2 can be done in polynomial time. We will show, by selecting appropriate ${ T^* }$ and assuming ${ m }$ is constant, we can finish Step 1 in polynomial time.</p> <p><b>Lemma 2</b>: If ${ T^* = \frac{P}{m}}$, then an optimal schedule of the long jobs can be found in ${ O(m^{m/\epsilon}) }$ time.</p> <p>Proof. We noticed that the number of long jobs is no more than ${ \frac{P}{\epsilon T^*} = P / \left(\epsilon \frac{P}{m}\right) = \frac{m}{\epsilon} }$. Each long job have ${ m }$ choices, so we can try all posible schedule in ${ O(m^{m/\epsilon}) }$ time. ${ \square }$</p> <h1 id="relaxed-decision-procedure">Relaxed decision procedure</h1> <p>In the next part, we will give an algorithm ${ \mathcal{B}_{\epsilon} }$, that takes a time ${ T }$ as input. And algorithm ${ \mathcal{B}_{\epsilon} }$ either prove that no schedule of length ${ T }$, or else find a schedule of length ${ (1+\epsilon)T }$. Suppose we have the above algorithm, we will show how we can employ ${ \mathcal{B}_{\epsilon} }$ to solve the original problem.</p> <h2 id="bisection-search">Bisection Search</h2> <p><b>Here, we ask all the process times are integer.</b> First we give the lower and upper bound of ${ OPT }$.</p> <center>$$ L_0 = \max \left\{ \Bigl\lceil \sum_{j=1}^n p_j / m \Bigr\rceil, \max_{j=1,\cdots,n}p_j \right\} $$</center> <center>$$ U_0 = \Bigl\lceil \sum_{j=1}^n p_j / m \Bigr\rceil+ \max_{j=1,\cdots,n}p_j $$</center> <p>Note ${ L_0, U_0 }$ gets from <a href="https://wu-haonan.github.io/2023/10/23/ADA_Lec_18.html">pervious blog</a> (${ OPT }$ makespan must be not less than max process time and average time on ${ m }$ machines. The upper bound can be get by considering the last complete job, which less and equal to average time plus max process time.) Through out the bisection search, we maintain two invariants (1) lower bound ${ L \leq OPT }$, and (2) compute a schedule with makespan at most ${ (1+\epsilon)U }$.</p> <p>In each iteration, the current interval is ${ [L,U] }$, we set ${ T=\lfloor (L+U)/2\rfloor }$, and run ${ \mathcal{B}_{\epsilon} }$.</p> <ol> <li> <p>If ${ \mathcal{B}_{\epsilon} }$ produces a schedule, then update ${ U \leftarrow T }$</p> </li> <li> <p>Otherwise, ${ L \leftarrow T + 1 }$.</p> </li> </ol> <p>The ${ \mathcal{B}_{\epsilon} }$ terminates untill ${ L =U }$.</p> <p>It’s clear to check in each iteration, two variants are keeping correct. And we know the difference between ${ L_0 }$ and ${ U_0 }$ is at most ${\max_{j=1,\cdots,n}p_j }$, denoted as ${ P_m }$. So ${ \mathcal{B}_{\epsilon} }$ stops in ${ O(\ln P_m) }$ steps.</p> <p>And, we know ${ L = U \leq OPT}$, and ${ \mathcal{B}_{\epsilon} }$ output schedule with makespan ${ (1+\epsilon)U = (1+\epsilon)L \leq OPT }$.</p> <h1 id="rounding-long-jobs">Rounding long jobs</h1> <p>Now we will give the algorithm ${ \mathcal{B}_{\epsilon} }$. First, we need to round the data and apply Dynamic Programming to solve the rounded data. In this section, we will show how to round and prove the approximation.</p> <p>Let ${ \mu = \epsilon^2 T }$; Round the process time of each long job ${ j}$ as ${ p_j’ = \lfloor \frac{p_j}{\mu} \rfloor \mu }$</p> <p><b> Lemma 3</b>: If ${ T \geq OPT }$, and Dynamic Programming can give a schedule on rounded jobs in target time ${ T }$. ${ \mathcal{B}_{\epsilon} }$ will give an ${ (1+\epsilon)T }$ schedule.</p> <p>Proof. For each job ${ j }$, the difference of ${ p_j }$ and ${ p_j’ }$ bounded by</p> <center>$$ p_j - p_j' = p_j - \Bigl\lfloor \frac{p_j}{\mu} \Bigr\rfloor \mu \leq \mu= \epsilon^2 T $$</center> <p>When Dynamic Programming gives schedule on rounded jobs, we directly use the schedule for original jobs. Because we only consider long job here, so we have ${ p_j \geq \epsilon T }$ by definition. And the makespan at most ${ T }$ for each machine, so there at most ${ \frac{T }{p_j} \leq \frac{T}{\epsilon T} = \frac{1}{\epsilon} }$ jobs.</p> <p>Hence, we increase the process time per machine at most ${\frac{1}{\epsilon} \cdot \epsilon^2 T = \epsilon T}$, that means the total length of schedule of original jobs is ${ (1+\epsilon)T }$ approximation. ${ \square }$</p> <h1 id="dynamic-programming-of-rounded-long-jobs">Dynamic Programming of rounded long jobs</h1> <p>Then all the process times of rounded jobs are multiple of ${ \mu }$ and let the largest be ${ r \mu }$.</p> <p>We sat a job is of type ${ i }$ if its rounded time is ${ i \mu }$.</p> <p>We define a vector ${ (n_1,n_2,\cdots,n_r) }$ represents a set of jobs where each ${ n_i }$ is the number of jobs of type ${ i }$.</p> <p>Let ${ \mathcal{C} }$ denote the set of all possible vectors for which the total rounded processing time is at most ${ T }$, i.e., the set of jobs represented by vector in ${ \mathcal{C} }$ can be completed in ${ T }$ in one machine.</p> <p>Denoted the vector of rounded instance is ${ (\hat{n_1},\hat{n_2},\cdots,\hat{n_r}) }$.</p> <p>We define the subproblem of DP as ${ F_k(n_1,n_2,\cdots,n_r) }$, that returns True or False, meaning if we can schedule instance ${ (n_1,n_2,\cdots,n_r) }$ on ${ k }$ machine in time ${ T }$.</p> <p>The state transition equation is: ${ F_k(n_1,n_2,\cdots,n_r) = \text{TRUE} }$ if and only if</p> <ol> <li> <p>${ (n_1,n_2,\cdots,n_r) \in \mathcal{C} }$ or</p> </li> <li> <p><b>There exists</b> ${ (s_1,s_2,\cdots,s_r) \in \mathcal{C} }$, such that ${ F_k(n_1-s_1,n_2-s_2,\cdots,n_r-s_r) = \text{TRUE} }$</p> </li> </ol> <p>And finnaly we need to compute ${ F_m(\hat{n_1},\hat{n_2},\cdots,\hat{n_r}) }$. Additionally, retrieve the corresponding schedule in the DP, we can return the schedule for all long rounded jobs.</p> <h2 id="run-time-in-polynomial">Run-time in Polynomial</h2> <p>We need to prove the total running time is in polynomial time by showing the number of iterations in DP is in polynomial time and each iteration consumes in polynomial time. The key points are proving ${ \vert \mathcal{C} \vert }$ and ${ r }$</p> <p><b>Claim 1</b>: ${ r }$ is a constant</p> <p>Proof. Let ${ p_m = \max_j p_j }$. As ${ P_m \leq OPT \leq T }$. So the largest rounded processing time is ${ \Bigl\lfloor \frac{p_m}{\mu} \Bigr\rfloor \mu \leq \Bigl\lfloor \frac{T}{\mu} \Bigr\rfloor \mu }$. So ${r= \Bigl\lfloor \frac{T}{\mu} \Bigr\rfloor \leq \frac{T}{\mu} = \frac{T}{\epsilon^2 T} = \frac{1}{\epsilon^2} }$ is a constant. ${ \square }$</p> <p><b>Claim 2</b>: The number of rounded jobs per machine is a constant.</p> <p>Proof. From previous discussion, we already prove that there is at most ${ \frac{T }{p_j} \leq \frac{T}{\epsilon T} = \frac{1}{\epsilon} }$ jobs. ${ \square }$</p> <p><b>Claim 3</b>: ${ \vert \mathcal{C} \vert }$ is a constant.</p> <p>Proof. One machine can only have ${ \frac{1}{\epsilon} }$ jobs, so ${ \forall (n_1,n_2,\cdots,n_r) \in \mathcal{C} }$, ${ n_1,n_2,\cdots,n_r \leq \frac{1}{\epsilon} }$, and ${ r }$ is constant. So, ${ \vert \mathcal{C} \vert }$ is a constant. ${ \square }$</p> <p><b>Claim 4</b>: Number of iterations in DP is polynomial.</p> <p>Proof. One machine can only have ${ \frac{1}{\epsilon} }$ jobs, so the total long jobs can not greater that ${ \frac{m}{\epsilon} }$. Hence, ${ \hat{n_i} \leq \frac{m}{\epsilon} }$. So Number of iterations in DP is at most ${ m \left(\frac{m}{\epsilon}\right)^r \leq m \left(\frac{m}{\epsilon}\right)^{1 /\epsilon^2} }$. ${ \square }$</p> <p>So, it’s clear that total running time of DP is in polynomial time.</p>]]></content><author><name></name></author><category term="Algorithm_Design_and_Analysis"/><category term="Algorithm_Design_and_Analysis"/><summary type="html"><![CDATA[From this blog, we will talk about “rounding data” strategy and dynamic programing applied in approximation algorithm in detail. And this blog will focus on the problem “scheduling jobs on identical parallel macines”.]]></summary></entry><entry><title type="html">Gready Alg and Local Search:Minimum-Degree Spanning Tree</title><link href="https://wu-haonan.github.io/blog/2023/ADA_Lec_19/" rel="alternate" type="text/html" title="Gready Alg and Local Search:Minimum-Degree Spanning Tree"/><published>2023-10-25T00:00:00+00:00</published><updated>2023-10-25T00:00:00+00:00</updated><id>https://wu-haonan.github.io/blog/2023/ADA_Lec_19</id><content type="html" xml:base="https://wu-haonan.github.io/blog/2023/ADA_Lec_19/"><![CDATA[<p>This blog is still talking about greedy algorithm and local search. And this blog will focus on the minimum-degree Spanning Tree problem.</p> <h1 id="statement-of-problem">Statement of Problem</h1> <p><strong>Minimum-degree spanning trees:</strong></p> <ul> <li> <p>Instance: Graph $G = (V, E)$ with $\vert V\vert = n$.</p> </li> <li> <p>Solution: Find a spanning tree $T$ of $G$.</p> </li> <li> <p>Objective: minimize the maximum degree of vertices in $T$.</p> </li> </ul> <p>Note: A spanning tree has maximum degree two <strong>iff</strong> it is a Hamiltonian path. And deciding if a graph has a Hamiltonian path is NP complete.</p> <p><strong>Theorem:</strong> Given a graph it is NP-complete to decide whether it has a spanning tree of maximum degree two.</p> <h1 id="main-idea-of-local-search-algorithm">Main idea of Local Search Algorithm</h1> <p>Start with arbitrary spanning tree $T$</p> <p>To reduce degree of vertex $u$ with high ${d_T(u)}$, find all edge ${{v, w}}$ s.t. ${T + {v, w}}$ create a cycle with $u$. If $\max{d_T(v), d_T(w)} \leq d_T(u) - 2$; We can pick up an edge $e$ in this cycle which is also incident to node ${u}$. Then we create $T’ = T - e + (v, w)$ (we call it a local move).</p> <p>Note: $d_{T’}(u) = d_T(u) - 1$ and $\max{d_T(v), d_T(w)} \leq d_T(u) - 2 + 1 = d_T(u) - 1$. That means we indeed reduce the degree in this local part of the spanning tree.</p> <p>In polynomial time, we can do local move on very high degree vertices.</p> <h1 id="algorithm-and-analysis">Algorithm and Analysis</h1> <h2 id="local-search-algorithm">Local Search Algorithm:</h2> <ul> <li> <p>Step 1: Start with arbitrary spanning tree $T$. Let ${\Delta(T) = \max_{u \in V} d_T(u); l = \left\lceil \log n \right\rceil}$</p> </li> <li> <p>Step 2: pick a node $u$ with degree at least $\Delta(T) - l$ and try to reduce its degree by local move.</p> </li> <li> <p>Step 3: Stop if no improvement possible for any node $u$ with degree at least $\Delta(T) - l$.</p> </li> </ul> <h2 id="approximation-analysis">Approximation Analysis</h2> <p>Let’s first analysis the approximation of algorithm.</p> <p><strong>Theorem:</strong> Let $T^*$ be a locally optimal tree. Then ${\Delta(T^*) \leq 2OPT + l, l = \left\lceil \log n \right\rceil}$</p> <ol> <li>We can first find the lower bound of $OPT$. Let’s remove $k$ edges from $T$; this creates $k + 1$ connected components.</li> </ol> <ul> <li> <p>Let’s denote the set $S$ of nodes such that each $e \in E$ connecting two of the $k + 1$ components is incident on $S$. (In another word, we find all the edge in $G$ that connected any pair of the components, and we pick up the nodes of these edge in set $S$.)</p> </li> <li> <p>For any spanning tree of $G$, it must have at least $k$ edges with endpoints in these $k+1$ different components.</p> </li> <li> <p>For any spanning tree of $G$ average degree (here means the degree in the the spanning tree instead of the degree in $G$) of a node in $S$ is at least $\frac{k}{\vert S \vert }$. (If we want to connect to component, we have to add an edge with an endpoint in $S$, so the sum of degree of each note in $S$ is at least $k$, which implies the average degree is at least $\frac{k}{\vert S \vert }$.)</p> </li> <li> <p>Hence we have $OPT \geq \max_{s \in S}{d_T(s)} \geq \frac{k}{\vert S \vert}$.</p> </li> </ul> <ol> <li>Let $S_i$ be the set of nodes with degree at least $i$ in $T^*$.</li> </ol> <p><strong>Claim 1:</strong> For each $S_i$, with $i \geq \Delta(T) - l + 1$:</p> <p>a) there are at least $(i - 1)\vert S_i \vert + 1$ distinct edges of $T^*$ incident on $S_i$.</p> <p>b) after removing these edges, any edge connecting two different components are incident on $S_{i-1}$.</p> <p><strong>Claim 2:</strong> $\exists i \geq \Delta(T) - l + 1$ s.t. $\vert S_{i-1} \vert \leq 2\vert S_i \vert$.</p> <p>By claim 2, we can find such $i$ and take $S = S_{i-1}$, then we have</p> \[OPT \geq \frac{(i-1)\vert S_i\vert + 1}{\vert S_{i-1} \vert } \geq \frac{(i-1)\vert S_i \vert + 1}{2 \vert S_i \vert } \geq \frac{i-1}{2} + \frac{1}{2 \vert S_i \vert} \geq (\Delta(T)-l)/2 \geq (\Delta(T^*)-l)/2\] <p>Note, here we don’t know exactly the edge set that will generate set $S_{i-1}$. But, we can observe the node set $S_{i}$, if we remove all the edges of $T^*$ that are incident on $S_i$, we can get $S_{i-1}$. In this process, we know $k \geq (i - 1) \vert S_i \vert + 1$. Thus, we can use the conclusion $OPT \geq \frac{k}{\vert S \vert}$. Additionally, we do local move for these high degree nodes, so $\Delta(T) \geq \Delta(T^*)$.</p> <p>Hence, we prove the theorem. $\square$</p> <p>Now, I will show the proof of above claims that are not trivial.</p> <p><strong>Claim 1:</strong> For each $S_i$, with $i \geq \Delta(T) - l + 1$:</p> <p>a) there are at least $(i - 1)\vert S_i\vert + 1$ distinct edges of $T^*$ incident on $S_i$.</p> <p>b) after removing these edges, any edge connecting two different components are incident on $S_{i-1}$.</p> <p><strong>Proof.</strong></p> <p>a) there are at least $(i - 1)\vert S_i\vert + 1$ distinct edges of $T^*$ incident on $S_i$.</p> <p>There are at least $i\vert S_i \vert$ edges of $T^*$ incident on nodes of $S_i$ as each vertex of $S_i$ has degree at least $i$. Because $T^*$ is a spanning tree, so there at most $\vert S_i \vert - 1$ join two vertices in $S_i$ (In another word, the edges that linked two nodes in $S_i$ are count twice in $i \vert S_i \vert $.) Hence, there at least $i \vert S_i \vert - \vert S_i \vert - 1 = (i-1)\vert S_i \vert +1 $ distinct edges of $T^*$ are incident on $S_i$.</p> <p>b) after removing these edges, any edge connecting two different components are incident on $S_{i-1}$.</p> <p>After removing all the edges in $T^*$ incident on $S_i$, we will break $T^*$ into several components. Now, we focus on the edges (in $G$) that connect two components (after removing above edges).</p> <p><strong>Case 1:</strong> The edge incident to $S_i$, because $S_i \subseteq S_{i-1}$. Hence this kind of edges are also incident to $S_{i-1}$.</p> <p><strong>Case 2:</strong> The edge is not incident to $S_i$, but we know it can connect two component. And the two components are created by removing the edges in $T^*$ incident to $S_i$. So, there exists a node $u$ belonging to $S_i$ in one of the two components. So, this edge $e={s,t}$ creates a cycle in $T^*+e$, and the cycle includes node $u$. Because $T^*$ is locally optimal, and $d_{T^*}(u) \geq i \geq \Delta(T) - \ell + 1$, and we know the degree of $s,t$ cannot be both $ \leq i -2$, otherwise, their degree in $T^*$ will must be $\leq d_{T^*}(u)-2$ and we have can do local move for $u$ by $T^*+e$ ($d_{T^*}(u) \geq \Delta(T) - \ell$, we can pick up $u$ in <strong>algorithm</strong>) . Hence, one of endpoints in $e$ (w.l.g we denote as $s$) will have $d_{T^*}(s) &gt; i-2 $ i.e. $d_{T^*}(s) \geq i- 1$. Thus, we get $s \in S_{i-1}$. That means $e$ is incident on $S_{i-1}$. $\square$</p> <p><strong>Claim 2:</strong> $\exists i \geq \Delta(T) - l + 1$ s.t. $\vert S_{i-1} \vert \leq 2\vert S_i \vert$.</p> <p><strong>Proof.</strong> Suppose, for any $i\geq \Delta(T) - l + 1$, we have $\vert S_{i-1} \vert &gt; 2\vert S_i \vert$. Let $i = \Delta(T) - l + 1$, we have \(\vert S_{\Delta(T) -\ell} \vert &gt; 2 \vert S_{\Delta(T) -\ell+1} \vert \cdots 2^\ell \vert S_{\Delta(T)} \vert &gt; 2^{\lceil \lg n \rceil} \vert S_{\Delta(T)} \vert &gt; n \vert S_{\Delta(T)} \vert &gt; n\)</p> <p>Note, here $\vert S_{\Delta(T)}\vert \geq 1$, otherwise, it cannot be a spanning tree. The above conclusion contradicts to the fact that there are $n$ vertices in the graph $G$. $\square$</p> <h3 id="review-the-idea-of-proof-for-approx-ratio">Review the idea of proof for approx ratio</h3> <p>The <strong>algorithm</strong> starts with an arbitrary spanning tree $T$ and using $\Delta(T)$ as parameter to conduct the local search, and terminates resulting in $T^*$. We define a series set $S_i$ (where $i$ relates to $\Delta(T)$), and we prove that if we moving all the edges (denote as $E_i$) in $T^*$ incident $S_i$, we find any edges that connect two components (generating by removing $E_i$) are incident to $S_{i-1}$, and we can lower bound (which relates to $\vert S_i \vert$) the size of $\vert E_i \vert $ . By claim 2, we can find a specific $i-1$, and consider this specific set $S_{i-1}$, that can have a upper bound (relates to size of $\vert S_i\vert$). Through the relation between average degree in $T^*$ of $S_{i-1}$ and $OPT$, we can get the lower bound of $OPT$ is $(\Delta(T)-\ell) /2$, which will not less than $(\Delta(T^*)-\ell) /2$.</p> <h2 id="running-time-analysis">Running time analysis</h2> <p>Then, let’s analysis the running time. In this proof, we will learn the strategy of potential function.</p> <p><strong>Theorem:</strong> The algorithm finds a locally optimal tree polynomial time. For tree $T$ define the potential function $\phi(T) = \sum_{v \in V} 3^{d_T(v)}$.</p> <ul> <li> <p>Initially: $\phi(T) \leq n^3\Delta(T) \leq n3^n$</p> </li> <li> <p>Lowest: for Hamiltonian path: potential is $2\cdot3 + (n - 2)3^2 &gt; n$</p> </li> </ul> <p><strong>Claim 1:</strong> At each step potential function drops by $\left( 1 - \frac{2}{27n^3} \right)$.</p> <p>Then, we know after $\frac{27}{2} n^4 \ln 3$ local moves the potential function is:</p> \[\left(1 - \frac{2}{27n^3}\right)^{\frac{27}{2} n^4 \ln 3} \cdot (n3^n)\leq e^{-n \ln 3} \cdot (n3^n) = n\] <p>As lower bound of potential function of spanning tree is $n$, so after $O(n^4)$ moves no local move possible and we must terminates the algorithm. $\square$</p> <p>Now, I will show the decrease ration mentioned in <strong>claim 1</strong>.</p> <p><strong>Claim 1:</strong> At each step potential function drops by $\left( 1 - \frac{2}{27n^3} \right)$.</p> <p><strong>Proof.</strong> Denote the current spanning tree is $T’$. Suppose the algorithm reduces the degree of a vertex $u$ from $i$ to $i - 1$, where $i \geq \Delta(T) - \ell$, and adds an edge $(v, w)$. We know the degree of $v,w$ at most $i-2$ before local move. And we know the function $f(x) = 3^x - 3^{x-1} = 2 \cdot 3^{x-1} $ is a monotonically increasing.</p> <p>Then the increase in the potential function due to increasing the degree of $v$ and $w$ is at most $2 \cdot (3^{i-1} - 3^{i-2}) = 4 \cdot 3^{i-2}$. The decrease in the potential function due to decreasing the degree of $u$ is $3^i - 3^{i-1} = 2 \cdot 3^{i-1}$. And we have</p> \[3^\ell = 3^{\lceil \log_2 n\rceil} \leq 3 \cdot 3^{\log_2 n} \leq 3 \cdot 2^{2\log_2 n} = 3n^2\] <p>Therefore, the overall decrease in the potential function is at least</p> \[\begin{aligned} 2 \cdot 3^{i-1} - 4 \cdot 3^{i-2} &amp;= 2 \cdot 3^{i} \geq 2 \cdot 3^{\Delta(T) - \ell} \\ &amp;= \frac{2\cdot 3^{\Delta(T)}}{3^\ell}\\ &amp; \geq \frac{2\cdot 3^{\Delta(T)}}{3n^2} \\ &amp; = \frac{2\cdot n 3^{\Delta(T)}}{3n^3} \\ &amp; \geq \frac{2\cdot n 3^{\Delta(T')}}{3n^3} \\ &amp; \geq \frac{2\cdot \sum_{v\in V} 3^{d_{T'}(v)}}{27n^3} \\ &amp; = \frac{2}{27n^3} \cdot \phi(T') \\ \end{aligned}\] <p>Thus, for the resulting tree $T’’$ we have that $\phi(T’’) \leq \left( 1 - \frac{2}{27n^3} \right) \phi(T’)$. $\square$</p>]]></content><author><name></name></author><category term="Algorithm_Design_and_Analysis"/><category term="Algorithm_Design_and_Analysis"/><summary type="html"><![CDATA[This blog is still talking about greedy algorithm and local search. And this blog will focus on the minimum-degree Spanning Tree problem.]]></summary></entry></feed>